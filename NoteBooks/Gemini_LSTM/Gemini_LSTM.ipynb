{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4705aa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install optuna-integration[pytorch_lightning]\n",
    "#pip install --upgrade jupyter ipywidgets\n",
    "\n",
    "# Importaciones generales\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# PyTorch y PyTorch Lightning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, TQDMProgressBar\n",
    "\n",
    "# Scikit-learn para preprocesamiento\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Optuna para optimización de hiperparámetros\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "# Visualización\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "# Configuraciones adicionales\n",
    "warnings.filterwarnings('ignore')\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Configuración del dispositivo (GPU o CPU)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Dispositivo seleccionado: {DEVICE}\")\n",
    "\n",
    "# Configuración para reproducibilidad\n",
    "pl.seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc66f498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "df_train = pd.read_parquet(\"../../Datos/df_train.parquet\")\n",
    "df_test = pd.read_parquet(\"../../Datos/df_test.parquet\")\n",
    "\n",
    "# --- Creación de la columna de fecha ---\n",
    "# Para crear un índice de tiempo correcto, combinamos año y semana.\n",
    "# Usamos el día 1 de la semana (%w=1) para consistencia.\n",
    "df_train['date'] = pd.to_datetime(df_train['anio'].astype(str) + '-' + df_train['semana'].astype(str) + '-1', format='%Y-%W-%w')\n",
    "df_train = df_train.sort_values(by=['id_bar', 'date']).reset_index(drop=True)\n",
    "\n",
    "print(\"Datos de entrenamiento cargados y ordenados:\")\n",
    "print(df_train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652ccda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregación global\n",
    "df_agg = df_train.groupby('date')['dengue'].sum().reset_index()\n",
    "\n",
    "# Gráfico de la serie de tiempo agregada\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=df_agg['date'], y=df_agg['dengue'], mode='lines', name='Casos de Dengue Totales'))\n",
    "fig.update_layout(\n",
    "    title_text='Casos de Dengue Agregados a lo Largo del Tiempo',\n",
    "    xaxis_title='Fecha',\n",
    "    yaxis_title='Número de Casos de Dengue',\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32b4f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra de 5 barrios\n",
    "sample_barrios = df_train['id_bar'].unique()[:5]\n",
    "fig = make_subplots(rows=len(sample_barrios), cols=1, shared_xaxes=True, subplot_titles=[f\"Barrio {b}\" for b in sample_barrios])\n",
    "\n",
    "for i, bar_id in enumerate(sample_barrios):\n",
    "    df_barrio = df_train[df_train['id_bar'] == bar_id]\n",
    "    fig.add_trace(go.Scatter(x=df_barrio['date'], y=df_barrio['dengue'], mode='lines', name=f'Barrio {bar_id}'), row=i+1, col=1)\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"Casos de Dengue para una Muestra de Barrios\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba46192",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Usaremos la serie agregada para el análisis de autocorrelación\n",
    "dengue_agg_series = df_agg.set_index('date')['dengue']\n",
    "\n",
    "# Gráficos ACF y PACF\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "plot_acf(dengue_agg_series, lags=52, ax=axes[0])\n",
    "plot_pacf(dengue_agg_series, lags=52, ax=axes[1])\n",
    "axes[0].set_title('Autocorrelation Function (ACF)')\n",
    "axes[1].set_title('Partial Autocorrelation Function (PACF)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b396d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Codificación de ESTRATO ---\n",
    "df_train['ESTRATO'] = df_train['ESTRATO'].astype('category')\n",
    "df_processed = pd.get_dummies(df_train, columns=['ESTRATO'], prefix='estrato')\n",
    "\n",
    "# --- Selección de Variables ---\n",
    "target_col = 'dengue'\n",
    "# Excluimos variables de ID y tiempo que no son features directas\n",
    "features_to_exclude = ['id', 'id_bar', 'anio', 'semana', 'date', target_col]\n",
    "feature_cols = [col for col in df_processed.columns if col not in features_to_exclude]\n",
    "\n",
    "# Separar features numéricas para escalado\n",
    "numerical_features = df_train.select_dtypes(include=np.number).columns.tolist()\n",
    "numerical_features.remove('id_bar')\n",
    "numerical_features.remove('anio')\n",
    "# 'semana' es cíclica, pero la trataremos como numérica por simplicidad aquí\n",
    "# 'dengue' (target) se escalará por separado\n",
    "\n",
    "# Las features numéricas reales para el escalador\n",
    "features_to_scale = [f for f in feature_cols if f in numerical_features]\n",
    "\n",
    "print(f\"Número total de features: {len(feature_cols)}\")\n",
    "print(f\"Features a escalar: {features_to_scale}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a13534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- División Temporal ---\n",
    "train_df = df_processed[df_processed['anio'] < 2021]\n",
    "val_df = df_processed[df_processed['anio'] == 2021]\n",
    "\n",
    "print(f\"Tamaño del set de entrenamiento: {len(train_df)}\")\n",
    "print(f\"Tamaño del set de validación: {len(val_df)}\")\n",
    "\n",
    "# --- Escalado ---\n",
    "# Escalador para las features\n",
    "feature_scaler = StandardScaler()\n",
    "train_df.loc[:, features_to_scale] = feature_scaler.fit_transform(train_df[features_to_scale])\n",
    "val_df.loc[:, features_to_scale] = feature_scaler.transform(val_df[features_to_scale])\n",
    "# Escalamos todo el dataframe para el entrenamiento final\n",
    "df_processed.loc[:, features_to_scale] = feature_scaler.transform(df_processed[features_to_scale])\n",
    "\n",
    "\n",
    "# Escalador para el target (muy importante para la predicción)\n",
    "target_scaler = StandardScaler()\n",
    "train_df.loc[:, [target_col]] = target_scaler.fit_transform(train_df[[target_col]])\n",
    "val_df.loc[:, [target_col]] = target_scaler.transform(val_df[[target_col]])\n",
    "df_processed.loc[:, [target_col]] = target_scaler.transform(df_processed[[target_col]])\n",
    "\n",
    "print(\"\\nEscalado completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f43e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset personalizado para series de tiempo multivariadas.\n",
    "    Genera secuencias de datos y sus correspondientes etiquetas.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, group_ids, feature_cols, target_col, sequence_length):\n",
    "        self.data = data\n",
    "        self.group_ids = group_ids\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_col = target_col\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        self.sequences = self._create_sequences()\n",
    "\n",
    "    def _create_sequences(self):\n",
    "        \"\"\"\n",
    "        Crea secuencias para cada grupo (id_bar).\n",
    "        \"\"\"\n",
    "        seqs = []\n",
    "        # Agrupamos por barrio para no crear secuencias que mezclen datos de distintos lugares\n",
    "        for bar_id in self.group_ids:\n",
    "            group_data = self.data[self.data['id_bar'] == bar_id]\n",
    "            features = group_data[self.feature_cols].values\n",
    "            target = group_data[self.target_col].values\n",
    "            \n",
    "            num_samples = len(group_data)\n",
    "            if num_samples > self.sequence_length:\n",
    "                for i in range(num_samples - self.sequence_length):\n",
    "                    x = features[i:i + self.sequence_length]\n",
    "                    y = target[i + self.sequence_length]\n",
    "                    seqs.append((x, y))\n",
    "        return seqs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.sequences[idx]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Ejemplo de uso (los DataLoaders se crearán dentro de la función de Optuna)\n",
    "# ya que dependen del 'sequence_length' y 'batch_size'\n",
    "print(\"Clase TimeSeriesDataset definida.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351cd1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForecaster(nn.Module):\n",
    "    \"\"\"\n",
    "    Arquitectura del modelo LSTM.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, n_layers, dropout_prob):\n",
    "        super(LSTMForecaster, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True,  # Importante: (batch, seq, feature)\n",
    "            dropout=dropout_prob if n_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # LayerNorm aplicado a la salida de la LSTM\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, sequence_length, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        # Tomamos la salida del último paso de tiempo\n",
    "        last_time_step_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Aplicamos normalización y dropout\n",
    "        out = self.layer_norm(last_time_step_out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Capa final para la predicción\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out.squeeze(-1) # Devolvemos un tensor de (batch_size)\n",
    "\n",
    "print(\"Clase LSTMForecaster definida.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb24e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la función objetivo para Optuna (VERSIÓN CORREGIDA)\n",
    "def objective(trial: optuna.trial.Trial):\n",
    "    # --- 1. Definir espacio de búsqueda de hiperparámetros ---\n",
    "    params = {\n",
    "        'sequence_length': trial.suggest_int('sequence_length', 8, 20),\n",
    "        'hidden_size': trial.suggest_int('hidden_size', 32, 128, log=True),\n",
    "        'n_layers': trial.suggest_int('n_layers', 1, 3),\n",
    "        'dropout_prob': trial.suggest_float('dropout_prob', 0.1, 0.5),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [32, 64, 128]),\n",
    "    }\n",
    "\n",
    "    # --- 2. Crear Datasets y DataLoaders ---\n",
    "    # Este paso es idéntico, pero lo repetimos por claridad\n",
    "    try:\n",
    "        train_dataset = TimeSeriesDataset(\n",
    "            data=train_df, group_ids=train_df['id_bar'].unique(),\n",
    "            feature_cols=feature_cols, target_col=target_col,\n",
    "            sequence_length=params['sequence_length']\n",
    "        )\n",
    "        val_dataset = TimeSeriesDataset(\n",
    "            data=val_df, group_ids=val_df['id_bar'].unique(),\n",
    "            feature_cols=feature_cols, target_col=target_col,\n",
    "            sequence_length=params['sequence_length']\n",
    "        )\n",
    "        \n",
    "        # Si el dataset está vacío, la prueba no es válida\n",
    "        if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
    "            return float('inf')\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True, num_workers=2, drop_last=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False, num_workers=2)\n",
    "    except Exception as e:\n",
    "        # Si algo falla en la creación (p.ej. secuencias inválidas), la prueba no es válida\n",
    "        # print(f\"Skipping trial due to DataLoader error: {e}\") # Descomentar para depurar\n",
    "        return float('inf')\n",
    "\n",
    "\n",
    "    # --- 3. Instanciar modelo, optimizador y función de pérdida ---\n",
    "    input_size = len(feature_cols)\n",
    "    model = LSTMForecaster(\n",
    "        input_size=input_size,\n",
    "        hidden_size=params['hidden_size'],\n",
    "        n_layers=params['n_layers'],\n",
    "        dropout_prob=params['dropout_prob']\n",
    "    ).to(DEVICE)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=params['learning_rate'])\n",
    "    # Usamos MSE para optimizar (es más sensible a grandes errores), pero MAE para evaluar\n",
    "    loss_fn_train = nn.MSELoss()\n",
    "    loss_fn_val = nn.L1Loss() # MAE para validación\n",
    "\n",
    "    # --- 4. Bucle de Entrenamiento y Validación Manual ---\n",
    "    n_epochs = 30\n",
    "    patience = 5\n",
    "    best_val_mae = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        # Bucle de Entrenamiento\n",
    "        model.train()\n",
    "        for x_batch, y_batch in train_loader:\n",
    "            x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            preds = model(x_batch)\n",
    "            loss = loss_fn_train(preds, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Bucle de Validación\n",
    "        model.eval()\n",
    "        total_val_mae = 0\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "                preds = model(x_batch)\n",
    "                total_val_mae += loss_fn_val(preds, y_batch).item() * x_batch.size(0)\n",
    "        \n",
    "        # Calcular MAE promedio de validación\n",
    "        avg_val_mae = total_val_mae / len(val_dataset)\n",
    "\n",
    "        # --- Integración con Optuna (Pruning) y Early Stopping ---\n",
    "        # 1. Reportar la métrica a Optuna\n",
    "        trial.report(avg_val_mae, epoch)\n",
    "        \n",
    "        # 2. Verificar si la prueba debe ser podada\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "        # 3. Lógica de Early Stopping\n",
    "        if avg_val_mae < best_val_mae:\n",
    "            best_val_mae = avg_val_mae\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            break # Detener el entrenamiento si no hay mejora\n",
    "\n",
    "    return best_val_mae\n",
    "\n",
    "# --- Ejecutar el estudio de Optuna (sin cambios) ---\n",
    "print(\"Iniciando búsqueda de hiperparámetros con Optuna (versión corregida)...\")\n",
    "study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner())\n",
    "# Aumentar n_trials para una búsqueda más exhaustiva (ej. 50-100). Usamos 20 por tiempo.\n",
    "study.optimize(objective, n_trials=20, timeout=1800) # Timeout de 30 minutos\n",
    "\n",
    "print(\"\\nBúsqueda de hiperparámetros finalizada.\")\n",
    "print(f\"Mejor MAE de validación: {study.best_value:.4f}\")\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2d830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Obtener mejores parámetros ---\n",
    "best_params = study.best_params\n",
    "\n",
    "# --- Crear DataLoader con todos los datos de entrenamiento ---\n",
    "final_sequence_length = best_params['sequence_length']\n",
    "final_batch_size = best_params['batch_size']\n",
    "\n",
    "full_train_dataset = TimeSeriesDataset(\n",
    "    data=df_processed, # Usamos el dataframe procesado completo\n",
    "    group_ids=df_processed['id_bar'].unique(),\n",
    "    feature_cols=feature_cols,\n",
    "    target_col=target_col,\n",
    "    sequence_length=final_sequence_length\n",
    ")\n",
    "\n",
    "full_train_loader = DataLoader(full_train_dataset, batch_size=final_batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "# --- Instanciar y entrenar el modelo final ---\n",
    "final_model = LSTMForecaster(\n",
    "    input_size=len(feature_cols),\n",
    "    hidden_size=best_params['hidden_size'],\n",
    "    n_layers=best_params['n_layers'],\n",
    "    dropout_prob=best_params['dropout_prob']\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.AdamW(final_model.parameters(), lr=best_params['learning_rate'])\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "print(\"\\nEntrenando el modelo final sobre todos los datos...\")\n",
    "final_model.train()\n",
    "num_epochs_final = 25 # Número de épocas para el entrenamiento final\n",
    "\n",
    "for epoch in range(num_epochs_final):\n",
    "    total_loss = 0\n",
    "    for batch in full_train_loader:\n",
    "        x, y = batch\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        preds = final_model(x)\n",
    "        loss = loss_fn(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(full_train_loader)\n",
    "    print(f\"Época {epoch+1}/{num_epochs_final} - Loss: {avg_loss:.6f}\")\n",
    "\n",
    "print(\"Entrenamiento final completado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20c0c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.eval()\n",
    "predictions = []\n",
    "num_pred_weeks = 52 # Semanas a predecir para 2022\n",
    "\n",
    "all_barrios = df_processed['id_bar'].unique()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for bar_id in all_barrios:\n",
    "        # 1. Obtener la última secuencia conocida para el barrio\n",
    "        bar_data = df_processed[df_processed['id_bar'] == bar_id].tail(final_sequence_length)\n",
    "        \n",
    "        current_sequence_features = torch.tensor(bar_data[feature_cols].values, dtype=torch.float32).to(DEVICE)\n",
    "        \n",
    "        # Guardar las características exógenas de la última semana para llevarlas adelante\n",
    "        last_week_exog_features = current_sequence_features[-1, :].clone()\n",
    "        \n",
    "        # 2. Bucle de predicción autorregresiva\n",
    "        for week_num in range(1, num_pred_weeks + 1):\n",
    "            # Preparar la entrada para el modelo\n",
    "            input_tensor = current_sequence_features.unsqueeze(0) # (1, seq_len, num_features)\n",
    "            \n",
    "            # Predecir el siguiente valor (escalado)\n",
    "            scaled_pred = final_model(input_tensor).item() # Sale como un escalar\n",
    "            \n",
    "            # Invertir el escalado para obtener el valor real de 'dengue'\n",
    "            # Usamos reshape(-1, 1) ya que el scaler espera un array 2D\n",
    "            unscaled_pred_array = target_scaler.inverse_transform(np.array([[scaled_pred]]))\n",
    "            unscaled_pred = max(0, unscaled_pred_array[0, 0]) # Asegurar que los casos no sean negativos\n",
    "            \n",
    "            # Guardar la predicción\n",
    "            submission_id = f\"{bar_id}_2022_{week_num}\"\n",
    "            predictions.append({'id': submission_id, 'dengue': unscaled_pred, 'id_bar': bar_id, 'anio': 2022, 'semana': week_num})\n",
    "            \n",
    "            # 3. Construir la nueva característica para la semana predicha\n",
    "            new_feature_row = last_week_exog_features.clone()\n",
    "            \n",
    "            # El target 'dengue' no es una feature, así que no necesitamos actualizarlo en el vector de features\n",
    "            # Si el dengue de la semana anterior fuera una feature (lag), aquí se actualizaría.\n",
    "            # En nuestro caso, el vector de features exógenas se mantiene constante.\n",
    "            \n",
    "            # 4. Actualizar la secuencia: quitar el primer paso, añadir el nuevo\n",
    "            new_feature_row_expanded = new_feature_row.unsqueeze(0) # Shape: (1, num_features)\n",
    "            current_sequence_features = torch.cat((current_sequence_features[1:, :], new_feature_row_expanded), dim=0)\n",
    "\n",
    "print(\"Predicciones para 2022 generadas.\")\n",
    "\n",
    "# --- Crear y guardar el archivo de sumisión ---\n",
    "submission_df = pd.DataFrame(predictions)\n",
    "submission_df.to_csv(\"submission.csv\", index=False)\n",
    "print(\"\\nArchivo submission.csv creado exitosamente.\")\n",
    "print(submission_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fb368e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar dataframe para visualización\n",
    "pred_df = submission_df.copy()\n",
    "pred_df['date'] = pd.to_datetime('2022-' + pred_df['semana'].astype(str) + '-1', format='%Y-%W-%w')\n",
    "\n",
    "# Graficar para la misma muestra de barrios\n",
    "fig = make_subplots(\n",
    "    rows=len(sample_barrios), cols=1,\n",
    "    shared_xaxes=True,\n",
    "    subplot_titles=[f\"Barrio {b}\" for b in sample_barrios]\n",
    ")\n",
    "\n",
    "for i, bar_id in enumerate(sample_barrios):\n",
    "    # Datos históricos\n",
    "    hist_data = df_train[df_train['id_bar'] == bar_id]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=hist_data['date'], y=hist_data['dengue'],\n",
    "        mode='lines', name='Histórico', legendgroup='hist',\n",
    "        showlegend=(i==0), line=dict(color='blue')\n",
    "    ), row=i+1, col=1)\n",
    "\n",
    "    # Datos de predicción\n",
    "    pred_data = pred_df[pred_df['id_bar'] == bar_id]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pred_data['date'], y=pred_data['dengue'],\n",
    "        mode='lines', name='Predicción 2022', legendgroup='pred',\n",
    "        showlegend=(i==0), line=dict(color='red', dash='dash')\n",
    "    ), row=i+1, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=1000,\n",
    "    title_text=\"Comparación de Datos Históricos vs. Predicciones para 2022\",\n",
    "    xaxis_title='Fecha',\n",
    "    yaxis_title='Casos de Dengue'\n",
    ")\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
