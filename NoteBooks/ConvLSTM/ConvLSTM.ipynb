{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9addb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de visualización\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Cargar los datos\n",
    "df_train = pd.read_parquet('../../Datos/df_train.parquet')\n",
    "df_test = pd.read_parquet('../../Datos/df_test.parquet')\n",
    "sample_submission = pd.read_csv('../../Datos/sample_submission.csv')\n",
    "\n",
    "print(\"Forma de los datos:\")\n",
    "print(f\"Train: {df_train.shape}\")\n",
    "print(f\"Test: {df_test.shape}\")\n",
    "print(f\"Sample submission: {sample_submission.shape}\")\n",
    "\n",
    "# Información básica sobre los datasets\n",
    "print(\"\\nInformación del dataset de entrenamiento:\")\n",
    "print(df_train.info())\n",
    "print(\"\\nPrimeras filas del dataset de entrenamiento:\")\n",
    "print(df_train.head())\n",
    "\n",
    "# Estadísticas descriptivas\n",
    "print(\"\\nEstadísticas descriptivas - Variables numéricas:\")\n",
    "print(df_train.describe())\n",
    "\n",
    "# Análisis de valores faltantes\n",
    "print(\"\\nValores faltantes en train:\")\n",
    "print(df_train.isnull().sum())\n",
    "print(\"\\nValores faltantes en test:\")\n",
    "print(df_test.isnull().sum())\n",
    "\n",
    "# Crear columna de fecha para análisis temporal\n",
    "df_train['fecha'] = pd.to_datetime(df_train['anio'].astype(str) + '-W' + \n",
    "                                  df_train['semana'].astype(str).str.zfill(2) + '-1', \n",
    "                                  format='%Y-W%W-%w')\n",
    "df_test['fecha'] = pd.to_datetime(df_test['anio'].astype(str) + '-W' + \n",
    "                                 df_test['semana'].astype(str).str.zfill(2) + '-1', \n",
    "                                 format='%Y-W%W-%w')\n",
    "\n",
    "# Análisis por barrio\n",
    "print(f\"\\nNúmero de barrios únicos: {df_train['id_bar'].nunique()}\")\n",
    "print(f\"Rango de años en train: {df_train['anio'].min()} - {df_train['anio'].max()}\")\n",
    "print(f\"Rango de años en test: {df_test['anio'].min()} - {df_test['anio'].max()}\")\n",
    "\n",
    "# Visualización de series de tiempo por barrio\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Seleccionar 4 barrios aleatorios para visualización\n",
    "barrios_muestra = df_train['id_bar'].unique()[:4]\n",
    "\n",
    "for i, barrio in enumerate(barrios_muestra):\n",
    "    data_barrio = df_train[df_train['id_bar'] == barrio].sort_values('fecha')\n",
    "    axes[i].plot(data_barrio['fecha'], data_barrio['dengue'], linewidth=2)\n",
    "    axes[i].set_title(f'Casos de Dengue - Barrio {barrio}')\n",
    "    axes[i].set_xlabel('Fecha')\n",
    "    axes[i].set_ylabel('Casos de Dengue')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis de estacionalidad\n",
    "df_train['mes'] = df_train['fecha'].dt.month\n",
    "casos_por_mes = df_train.groupby('mes')['dengue'].agg(['mean', 'std'])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.errorbar(casos_por_mes.index, casos_por_mes['mean'], \n",
    "             yerr=casos_por_mes['std'], fmt='o-', capsize=5, capthick=2)\n",
    "plt.xlabel('Mes')\n",
    "plt.ylabel('Casos promedio de Dengue')\n",
    "plt.title('Estacionalidad de casos de Dengue')\n",
    "plt.xticks(range(1, 13))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Correlación entre variables\n",
    "variables_numericas = ['dengue', 'ESTRATO', 'area_barrio', 'concentraciones', \n",
    "                      'vivienda', 'equipesado', 'sumideros', 'maquina',\n",
    "                      'lluvia_mean', 'lluvia_var', 'lluvia_max', 'lluvia_min',\n",
    "                      'temperatura_mean', 'temperatura_var', 'temperatura_max', 'temperatura_min']\n",
    "\n",
    "correlation_matrix = df_train[variables_numericas].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, fmt='.2f')\n",
    "plt.title('Matriz de Correlación de Variables')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis de la variable objetivo\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(df_train['dengue'], bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('Casos de Dengue')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Distribución de casos de Dengue')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(np.log1p(df_train['dengue']), bins=50, edgecolor='black', alpha=0.7)\n",
    "plt.xlabel('log(1 + Casos de Dengue)')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Distribución logarítmica de casos de Dengue')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evolución temporal agregada\n",
    "casos_semanales = df_train.groupby('fecha')['dengue'].sum().reset_index()\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.plot(casos_semanales['fecha'], casos_semanales['dengue'], linewidth=2)\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Total de casos de Dengue')\n",
    "plt.title('Evolución temporal del total de casos de Dengue')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab0abdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import pickle\n",
    "\n",
    "# Configurar dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Preparación de datos para series de tiempo\n",
    "class DengueTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, sequence_length=12, target_length=1, is_test=False):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.target_length = target_length\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        # Ordenar por barrio y fecha\n",
    "        self.data = data.sort_values(['id_bar', 'fecha'])\n",
    "        \n",
    "        # Características a usar\n",
    "        self.feature_cols = ['ESTRATO', 'area_barrio', 'concentraciones', \n",
    "                            'vivienda', 'equipesado', 'sumideros', 'maquina',\n",
    "                            'lluvia_mean', 'lluvia_var', 'lluvia_max', 'lluvia_min',\n",
    "                            'temperatura_mean', 'temperatura_var', 'temperatura_max', \n",
    "                            'temperatura_min', 'semana', 'mes']\n",
    "        \n",
    "        self.target_col = 'dengue'\n",
    "        \n",
    "        # Escaladores\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.target_scaler = MinMaxScaler()\n",
    "        # Crear secuencias por barrio\n",
    "        self.sequences = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for barrio in self.data['id_bar'].unique():\n",
    "            barrio_data = self.data[self.data['id_bar'] == barrio].copy()\n",
    "            \n",
    "            if len(barrio_data) >= sequence_length + target_length:\n",
    "                # Escalar características\n",
    "                features = self.feature_scaler.fit_transform(barrio_data[self.feature_cols])\n",
    "                if not self.is_test:\n",
    "                    targets = self.target_scaler.fit_transform(barrio_data[[self.target_col]])\n",
    "                else:\n",
    "                    # Para datos de prueba, crear targets ficticios de ceros\n",
    "                    targets = np.zeros((len(barrio_data), 1))\n",
    "                targets = self.target_scaler.fit_transform(barrio_data[[self.target_col]])\n",
    "                \n",
    "                # Crear secuencias\n",
    "                for i in range(len(barrio_data) - sequence_length - target_length + 1):\n",
    "                    seq = features[i:i+sequence_length]\n",
    "                    target = targets[i+sequence_length:i+sequence_length+target_length]\n",
    "                    \n",
    "                    self.sequences.append(seq)\n",
    "                    self.targets.append(target)\n",
    "        \n",
    "        self.sequences = np.array(self.sequences)\n",
    "        self.targets = np.array(self.targets)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.FloatTensor(self.sequences[idx]), \n",
    "                torch.FloatTensor(self.targets[idx]))\n",
    "\n",
    "# Modelo ConvLSTM\n",
    "class ConvLSTMCell(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, kernel_size, bias=True):\n",
    "        super(ConvLSTMCell, self).__init__()\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = kernel_size // 2\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.conv = nn.Conv1d(in_channels=self.input_dim + self.hidden_dim,\n",
    "                              out_channels=4 * self.hidden_dim,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              padding=self.padding,\n",
    "                              bias=self.bias)\n",
    "        \n",
    "    def forward(self, input_tensor, cur_state):\n",
    "        h_cur, c_cur = cur_state\n",
    "        \n",
    "        combined = torch.cat([input_tensor, h_cur], dim=1)\n",
    "        combined_conv = self.conv(combined)\n",
    "        \n",
    "        cc_i, cc_f, cc_o, cc_g = torch.split(combined_conv, self.hidden_dim, dim=1)\n",
    "        i = torch.sigmoid(cc_i)\n",
    "        f = torch.sigmoid(cc_f)\n",
    "        o = torch.sigmoid(cc_o)\n",
    "        g = torch.tanh(cc_g)\n",
    "        \n",
    "        c_next = f * c_cur + i * g\n",
    "        h_next = o * torch.tanh(c_next)\n",
    "        \n",
    "        return h_next, c_next\n",
    "    \n",
    "    def init_hidden(self, batch_size, length):\n",
    "        return (torch.zeros(batch_size, self.hidden_dim, length, device=self.conv.weight.device),\n",
    "                torch.zeros(batch_size, self.hidden_dim, length, device=self.conv.weight.device))\n",
    "\n",
    "class ConvLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, kernel_sizes, num_layers, \n",
    "                 batch_first=True, bias=True, return_all_layers=False):\n",
    "        super(ConvLSTM, self).__init__()\n",
    "        \n",
    "        self._check_kernel_size_consistency(kernel_sizes)\n",
    "        \n",
    "        kernel_sizes = self._extend_for_multilayer(kernel_sizes, num_layers)\n",
    "        hidden_dims = self._extend_for_multilayer(hidden_dims, num_layers)\n",
    "        \n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dims = hidden_dims\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_first = batch_first\n",
    "        self.bias = bias\n",
    "        self.return_all_layers = return_all_layers\n",
    "        \n",
    "        cell_list = []\n",
    "        for i in range(0, self.num_layers):\n",
    "            cur_input_dim = self.input_dim if i == 0 else self.hidden_dims[i - 1]\n",
    "            \n",
    "            cell_list.append(ConvLSTMCell(input_dim=cur_input_dim,\n",
    "                                         hidden_dim=self.hidden_dims[i],\n",
    "                                         kernel_size=self.kernel_sizes[i],\n",
    "                                         bias=self.bias))\n",
    "            \n",
    "        self.cell_list = nn.ModuleList(cell_list)\n",
    "        \n",
    "    def forward(self, input_tensor, hidden_state=None):\n",
    "        if not self.batch_first:\n",
    "            input_tensor = input_tensor.permute(1, 0, 2, 3)\n",
    "            \n",
    "        batch_size, seq_len, channels, length = input_tensor.size()\n",
    "        \n",
    "        if hidden_state is None:\n",
    "            hidden_state = self._init_hidden(batch_size, length)\n",
    "            \n",
    "        layer_output_list = []\n",
    "        last_state_list = []\n",
    "        \n",
    "        cur_layer_input = input_tensor\n",
    "        \n",
    "        for layer_idx in range(self.num_layers):\n",
    "            h, c = hidden_state[layer_idx]\n",
    "            output_inner = []\n",
    "            \n",
    "            for t in range(seq_len):\n",
    "                h, c = self.cell_list[layer_idx](input_tensor=cur_layer_input[:, t, :, :],\n",
    "                                                 cur_state=[h, c])\n",
    "                output_inner.append(h)\n",
    "                \n",
    "            layer_output = torch.stack(output_inner, dim=1)\n",
    "            cur_layer_input = layer_output\n",
    "            \n",
    "            layer_output_list.append(layer_output)\n",
    "            last_state_list.append([h, c])\n",
    "            \n",
    "        if self.return_all_layers:\n",
    "            return layer_output_list, last_state_list\n",
    "        else:\n",
    "            return layer_output_list[-1], last_state_list\n",
    "    \n",
    "    def _init_hidden(self, batch_size, length):\n",
    "        init_states = []\n",
    "        for i in range(self.num_layers):\n",
    "            init_states.append(self.cell_list[i].init_hidden(batch_size, length))\n",
    "        return init_states\n",
    "    \n",
    "    @staticmethod\n",
    "    def _check_kernel_size_consistency(kernel_sizes):\n",
    "        if not (isinstance(kernel_sizes, tuple) or\n",
    "                (isinstance(kernel_sizes, list) and all([isinstance(elem, tuple) for elem in kernel_sizes]))):\n",
    "            raise ValueError('`kernel_sizes` must be tuple or list of tuples')\n",
    "            \n",
    "    @staticmethod\n",
    "    def _extend_for_multilayer(param, num_layers):\n",
    "        if not isinstance(param, list):\n",
    "            param = [param] * num_layers\n",
    "        return param\n",
    "\n",
    "# Modelo completo\n",
    "class DenguePredictor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, kernel_sizes, num_layers, dropout=0.2):\n",
    "        super(DenguePredictor, self).__init__()\n",
    "        \n",
    "        # Reshape input para ConvLSTM (añadir dimensión espacial)\n",
    "        self.input_reshape = nn.Unflatten(2, (input_dim, 1))\n",
    "        \n",
    "        # ConvLSTM\n",
    "        self.convlstm = ConvLSTM(input_dim, hidden_dims, kernel_sizes, \n",
    "                                 num_layers, batch_first=True)\n",
    "        \n",
    "        # Capas finales\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(hidden_dims[-1], 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, features)\n",
    "        batch_size, seq_len, features = x.shape\n",
    "        \n",
    "        # Reshape para ConvLSTM: (batch, seq_len, channels, length)\n",
    "        x = x.unsqueeze(-1)  # (batch, seq_len, features, 1)\n",
    "        \n",
    "        # ConvLSTM\n",
    "        lstm_out, _ = self.convlstm(x)\n",
    "        \n",
    "        # Tomar la última salida temporal\n",
    "        lstm_out = lstm_out[:, -1, :, :]  # (batch, hidden_dim, 1)\n",
    "        lstm_out = lstm_out.squeeze(-1)    # (batch, hidden_dim)\n",
    "        \n",
    "        # Capas densas\n",
    "        x = self.dropout(lstm_out)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.activation(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Función objetivo para Optuna\n",
    "def objective(trial):\n",
    "    # Hiperparámetros a optimizar\n",
    "    hidden_dim1 = trial.suggest_int('hidden_dim1', 32, 128, step=32)\n",
    "    hidden_dim2 = trial.suggest_int('hidden_dim2', 16, 64, step=16)\n",
    "    kernel_size = trial.suggest_int('kernel_size', 3, 7, step=2)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    sequence_length = trial.suggest_int('sequence_length', 8, 24, step=4)\n",
    "    \n",
    "    # Crear dataset con los hiperparámetros\n",
    "    train_dataset = DengueTimeSeriesDataset(df_train, sequence_length=sequence_length, is_test=False)\n",
    "    val_dataset = DengueTimeSeriesDataset(df_test, sequence_length=sequence_length, is_test=True)\n",
    "    \n",
    "    # Crear dataset con los hiperparámetros\n",
    "    train_dataset = DengueTimeSeriesDataset(df_train, sequence_length=sequence_length)\n",
    "    val_dataset = DengueTimeSeriesDataset(df_test, sequence_length=sequence_length)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Crear modelo\n",
    "    model = DenguePredictor(\n",
    "        input_dim=len(train_dataset.feature_cols),\n",
    "        hidden_dims=[hidden_dim1, hidden_dim2][:num_layers],\n",
    "        kernel_sizes=(kernel_size,),\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    ).to(device)\n",
    "    \n",
    "    # Configurar entrenamiento\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5)\n",
    "    \n",
    "    # Entrenar\n",
    "    n_epochs = 50\n",
    "    early_stopping_patience = 10\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = criterion(y_pred.squeeze(), y_batch.squeeze())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                y_pred = model(X_batch)\n",
    "                loss = criterion(y_pred.squeeze(), y_batch.squeeze())\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= early_stopping_patience:\n",
    "            break\n",
    "            \n",
    "        # Reportar a Optuna\n",
    "        trial.report(val_loss, epoch)\n",
    "        \n",
    "        # Pruning\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "# Añadir columna 'mes' a df_test\n",
    "df_test['mes'] = df_test['fecha'].dt.month\n",
    "\n",
    "# Ejecutar optimización\n",
    "print(\"Iniciando optimización de hiperparámetros con Optuna...\")\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=30, timeout=3600)  # 1 hora máximo\n",
    "\n",
    "print(\"Mejores hiperparámetros encontrados:\")\n",
    "print(study.best_params)\n",
    "print(f\"Mejor valor de pérdida: {study.best_value}\")\n",
    "\n",
    "# Guardar el estudio\n",
    "with open('optuna_study.pkl', 'wb') as f:\n",
    "    pickle.dump(study, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54259e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar todos los datos para el entrenamiento final\n",
    "df_all = pd.concat([df_train, df_test], ignore_index=True)\n",
    "df_all = df_all.sort_values(['id_bar', 'fecha'])\n",
    "\n",
    "# Usar los mejores hiperparámetros\n",
    "best_params = study.best_params\n",
    "print(\"Entrenando modelo final con los mejores hiperparámetros...\")\n",
    "\n",
    "# Crear dataset con todos los datos\n",
    "final_dataset = DengueTimeSeriesDataset(\n",
    "    df_all, \n",
    "    sequence_length=best_params['sequence_length']\n",
    ")\n",
    "\n",
    "final_loader = DataLoader(\n",
    "    final_dataset, \n",
    "    batch_size=best_params['batch_size'], \n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Crear modelo final\n",
    "final_model = DenguePredictor(\n",
    "    input_dim=len(final_dataset.feature_cols),\n",
    "    hidden_dims=[best_params['hidden_dim1'], \n",
    "                 best_params.get('hidden_dim2', best_params['hidden_dim1']//2)][:best_params['num_layers']],\n",
    "    kernel_sizes=(best_params['kernel_size'],),\n",
    "    num_layers=best_params['num_layers'],\n",
    "    dropout=best_params['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Configurar entrenamiento\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)\n",
    "\n",
    "# Entrenar modelo final\n",
    "n_epochs = 100\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    final_model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for X_batch, y_batch in final_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = final_model(X_batch)\n",
    "        loss = criterion(y_pred.squeeze(), y_batch.squeeze())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(final_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "    scheduler.step(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{n_epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# Guardar modelo final\n",
    "torch.save({\n",
    "    'model_state_dict': final_model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'best_params': best_params,\n",
    "    'feature_scaler': final_dataset.feature_scaler,\n",
    "    'target_scaler': final_dataset.target_scaler,\n",
    "    'feature_cols': final_dataset.feature_cols\n",
    "}, 'final_dengue_model.pth')\n",
    "\n",
    "# Visualizar pérdida de entrenamiento\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses)\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida MSE')\n",
    "plt.title('Pérdida de entrenamiento del modelo final')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bceb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para generar predicciones\n",
    "def generate_predictions_2022(model, df_historical, scalers, sequence_length, feature_cols):\n",
    "    predictions = []\n",
    "    ids = []\n",
    "    \n",
    "    # Procesar cada barrio\n",
    "    for barrio in df_historical['id_bar'].unique():\n",
    "        barrio_data = df_historical[df_historical['id_bar'] == barrio].copy()\n",
    "        barrio_data = barrio_data.sort_values('fecha')\n",
    "        \n",
    "        # Tomar las últimas sequence_length semanas como contexto inicial\n",
    "        if len(barrio_data) >= sequence_length:\n",
    "            # Preparar features\n",
    "            context_data = barrio_data.tail(sequence_length).copy()\n",
    "            \n",
    "            # Predecir para cada semana del 2022\n",
    "            for week in range(1, 53):  # 52 semanas\n",
    "                # Crear features para la predicción\n",
    "                features = context_data[feature_cols].values\n",
    "                features_scaled = scalers['feature_scaler'].transform(features)\n",
    "                \n",
    "                # Convertir a tensor\n",
    "                X = torch.FloatTensor(features_scaled).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Predecir\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    y_pred_scaled = model(X).cpu().numpy()\n",
    "                \n",
    "                # Desescalar predicción\n",
    "                y_pred = scalers['target_scaler'].inverse_transform(y_pred_scaled.reshape(-1, 1))\n",
    "                y_pred = max(0, y_pred[0, 0])  # Asegurar no negativos\n",
    "                \n",
    "                # Guardar predicción\n",
    "                id_str = f\"{barrio}_2022_{week:02d}\"\n",
    "                ids.append(id_str)\n",
    "                predictions.append(y_pred)\n",
    "                \n",
    "                # Actualizar contexto para la siguiente predicción\n",
    "                # Crear nueva fila con la predicción\n",
    "                new_row = context_data.iloc[-1:].copy()\n",
    "                new_row['dengue'] = y_pred\n",
    "                new_row['semana'] = week\n",
    "                new_row['fecha'] = pd.to_datetime(f'2022-W{week:02d}-1', format='%Y-W%W-%w')\n",
    "                new_row['mes'] = new_row['fecha'].dt.month\n",
    "                \n",
    "                # Actualizar contexto deslizante\n",
    "                context_data = pd.concat([context_data.iloc[1:], new_row], ignore_index=True)\n",
    "    \n",
    "    return ids, predictions\n",
    "\n",
    "# Cargar modelo y escaladores\n",
    "checkpoint = torch.load('final_dengue_model.pth')\n",
    "final_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "final_model.eval()\n",
    "\n",
    "scalers = {\n",
    "    'feature_scaler': checkpoint['feature_scaler'],\n",
    "    'target_scaler': checkpoint['target_scaler']\n",
    "}\n",
    "\n",
    "# Generar predicciones\n",
    "print(\"Generando predicciones para 2022...\")\n",
    "pred_ids, pred_values = generate_predictions_2022(\n",
    "    final_model, \n",
    "    df_all, \n",
    "    scalers,\n",
    "    best_params['sequence_length'],\n",
    "    checkpoint['feature_cols']\n",
    ")\n",
    "\n",
    "# Crear dataframe de submission\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': pred_ids,\n",
    "    'dengue': pred_values\n",
    "})\n",
    "\n",
    "# Verificar que coincida con el formato esperado\n",
    "print(f\"Predicciones generadas: {len(submission_df)}\")\n",
    "print(f\"Formato esperado: {len(sample_submission)}\")\n",
    "\n",
    "# Asegurar que el orden coincida con sample_submission\n",
    "submission_df = submission_df.set_index('id').loc[sample_submission['id']].reset_index()\n",
    "\n",
    "# Guardar submission\n",
    "submission_df.to_csv('submission_convlstm.csv', index=False)\n",
    "print(\"Archivo de submission guardado como 'submission_convlstm.csv'\")\n",
    "\n",
    "# Visualizar algunas predicciones\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "barrios_muestra = df_all['id_bar'].unique()[:4]\n",
    "\n",
    "for i, barrio in enumerate(barrios_muestra):\n",
    "    # Datos históricos\n",
    "    hist_data = df_all[df_all['id_bar'] == barrio].sort_values('fecha')\n",
    "    \n",
    "    # Predicciones 2022\n",
    "    pred_data = submission_df[submission_df['id'].str.startswith(f\"{barrio}_2022\")]\n",
    "    pred_data['fecha'] = pd.to_datetime('2022') + pd.to_timedelta(\n",
    "        pred_data['id'].str.extract(r'_(\\d+)$')[0].astype(int) * 7, unit='D'\n",
    "    )\n",
    "    \n",
    "    # Graficar\n",
    "    axes[i].plot(hist_data['fecha'], hist_data['dengue'], \n",
    "                 label='Histórico', linewidth=2)\n",
    "    axes[i].plot(pred_data['fecha'], pred_data['dengue'], \n",
    "                 label='Predicción 2022', linewidth=2, linestyle='--', color='red')\n",
    "    axes[i].set_title(f'Barrio {barrio}')\n",
    "    axes[i].set_xlabel('Fecha')\n",
    "    axes[i].set_ylabel('Casos de Dengue')\n",
    "    axes[i].legend()\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estadísticas de las predicciones\n",
    "print(\"\\nEstadísticas de las predicciones:\")\n",
    "print(submission_df['dengue'].describe())\n",
    "\n",
    "# Comparar con las estadísticas históricas\n",
    "print(\"\\nEstadísticas históricas (train):\")\n",
    "print(df_train['dengue'].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
