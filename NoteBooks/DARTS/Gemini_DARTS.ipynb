{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe75d87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Librerías importadas correctamente.\n",
      "\n",
      "--- Iniciando Sección 1: Análisis Exploratorio de Datos ---\n",
      "Error: No se encontraron los archivos CSV. Por favor, verifica la ruta en la variable 'path'.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     56\u001b[39m     df = df.set_index(\u001b[33m'\u001b[39m\u001b[33mfecha\u001b[39m\u001b[33m'\u001b[39m).resample(\u001b[33m'\u001b[39m\u001b[33mW-MON\u001b[39m\u001b[33m'\u001b[39m).sum().reset_index()\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m df_train = preprocess_data(\u001b[43mdf_train\u001b[49m)\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPreprocesamiento inicial completado.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPrimeras 5 filas del set de entrenamiento:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'df_train' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# SECCIÓN 0: IMPORTACIÓN DE LIBRERÍAS Y CONFIGURACIÓN INICIAL\n",
    "# ==============================================================================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models import TCNModel, RNNModel\n",
    "from darts.metrics import rmse\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "\n",
    "# Ignorar advertencias para una salida más limpia\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración de Matplotlib para los gráficos\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "print(\"Librerías importadas correctamente.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# SECCIÓN 1: ANÁLISIS EXPLORATORIO DE DATOS (EDA) Y PREPARACIÓN\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Iniciando Sección 1: Análisis Exploratorio de Datos ---\")\n",
    "\n",
    "# --- 1.1 Carga de Datos ---\n",
    "# NOTA: Asegúrate de que la ruta a tus archivos sea la correcta.\n",
    "# El usuario especificó \"../../Datos/\", ajusta la variable 'path' si es necesario.\n",
    "try:\n",
    "    path = \"./\" # Se asume que los archivos están en el mismo directorio del notebook.\n",
    "                 # Cambia a \"../../Datos/\" si esa es tu estructura de carpetas.\n",
    "    df_train = pd.read_csv(f'{path}df_train.csv')\n",
    "    df_test = pd.read_csv(f'{path}df_test.csv')\n",
    "    sample_submission = pd.read_csv(f'{path}sample_submission.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: No se encontraron los archivos CSV. Por favor, verifica la ruta en la variable 'path'.\")\n",
    "    # Salir si los archivos no se encuentran\n",
    "    exit()\n",
    "\n",
    "\n",
    "# --- 1.2 Preprocesamiento y Limpieza de Datos ---\n",
    "# Función para preprocesar los dataframes\n",
    "def preprocess_data(df):\n",
    "    # Asumiendo que las columnas se llaman 'FECHA' y 'UNIDADES_VENDIDAS'\n",
    "    # Renombrar columnas para estandarizar\n",
    "    df = df.rename(columns={\n",
    "        'FECHA': 'fecha',\n",
    "        'UNIDADES_VENDIDAS': 'unidades_vendidas'\n",
    "    })\n",
    "    # Convertir la columna 'fecha' a formato datetime\n",
    "    df['fecha'] = pd.to_datetime(df['fecha'])\n",
    "    # Agrupar por semana para asegurar una frecuencia constante\n",
    "    # Se suma las unidades vendidas por semana\n",
    "    df = df.set_index('fecha').resample('W-MON').sum().reset_index()\n",
    "    return df\n",
    "\n",
    "df_train = preprocess_data(df_train)\n",
    "\n",
    "print(\"Preprocesamiento inicial completado.\")\n",
    "print(\"Primeras 5 filas del set de entrenamiento:\")\n",
    "print(df_train.head())\n",
    "print(\"\\nÚltimas 5 filas del set de entrenamiento:\")\n",
    "print(df_train.tail())\n",
    "\n",
    "# --- 1.3 Creación de la Serie de Tiempo con Darts ---\n",
    "# Darts requiere un objeto TimeSeries. Usaremos 'fecha' como el índice de tiempo\n",
    "# y 'unidades_vendidas' como el valor.\n",
    "full_series = TimeSeries.from_dataframe(\n",
    "    df_train,\n",
    "    time_col='fecha',\n",
    "    value_cols='unidades_vendidas',\n",
    "    freq='W-MON' # Frecuencia semanal, comenzando en lunes\n",
    ")\n",
    "\n",
    "# --- 1.4 Visualización de la Serie de Tiempo Completa ---\n",
    "plt.figure(figsize=(14, 7))\n",
    "full_series.plot(label='Ventas históricas')\n",
    "plt.title('Serie de Tiempo Completa de Unidades Vendidas')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Unidades Vendidas')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- 1.5 División en Sets de Entrenamiento y Validación ---\n",
    "# Entrenamiento: datos con año < 2021\n",
    "# Validación: datos con año >= 2021\n",
    "train_series, val_series = full_series.split_before(pd.Timestamp('20210101'))\n",
    "\n",
    "print(f\"\\nFechas de entrenamiento: de {train_series.start_time()} a {train_series.end_time()}\")\n",
    "print(f\"Fechas de validación: de {val_series.start_time()} a {val_series.end_time()}\")\n",
    "\n",
    "# Visualización de la división\n",
    "plt.figure(figsize=(14, 7))\n",
    "train_series.plot(label='Entrenamiento (train)')\n",
    "val_series.plot(label='Validación (validation)')\n",
    "plt.title('División en Datos de Entrenamiento y Validación')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Unidades Vendidas')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- 1.6 Normalización de los Datos ---\n",
    "# Escalar los datos es una buena práctica para redes neuronales.\n",
    "# Se ajusta el escalador SÓLO con los datos de entrenamiento para evitar fuga de datos.\n",
    "scaler = Scaler()\n",
    "train_series_scaled = scaler.fit_transform(train_series)\n",
    "val_series_scaled = scaler.transform(val_series)\n",
    "full_series_scaled = scaler.transform(full_series)\n",
    "\n",
    "print(\"Datos normalizados para el entrenamiento de modelos.\")\n",
    "print(\"--- Fin Sección 1 ---\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECCIÓN 2: IMPLEMENTACIÓN Y COMPARACIÓN DE MODELOS (TCN, LSTM, GRU)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Iniciando Sección 2: Implementación y Selección de Modelos ---\")\n",
    "\n",
    "# --- 2.1 Definición de Parámetros Comunes para los Modelos ---\n",
    "# Estos parámetros se eligen de forma empírica. Para un proyecto real, se deberían optimizar.\n",
    "INPUT_CHUNK_LENGTH = 52  # Usar las últimas 52 semanas (1 año) para predecir\n",
    "OUTPUT_CHUNK_LENGTH = 12 # Predecir 12 semanas hacia adelante\n",
    "N_EPOCHS = 100           # Número de épocas para entrenar\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Parámetros para el optimizador de Pytorch\n",
    "torch_optimizer_kwargs = {\n",
    "    \"lr\": 1e-3, # Tasa de aprendizaje\n",
    "}\n",
    "\n",
    "# Configuración para la reproducibilidad\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# --- 2.2 Creación y Entrenamiento de Modelos ---\n",
    "# Se crea un diccionario para almacenar los modelos y sus pronósticos\n",
    "models = {}\n",
    "predictions = {}\n",
    "losses = {}\n",
    "\n",
    "# Modelo 1: Temporal Convolutional Network (TCN)\n",
    "print(\"\\nEntrenando modelo TCN...\")\n",
    "model_tcn = TCNModel(\n",
    "    input_chunk_length=INPUT_CHUNK_LENGTH,\n",
    "    output_chunk_length=OUTPUT_CHUNK_LENGTH,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    dropout=0.1,\n",
    "    dilation_base=2,\n",
    "    weight_norm=True,\n",
    "    kernel_size=5,\n",
    "    num_filters=3,\n",
    "    random_state=42,\n",
    "    pl_trainer_kwargs={\"accelerator\": \"auto\"} # Usa GPU si está disponible\n",
    ")\n",
    "model_tcn.fit(train_series_scaled, verbose=False)\n",
    "pred_tcn_scaled = model_tcn.predict(n=len(val_series))\n",
    "pred_tcn = scaler.inverse_transform(pred_tcn_scaled) # Re-escalar a valores originales\n",
    "losses['TCN'] = rmse(val_series, pred_tcn)\n",
    "models['TCN'] = model_tcn\n",
    "predictions['TCN'] = pred_tcn\n",
    "\n",
    "# Modelo 2: Long Short-Term Memory (LSTM)\n",
    "print(\"Entrenando modelo LSTM...\")\n",
    "model_lstm = RNNModel(\n",
    "    model='LSTM',\n",
    "    input_chunk_length=INPUT_CHUNK_LENGTH,\n",
    "    output_chunk_length=OUTPUT_CHUNK_LENGTH,\n",
    "    hidden_dim=25,\n",
    "    n_rnn_layers=2,\n",
    "    dropout=0.1,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    optimizer_kwargs=torch_optimizer_kwargs,\n",
    "    random_state=42,\n",
    "    pl_trainer_kwargs={\"accelerator\": \"auto\"}\n",
    ")\n",
    "model_lstm.fit(train_series_scaled, verbose=False)\n",
    "pred_lstm_scaled = model_lstm.predict(n=len(val_series))\n",
    "pred_lstm = scaler.inverse_transform(pred_lstm_scaled)\n",
    "losses['LSTM'] = rmse(val_series, pred_lstm)\n",
    "models['LSTM'] = model_lstm\n",
    "predictions['LSTM'] = pred_lstm\n",
    "\n",
    "# Modelo 3: Gated Recurrent Unit (GRU)\n",
    "print(\"Entrenando modelo GRU...\")\n",
    "model_gru = RNNModel(\n",
    "    model='GRU',\n",
    "    input_chunk_length=INPUT_CHUNK_LENGTH,\n",
    "    output_chunk_length=OUTPUT_CHUNK_LENGTH,\n",
    "    hidden_dim=25,\n",
    "    n_rnn_layers=2,\n",
    "    dropout=0.1,\n",
    "    n_epochs=N_EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    optimizer_kwargs=torch_optimizer_kwargs,\n",
    "    random_state=42,\n",
    "    pl_trainer_kwargs={\"accelerator\": \"auto\"}\n",
    ")\n",
    "model_gru.fit(train_series_scaled, verbose=False)\n",
    "pred_gru_scaled = model_gru.predict(n=len(val_series))\n",
    "pred_gru = scaler.inverse_transform(pred_gru_scaled)\n",
    "losses['GRU'] = rmse(val_series, pred_gru)\n",
    "models['GRU'] = model_gru\n",
    "predictions['GRU'] = pred_gru\n",
    "\n",
    "\n",
    "# --- 2.3 Comparación de Resultados ---\n",
    "print(\"\\n--- Comparación de Modelos en el Set de Validación ---\")\n",
    "for model_name, loss in losses.items():\n",
    "    print(f\"Loss (RMSE) del modelo {model_name}: {loss:.4f}\")\n",
    "\n",
    "# Encontrar el mejor modelo\n",
    "best_model_name = min(losses, key=losses.get)\n",
    "best_model_loss = losses[best_model_name]\n",
    "print(f\"\\nEl mejor modelo es '{best_model_name}' con un RMSE de {best_model_loss:.4f}\")\n",
    "\n",
    "# --- 2.4 Visualización de los Pronósticos de Validación ---\n",
    "plt.figure(figsize=(14, 8))\n",
    "val_series.plot(label='Valores Reales (Validación)', lw=3)\n",
    "\n",
    "for model_name, pred in predictions.items():\n",
    "    pred.plot(label=f'Pronóstico {model_name}')\n",
    "\n",
    "plt.title('Comparación de Pronósticos en el Set de Validación')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Unidades Vendidas')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(\"--- Fin Sección 2 ---\")\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# SECCIÓN 3: ENTRENAMIENTO FINAL Y GENERACIÓN DE PRONÓSTICOS PARA 2022\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Iniciando Sección 3: Entrenamiento Final y Pronóstico ---\")\n",
    "\n",
    "# --- 3.1 Preparación del Dataset Completo ---\n",
    "# Combinar df_train y df_test\n",
    "df_test = preprocess_data(df_test)\n",
    "df_full = pd.concat([df_train, df_test], ignore_index=True)\n",
    "\n",
    "print(\"Datos de entrenamiento y prueba combinados.\")\n",
    "print(f\"Total de registros para el entrenamiento final: {len(df_full)}\")\n",
    "\n",
    "# Crear la serie de tiempo completa para el re-entrenamiento\n",
    "final_series = TimeSeries.from_dataframe(\n",
    "    df_full,\n",
    "    time_col='fecha',\n",
    "    value_cols='unidades_vendidas',\n",
    "    freq='W-MON'\n",
    ")\n",
    "\n",
    "# Normalizar la serie de tiempo completa\n",
    "final_scaler = Scaler()\n",
    "final_series_scaled = final_scaler.fit_transform(final_series)\n",
    "\n",
    "\n",
    "# --- 3.2 Re-entrenamiento del Mejor Modelo ---\n",
    "print(f\"\\nRe-entrenando el mejor modelo ('{best_model_name}') con todos los datos...\")\n",
    "\n",
    "# Obtener la arquitectura del mejor modelo\n",
    "best_model_architecture = models[best_model_name]\n",
    "\n",
    "# Entrenar el modelo en el dataset completo\n",
    "best_model_architecture.fit(final_series_scaled, verbose=False)\n",
    "\n",
    "print(\"Re-entrenamiento completado.\")\n",
    "\n",
    "# --- 3.3 Generación del Pronóstico para 2022 ---\n",
    "# Determinar el número de semanas a predecir en 2022\n",
    "# El archivo sample_submission indica las semanas que necesitamos\n",
    "# Vamos a calcular cuántas semanas hay desde el final de nuestros datos hasta fin de 2022.\n",
    "last_date_in_data = final_series.end_time()\n",
    "forecast_end_date = pd.Timestamp('2022-12-31')\n",
    "# Darts predice el número de pasos (semanas en este caso)\n",
    "n_forecast_steps = (forecast_end_date.to_period('W-MON') - last_date_in_data.to_period('W-MON')).n\n",
    "\n",
    "print(f\"Última fecha en los datos: {last_date_in_data.strftime('%Y-%m-%d')}\")\n",
    "print(f\"Se pronosticarán {n_forecast_steps} semanas para cubrir hasta finales de 2022.\")\n",
    "\n",
    "# Realizar el pronóstico\n",
    "final_forecast_scaled = best_model_architecture.predict(n=n_forecast_steps)\n",
    "\n",
    "# Re-escalar el pronóstico a los valores originales\n",
    "final_forecast = final_scaler.inverse_transform(final_forecast_scaled)\n",
    "\n",
    "# --- 3.4 Visualización del Pronóstico Final ---\n",
    "plt.figure(figsize=(14, 7))\n",
    "final_series.plot(label='Datos Históricos (Train+Test)')\n",
    "final_forecast.plot(label='Pronóstico para 2022')\n",
    "plt.title('Pronóstico Final de Unidades Vendidas para 2022')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Unidades Vendidas')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 3.5 Creación del Archivo de Submission ---\n",
    "print(\"\\nCreando el archivo de submission...\")\n",
    "\n",
    "# Crear un DataFrame con las predicciones\n",
    "df_forecast = final_forecast.pd_dataframe().reset_index()\n",
    "df_forecast.columns = ['fecha', 'unidades_vendidas']\n",
    "\n",
    "# Filtrar las predicciones para que coincidan con las fechas de sample_submission\n",
    "# que son para el año 2022\n",
    "df_forecast = df_forecast[df_forecast['fecha'].dt.year == 2022]\n",
    "\n",
    "# Asegurarse de que el formato de 'unidades_vendidas' sea entero y no negativo\n",
    "df_forecast['unidades_vendidas'] = df_forecast['unidades_vendidas'].apply(lambda x: max(0, round(x))).astype(int)\n",
    "\n",
    "# Preparar el archivo de submission final\n",
    "# sample_submission tiene columnas 'id' y 'unidades_vendidas'\n",
    "# Suponemos que 'id' se corresponde con las fechas de la semana en 2022\n",
    "submission = sample_submission.copy()\n",
    "submission['fecha'] = pd.to_datetime(submission['id']) # Asumimos que 'id' es una fecha\n",
    "\n",
    "# Unir con nuestras predicciones\n",
    "submission = pd.merge(submission[['id', 'fecha']], df_forecast, on='fecha', how='left')\n",
    "submission = submission.drop(columns=['fecha'])\n",
    "\n",
    "# Llenar posibles valores faltantes (si el pronóstico no cubrió alguna semana)\n",
    "submission['unidades_vendidas'] = submission['unidades_vendidas'].fillna(0).astype(int)\n",
    "\n",
    "# Guardar el archivo final\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(\"Archivo 'submission.csv' generado exitosamente.\")\n",
    "print(\"Primeras 5 filas del archivo de submission:\")\n",
    "print(submission.head())\n",
    "\n",
    "print(\"\\n--- Proceso Finalizado ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
