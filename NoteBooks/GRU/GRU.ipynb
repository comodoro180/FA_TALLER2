{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0c70f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x174da297050>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import optuna\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Fijar semillas para reproducibilidad\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ffa6800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_parquet('../../Datos/df_train.parquet')\n",
    "df_test  = pd.read_parquet('../../Datos/df_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ece0b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear columna semana con dos dígitos\n",
    "for df in [df_train, df_test]:\n",
    "    df['semana_str'] = df['semana'].astype(str).str.zfill(2)\n",
    "    df['date'] = pd.to_datetime(\n",
    "        df['anio'].astype(str) + df['semana_str'] + '1',\n",
    "        format='%G%V%u'  # %G: año ISO, %V: semana ISO, %u: día de la semana (1=Lun)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104fa2e6",
   "metadata": {},
   "source": [
    "La clase **DengueDataset** transforma eficazmente los datos crudos de casos de dengue, agrupados por ubicación, en un formato adecuado para modelos de predicción de secuencias. Crea secuencias superpuestas de casos de dengue pasados (x) para predecir el siguiente caso de dengue (y), asegurando que los datos estén correctamente estructurados y tipados para su uso con modelos de PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b29c2008",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # Importa la librería PyTorch, fundamental para tensores y redes neuronales\n",
    "from torch.utils.data import Dataset # Importa la clase base Dataset de PyTorch, que necesitamos para crear nuestro dataset personalizado\n",
    "\n",
    "class DengueDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Clase personalizada para manejar los datos de dengue, preparándolos para modelos de series de tiempo en PyTorch.\n",
    "    Hereda de torch.utils.data.Dataset, lo que permite que PyTorch's DataLoader la use fácilmente.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, seq_len):\n",
    "        \"\"\"\n",
    "        Constructor de la clase DengueDataset.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame de Pandas con los datos de dengue.\n",
    "                              Debe contener al menos las columnas 'id_bar', 'date' y 'dengue'.\n",
    "            seq_len (int): Longitud de la secuencia de entrada (número de pasos de tiempo pasados\n",
    "                           a usar para la predicción).\n",
    "        \"\"\"\n",
    "        self.seq_len = seq_len # Almacena la longitud de la secuencia para uso posterior\n",
    "        self.samples = []      # Lista donde almacenaremos los pares (secuencia de entrada, valor objetivo)\n",
    "\n",
    "        # Agrupar por 'id_bar' (identificador de barrio/área) para procesar cada serie de tiempo individualmente.\n",
    "        # Esto es crucial porque cada barrio es una serie de tiempo independiente.\n",
    "        for bar, grp in df.groupby('id_bar'):\n",
    "            # Ordenar los valores de dengue por fecha para asegurar el orden cronológico.\n",
    "            # Convertimos la columna 'dengue' a un array de NumPy para facilitar el slicing.\n",
    "            vals = grp.sort_values('date')['dengue'].values\n",
    "\n",
    "            # Solo creamos muestras si la serie de tiempo es lo suficientemente larga.\n",
    "            # Necesitamos al menos 'seq_len' valores para la entrada (x) y 1 valor para el objetivo (y).\n",
    "            if len(vals) >= seq_len + 1:\n",
    "                # Iteramos para crear \"ventanas\" deslizantes de datos.\n",
    "                # 'i' será el índice de inicio de cada secuencia de entrada (x).\n",
    "                for i in range(len(vals) - seq_len):\n",
    "                    # Extraemos la secuencia de entrada (x): 'seq_len' valores desde el índice 'i'.\n",
    "                    x = vals[i: i + seq_len]\n",
    "                    # Extraemos el valor objetivo (y): el valor inmediatamente después de la secuencia x.\n",
    "                    y = vals[i + seq_len]\n",
    "                    # Añadimos el par (x, y) a nuestra lista de muestras.\n",
    "                    self.samples.append((x, y))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Método requerido por torch.utils.data.Dataset.\n",
    "        Devuelve el número total de muestras (pares de entrada-salida) en el dataset.\n",
    "        \"\"\"\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Método requerido por torch.utils.data.Dataset.\n",
    "        Devuelve una muestra específica (x, y) dado un índice.\n",
    "\n",
    "        Args:\n",
    "            idx (int): El índice de la muestra a recuperar.\n",
    "\n",
    "        Returns:\n",
    "            tuple: Una tupla que contiene:\n",
    "                   - x (torch.Tensor): La secuencia de entrada, con forma [seq_len, 1].\n",
    "                   - y (torch.Tensor): El valor objetivo.\n",
    "        \"\"\"\n",
    "        x, y = self.samples[idx] # Obtiene la secuencia de entrada (x) y el valor objetivo (y) de la lista.\n",
    "\n",
    "        # Convierte x a un tensor de PyTorch con tipo float32.\n",
    "        # .unsqueeze(-1) añade una dimensión al final, cambiando la forma de [seq_len] a [seq_len, 1].\n",
    "        # Esto es común para modelos RNN/LSTM que esperan características en la última dimensión.\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32).unsqueeze(-1)\n",
    "\n",
    "        # Convierte y a un tensor de PyTorch con tipo float32.\n",
    "        # y es un valor escalar, por lo que no necesita un unsqueeze.\n",
    "        y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "        return x_tensor, y_tensor # Devuelve los tensores de entrada y salida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5799109",
   "metadata": {},
   "source": [
    "El modelo **GRUForecast** está diseñado para tomar una secuencia de datos de dengue (x, por ejemplo, los casos de las últimas 7 semanas) y predecir el número de casos de dengue para la siguiente semana (y). La GRU es experta en encontrar patrones y dependencias dentro de esa secuencia temporal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ba8f4d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # Importa el módulo 'nn' de PyTorch, que contiene las capas de redes neuronales\n",
    "\n",
    "class GRUForecast(nn.Module):\n",
    "    \"\"\"\n",
    "    Define un modelo de pronóstico de series de tiempo utilizando una red GRU.\n",
    "    Hereda de nn.Module, la clase base para todos los módulos de redes neuronales en PyTorch.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        \"\"\"\n",
    "        Constructor del modelo GRUForecast.\n",
    "\n",
    "        Args:\n",
    "            input_size (int): El número de características en cada paso de tiempo de la secuencia de entrada.\n",
    "                              En nuestro caso de dengue, es 1 (el número de casos de dengue).\n",
    "            hidden_size (int): El número de características en el estado oculto (o celdas) de la GRU.\n",
    "                               Determina la capacidad de la GRU para aprender patrones complejos.\n",
    "            num_layers (int): El número de capas GRU apiladas. Más capas pueden aprender\n",
    "                              representaciones de nivel superior, pero aumentan la complejidad.\n",
    "            dropout (float): La probabilidad de aplicar Dropout en las capas GRU (excepto la última).\n",
    "                             Ayuda a prevenir el sobreajuste.\n",
    "        \"\"\"\n",
    "        super().__init__() # Llama al constructor de la clase padre (nn.Module). ¡Siempre necesario!\n",
    "\n",
    "        # Define la capa GRU (Gated Recurrent Unit).\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,    # Tamaño de las características de entrada en cada paso de tiempo.\n",
    "            hidden_size=hidden_size,  # Tamaño del estado oculto de la GRU.\n",
    "            num_layers=num_layers,    # Número de capas GRU apiladas.\n",
    "            dropout=dropout,          # Tasa de dropout aplicada entre las capas GRU.\n",
    "            batch_first=True          # Importante: Indica que la dimensión de lote (batch) es la primera.\n",
    "                                      # Las entradas esperadas serán de la forma [batch, seq_len, features].\n",
    "                                      # Si fuera False, sería [seq_len, batch, features].\n",
    "        )\n",
    "\n",
    "        # Define la capa de salida.\n",
    "        # Es una capa completamente conectada (lineal) que mapea la salida del estado oculto\n",
    "        # de la GRU (hidden_size) a una única predicción (1).\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Define el pase hacia adelante (forward pass) del modelo.\n",
    "        Describe cómo los datos fluyen a través de las capas del modelo.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): El tensor de entrada al modelo, con forma [batch_size, sequence_length, input_size].\n",
    "                              Para nuestro caso de dengue, sería [batch_size, seq_len, 1].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: El tensor de predicciones de salida, con forma [batch_size].\n",
    "                          Cada valor es la predicción de casos de dengue para el siguiente paso de tiempo.\n",
    "        \"\"\"\n",
    "        # x: El tensor de entrada. Su forma es [batch_size, seq_len, features].\n",
    "        # La GRU procesa la secuencia de entrada.\n",
    "        # 'out' contendrá la salida del estado oculto para CADA paso de tiempo de la secuencia.\n",
    "        # 'hidden' contendrá el estado oculto final después de procesar toda la secuencia.\n",
    "        out, _ = self.gru(x) # out: [batch_size, seq_len, hidden_size]. El segundo valor '_' se ignora (estado oculto final).\n",
    "\n",
    "        # Tomamos la salida del ÚLTIMO paso de tiempo de la secuencia.\n",
    "        # Esta es la salida de la GRU que encapsula la información de toda la secuencia.\n",
    "        # out[:, -1, :] selecciona todos los elementos del lote (:) en el último paso de tiempo (-1)\n",
    "        # y todas las características del estado oculto (:).\n",
    "        last = out[:, -1, :] # last: [batch_size, hidden_size]\n",
    "\n",
    "        # Pasamos la salida del último paso de tiempo a través de la capa lineal (completamente conectada).\n",
    "        # Esto reduce el 'hidden_size' a nuestro valor de predicción único (1).\n",
    "        # .squeeze() elimina dimensiones de tamaño 1. Si la salida fuera [batch_size, 1],\n",
    "        # .squeeze() la convierte a [batch_size], que es el formato deseado para predicciones escalares.\n",
    "        return self.fc(last).squeeze() # Salida final: [batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00f54196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función objetivo para la optimización de hiperparámetros con Optuna\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Función objetivo para la optimización de hiperparámetros con Optuna.\n",
    "    Optuna llamará a esta función repetidamente, sugiriendo diferentes hiperparámetros\n",
    "    en cada 'trial' (intento), y esta función devolverá la métrica de rendimiento\n",
    "    (en este caso, la pérdida en el conjunto de validación) para ese conjunto de hiperparámetros.\n",
    "\n",
    "    Args:\n",
    "        trial (optuna.trial.Trial): Objeto Trial proporcionado por Optuna, que permite\n",
    "                                    sugerir hiperparámetros y gestiona el estado del intento.\n",
    "\n",
    "    Returns:\n",
    "        float: El valor de la métrica objetivo (la pérdida promedio de MSE en el conjunto de validación).\n",
    "               Optuna intentará minimizar este valor.\n",
    "    \"\"\"\n",
    "    # --- 1. Sugerencias de Hiperparámetros con Optuna ---\n",
    "    # Optuna explora diferentes valores para estos parámetros en cada 'trial'.\n",
    "\n",
    "    # Longitud de la secuencia de entrada para la GRU.\n",
    "    # trial.suggest_int: Sugiere un entero en un rango definido, con un paso específico.\n",
    "    seq_len     = trial.suggest_int('seq_len', 4, 52, step=4) # Rango de 4 a 52, en incrementos de 4.\n",
    "\n",
    "    # Tamaño de la capa oculta de la GRU.\n",
    "    # trial.suggest_categorical: Sugiere un valor de una lista predefinida de opciones.\n",
    "    hidden_size = trial.suggest_categorical('hidden_size', [16, 32, 64, 128]) # Opciones discretas.\n",
    "\n",
    "    # Número de capas GRU apiladas.\n",
    "    num_layers  = trial.suggest_int('num_layers', 1, 2) # Puede ser 1 o 2 capas.\n",
    "\n",
    "    # Tasa de dropout para regularización.\n",
    "    # trial.suggest_float: Sugiere un número flotante en un rango continuo.\n",
    "    dropout     = trial.suggest_float('dropout', 0.0, 0.5) # Rango de 0.0 a 0.5.\n",
    "\n",
    "    # Tasa de aprendizaje para el optimizador Adam.\n",
    "    # trial.suggest_loguniform: Sugiere un flotante en un rango logarítmico (útil para tasas de aprendizaje).\n",
    "    lr          = trial.suggest_loguniform('lr', 1e-4, 1e-2) # Rango logarítmico de 0.0001 a 0.01.\n",
    "\n",
    "    # Tamaño del lote para el entrenamiento.\n",
    "    batch_size  = trial.suggest_categorical('batch_size', [16, 32, 64]) # Opciones discretas.\n",
    "\n",
    "    # --- 2. Preparar Datos y DataLoaders ---\n",
    "    # Se utiliza el dataset y los loaders con los hiperparámetros sugeridos.\n",
    "\n",
    "    # Crea una instancia de nuestro DengueDataset con el 'df_train' y el 'seq_len' sugerido.\n",
    "    dataset = DengueDataset(df_train, seq_len)\n",
    "\n",
    "    # Divide el dataset en conjuntos de entrenamiento y validación.\n",
    "    # n_train: 80% del dataset para entrenamiento.\n",
    "    n_train = int(len(dataset) * 0.8)\n",
    "    # n_val: El 20% restante para validación.\n",
    "    n_val   = len(dataset) - n_train\n",
    "    # random_split divide el dataset de forma aleatoria en los tamaños especificados.\n",
    "    train_ds, val_ds = torch.utils.data.random_split(dataset, [n_train, n_val])\n",
    "\n",
    "    # Crea DataLoaders para iterar sobre los datos en lotes.\n",
    "    # train_loader: Para el entrenamiento, con 'batch_size' sugerido y datos mezclados (shuffle=True).\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    # val_loader: Para la validación, con 'batch_size' sugerido (no es necesario mezclar para validación).\n",
    "    val_loader   = DataLoader(val_ds, batch_size=batch_size)\n",
    "\n",
    "    # --- 3. Modelo, Optimizador y Criterio de Pérdida ---\n",
    "    # Inicializa los componentes del modelo con los hiperparámetros sugeridos.\n",
    "\n",
    "    # Instancia el modelo GRUForecast. input_size es 1 porque tenemos una sola característica (dengue cases).\n",
    "    model     = GRUForecast(1, hidden_size, num_layers, dropout)\n",
    "    # Define el optimizador (Adam es una buena opción para redes neuronales) con la tasa de aprendizaje sugerida.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    # Define la función de pérdida (Mean Squared Error - MSE), común para problemas de regresión.\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # --- 4. Entrenamiento Rápido del Modelo ---\n",
    "    # Se realiza un entrenamiento simplificado con un número fijo de épocas (ej. 10)\n",
    "    # para evaluar el rendimiento de esta combinación de hiperparámetros.\n",
    "    # En un entrenamiento completo, esto sería más largo.\n",
    "    for epoch in range(10): # Iteramos por un número fijo de épocas de entrenamiento\n",
    "        model.train() # Pone el modelo en modo de entrenamiento (activa dropout, etc.)\n",
    "        for x_batch, y_batch in train_loader: # Itera sobre los lotes del conjunto de entrenamiento\n",
    "            optimizer.zero_grad() # Reinicia los gradientes acumulados de la iteración anterior\n",
    "            y_pred = model(x_batch) # Realiza un pase hacia adelante: predice 'y' a partir de 'x'\n",
    "            loss = criterion(y_pred, y_batch) # Calcula la pérdida entre las predicciones y los valores reales\n",
    "            loss.backward() # Calcula los gradientes de la pérdida con respecto a los parámetros del modelo\n",
    "            optimizer.step() # Actualiza los pesos del modelo usando los gradientes y la tasa de aprendizaje\n",
    "\n",
    "    # --- 5. Validación del Modelo ---\n",
    "    # Evalúa el rendimiento del modelo entrenado en el conjunto de validación.\n",
    "\n",
    "    model.eval() # Pone el modelo en modo de evaluación (desactiva dropout, etc.)\n",
    "    losses = []  # Lista para almacenar las pérdidas de cada lote de validación\n",
    "    with torch.no_grad(): # Desactiva el cálculo de gradientes para ahorrar memoria y acelerar\n",
    "                          # la inferencia, ya que no estamos entrenando en esta fase.\n",
    "        for x_batch, y_batch in val_loader: # Itera sobre los lotes del conjunto de validación\n",
    "            y_pred = model(x_batch) # Realiza predicciones\n",
    "            # Calcula la pérdida para el lote actual y la añade a la lista.\n",
    "            # .item() se usa para obtener el valor escalar de un tensor de PyTorch.\n",
    "            losses.append(criterion(y_pred, y_batch).item())\n",
    "\n",
    "    # --- 6. Devolver la Métrica Objetivo ---\n",
    "    # El valor promedio de las pérdidas en el conjunto de validación.\n",
    "    # Optuna intentará minimizar este valor para encontrar los mejores hiperparámetros.\n",
    "    return np.mean(losses) # Retorna el promedio de las pérdidas de validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff915218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-29 13:29:49,598] A new study created in memory with name: no-name-75da7a11-1dd2-46f7-8378-3bb43caced11\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6e9e17dd694080be7367442e2eb358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/60 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-06-29 13:30:05,760] Trial 0 finished with value: 4.3589935952966865 and parameters: {'seq_len': 24, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.2765393101316985, 'lr': 0.0004984585009470663, 'batch_size': 64}. Best is trial 0 with value: 4.3589935952966865.\n",
      "[I 2025-06-29 13:30:18,989] Trial 1 finished with value: 3.9438720616427334 and parameters: {'seq_len': 20, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.37100886065788374, 'lr': 0.009175082401699354, 'batch_size': 64}. Best is trial 1 with value: 3.9438720616427334.\n",
      "[I 2025-06-29 13:30:21,125] Trial 2 finished with value: 5.273074388504028 and parameters: {'seq_len': 24, 'hidden_size': 16, 'num_layers': 1, 'dropout': 0.003644705983679941, 'lr': 0.0015605076916763833, 'batch_size': 64}. Best is trial 1 with value: 3.9438720616427334.\n",
      "[I 2025-06-29 13:30:29,685] Trial 3 finished with value: 5.21680723325066 and parameters: {'seq_len': 4, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.18255394290364552, 'lr': 0.004450711794937655, 'batch_size': 16}. Best is trial 1 with value: 3.9438720616427334.\n",
      "[I 2025-06-29 13:30:37,520] Trial 4 finished with value: 4.878299135631985 and parameters: {'seq_len': 8, 'hidden_size': 32, 'num_layers': 2, 'dropout': 0.282248401513021, 'lr': 0.000555863472086251, 'batch_size': 16}. Best is trial 1 with value: 3.9438720616427334.\n",
      "[I 2025-06-29 13:31:04,345] Trial 5 finished with value: 4.547095452866903 and parameters: {'seq_len': 40, 'hidden_size': 64, 'num_layers': 1, 'dropout': 0.09563678076884047, 'lr': 0.0005759499021177728, 'batch_size': 16}. Best is trial 1 with value: 3.9438720616427334.\n",
      "[I 2025-06-29 13:31:28,807] Trial 6 finished with value: 4.845317961110009 and parameters: {'seq_len': 12, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.467132005243471, 'lr': 0.007881860991681703, 'batch_size': 16}. Best is trial 1 with value: 3.9438720616427334.\n",
      "[I 2025-06-29 13:31:45,966] Trial 7 finished with value: 4.366696220068705 and parameters: {'seq_len': 32, 'hidden_size': 32, 'num_layers': 2, 'dropout': 0.39624142602723816, 'lr': 0.00758991109423455, 'batch_size': 16}. Best is trial 1 with value: 3.9438720616427334.\n",
      "[I 2025-06-29 13:31:52,393] Trial 8 finished with value: 4.681520563364029 and parameters: {'seq_len': 52, 'hidden_size': 32, 'num_layers': 1, 'dropout': 0.13755550073819844, 'lr': 0.00516766065253369, 'batch_size': 32}. Best is trial 1 with value: 3.9438720616427334.\n",
      "[I 2025-06-29 13:32:17,704] Trial 9 finished with value: 3.819083282133428 and parameters: {'seq_len': 44, 'hidden_size': 64, 'num_layers': 1, 'dropout': 0.18496934678753418, 'lr': 0.0001492172142740527, 'batch_size': 16}. Best is trial 9 with value: 3.819083282133428.\n",
      "[I 2025-06-29 13:32:33,567] Trial 10 finished with value: 5.76490418612957 and parameters: {'seq_len': 52, 'hidden_size': 64, 'num_layers': 1, 'dropout': 0.01419680222112718, 'lr': 0.0001324881471151542, 'batch_size': 32}. Best is trial 9 with value: 3.819083282133428.\n",
      "[I 2025-06-29 13:32:45,496] Trial 11 finished with value: 4.550117124210704 and parameters: {'seq_len': 40, 'hidden_size': 128, 'num_layers': 1, 'dropout': 0.34015362797021986, 'lr': 0.000173515298668011, 'batch_size': 64}. Best is trial 9 with value: 3.819083282133428.\n",
      "[I 2025-06-29 13:32:47,251] Trial 12 finished with value: 4.803546461192044 and parameters: {'seq_len': 16, 'hidden_size': 16, 'num_layers': 1, 'dropout': 0.213890828696934, 'lr': 0.0017122224659157676, 'batch_size': 64}. Best is trial 9 with value: 3.819083282133428.\n",
      "[I 2025-06-29 13:33:20,204] Trial 13 finished with value: 4.172180826013738 and parameters: {'seq_len': 36, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.4738520390945745, 'lr': 0.0002239837462631833, 'batch_size': 64}. Best is trial 9 with value: 3.819083282133428.\n",
      "[I 2025-06-29 13:33:30,613] Trial 14 finished with value: 3.9729038693688135 and parameters: {'seq_len': 20, 'hidden_size': 64, 'num_layers': 1, 'dropout': 0.4093543919099819, 'lr': 0.003023514940832287, 'batch_size': 32}. Best is trial 9 with value: 3.819083282133428.\n",
      "[I 2025-06-29 13:35:00,256] Trial 15 finished with value: 3.581799608905141 and parameters: {'seq_len': 44, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.3385339701581277, 'lr': 0.00029824849862305483, 'batch_size': 16}. Best is trial 15 with value: 3.581799608905141.\n",
      "[I 2025-06-29 13:35:27,744] Trial 16 finished with value: 4.090079073498889 and parameters: {'seq_len': 44, 'hidden_size': 64, 'num_layers': 1, 'dropout': 0.31763887119856216, 'lr': 0.00031874341142518073, 'batch_size': 16}. Best is trial 15 with value: 3.581799608905141.\n",
      "[I 2025-06-29 13:35:46,536] Trial 17 finished with value: 6.280261421203614 and parameters: {'seq_len': 48, 'hidden_size': 16, 'num_layers': 2, 'dropout': 0.22991191905320074, 'lr': 0.00010440933326053447, 'batch_size': 16}. Best is trial 15 with value: 3.581799608905141.\n",
      "[I 2025-06-29 13:36:28,191] Trial 18 finished with value: 3.385411657038189 and parameters: {'seq_len': 32, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.08138760749131069, 'lr': 0.0003111516914422424, 'batch_size': 16}. Best is trial 18 with value: 3.385411657038189.\n",
      "[I 2025-06-29 13:37:23,653] Trial 19 finished with value: 4.136194894711177 and parameters: {'seq_len': 32, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.08285753304724609, 'lr': 0.00028163240423321974, 'batch_size': 16}. Best is trial 18 with value: 3.385411657038189.\n",
      "[I 2025-06-29 13:38:05,072] Trial 20 finished with value: 3.6241244205406735 and parameters: {'seq_len': 32, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.06570484403725319, 'lr': 0.0008598833136605942, 'batch_size': 16}. Best is trial 18 with value: 3.385411657038189.\n",
      "[I 2025-06-29 13:38:44,980] Trial 21 finished with value: 4.5721352980250405 and parameters: {'seq_len': 32, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.0903875408089817, 'lr': 0.000917045643172099, 'batch_size': 16}. Best is trial 18 with value: 3.385411657038189.\n",
      "[I 2025-06-29 13:39:21,622] Trial 22 finished with value: 3.5938769634379897 and parameters: {'seq_len': 28, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.03650379957458166, 'lr': 0.0003769131026056087, 'batch_size': 16}. Best is trial 18 with value: 3.385411657038189.\n",
      "[I 2025-06-29 13:40:03,154] Trial 23 finished with value: 5.171505613382473 and parameters: {'seq_len': 28, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.04274612199050137, 'lr': 0.00035766158286356026, 'batch_size': 16}. Best is trial 18 with value: 3.385411657038189.\n",
      "[I 2025-06-29 13:40:57,577] Trial 24 finished with value: 3.323225900894258 and parameters: {'seq_len': 40, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.1448301643205347, 'lr': 0.00041238942645232233, 'batch_size': 16}. Best is trial 24 with value: 3.323225900894258.\n",
      "[I 2025-06-29 13:41:39,614] Trial 25 finished with value: 5.715174470629011 and parameters: {'seq_len': 40, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.13802478649689892, 'lr': 0.00021174469992521423, 'batch_size': 32}. Best is trial 24 with value: 3.323225900894258.\n",
      "[I 2025-06-29 13:41:57,518] Trial 26 finished with value: 3.688655840187538 and parameters: {'seq_len': 44, 'hidden_size': 16, 'num_layers': 2, 'dropout': 0.15393047529506704, 'lr': 0.0013126080883724977, 'batch_size': 16}. Best is trial 24 with value: 3.323225900894258.\n",
      "[I 2025-06-29 13:42:15,650] Trial 27 finished with value: 3.9091337351571944 and parameters: {'seq_len': 36, 'hidden_size': 32, 'num_layers': 2, 'dropout': 0.26963468070302626, 'lr': 0.00046896842556433946, 'batch_size': 16}. Best is trial 24 with value: 3.323225900894258.\n",
      "[I 2025-06-29 13:43:16,870] Trial 28 finished with value: 4.322045555710792 and parameters: {'seq_len': 48, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.11688122557437712, 'lr': 0.0007470326344788177, 'batch_size': 16}. Best is trial 24 with value: 3.323225900894258.\n",
      "[I 2025-06-29 13:43:55,092] Trial 29 finished with value: 3.948231963884263 and parameters: {'seq_len': 36, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.1963120475219974, 'lr': 0.00024105896984025888, 'batch_size': 32}. Best is trial 24 with value: 3.323225900894258.\n",
      "[I 2025-06-29 13:44:44,175] Trial 30 finished with value: 4.259900320407956 and parameters: {'seq_len': 28, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.29249690154845126, 'lr': 0.0004476150244728894, 'batch_size': 16}. Best is trial 24 with value: 3.323225900894258.\n",
      "[I 2025-06-29 13:45:15,868] Trial 31 finished with value: 4.956022001976191 and parameters: {'seq_len': 24, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.04504397771008245, 'lr': 0.00036651504765440124, 'batch_size': 16}. Best is trial 24 with value: 3.323225900894258.\n",
      "[I 2025-06-29 13:45:47,763] Trial 32 finished with value: 3.701316780822222 and parameters: {'seq_len': 24, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.04897833209145122, 'lr': 0.0006759812506084551, 'batch_size': 16}. Best is trial 24 with value: 3.323225900894258.\n",
      "[I 2025-06-29 13:46:33,562] Trial 33 finished with value: 3.058107390290215 and parameters: {'seq_len': 36, 'hidden_size': 64, 'num_layers': 2, 'dropout': 2.2227700877480716e-05, 'lr': 0.00039977465966682576, 'batch_size': 16}. Best is trial 33 with value: 3.058107390290215.\n",
      "[I 2025-06-29 13:47:23,450] Trial 34 finished with value: 4.378029978856808 and parameters: {'seq_len': 40, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.11081580394378956, 'lr': 0.0011238296323066055, 'batch_size': 16}. Best is trial 33 with value: 3.058107390290215.\n",
      "[I 2025-06-29 13:47:48,666] Trial 35 finished with value: 3.65720077753067 and parameters: {'seq_len': 48, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.007494053270360956, 'lr': 0.00018415051298139394, 'batch_size': 64}. Best is trial 33 with value: 3.058107390290215.\n",
      "[I 2025-06-29 13:48:04,928] Trial 36 finished with value: 6.1555456547510055 and parameters: {'seq_len': 36, 'hidden_size': 16, 'num_layers': 2, 'dropout': 0.35505216962027597, 'lr': 0.00028095885355386554, 'batch_size': 16}. Best is trial 33 with value: 3.058107390290215.\n",
      "[I 2025-06-29 13:49:03,826] Trial 37 finished with value: 3.9616909623146057 and parameters: {'seq_len': 44, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.24845280446571602, 'lr': 0.0006284519940309147, 'batch_size': 16}. Best is trial 33 with value: 3.058107390290215.\n",
      "[I 2025-06-29 13:50:18,409] Trial 38 finished with value: 2.9652533182283727 and parameters: {'seq_len': 40, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.16171591245241718, 'lr': 0.0004648495361268025, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 13:50:38,652] Trial 39 finished with value: 3.7370214273289935 and parameters: {'seq_len': 40, 'hidden_size': 32, 'num_layers': 2, 'dropout': 0.16432742109036638, 'lr': 0.0005112405780798051, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 13:50:55,601] Trial 40 finished with value: 3.6094761436635796 and parameters: {'seq_len': 32, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.07248141915119267, 'lr': 0.0019293548752367306, 'batch_size': 64}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 13:52:05,323] Trial 41 finished with value: 3.530251999696096 and parameters: {'seq_len': 36, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.11091278650939251, 'lr': 0.0004067306473439658, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 13:53:34,273] Trial 42 finished with value: 3.2946254710356393 and parameters: {'seq_len': 36, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.11831584974694741, 'lr': 0.00043149140579461616, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 13:54:49,320] Trial 43 finished with value: 3.534223454754527 and parameters: {'seq_len': 40, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.16412208680598567, 'lr': 0.0005329178711351411, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 13:56:32,501] Trial 44 finished with value: 3.5137689468406497 and parameters: {'seq_len': 36, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.12486062743899003, 'lr': 0.0006275008609276635, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 13:57:37,511] Trial 45 finished with value: 4.3911909971918375 and parameters: {'seq_len': 32, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.024451529274485302, 'lr': 0.0007635237429275619, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 13:57:49,282] Trial 46 finished with value: 5.322045933632624 and parameters: {'seq_len': 40, 'hidden_size': 32, 'num_layers': 2, 'dropout': 0.18882617960667214, 'lr': 0.0002436329637168212, 'batch_size': 32}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 13:57:58,300] Trial 47 finished with value: 4.056415237810301 and parameters: {'seq_len': 4, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.0010425344101119473, 'lr': 0.00014575033854260616, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 13:58:40,983] Trial 48 finished with value: 3.6583735753189432 and parameters: {'seq_len': 20, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.143259803033352, 'lr': 0.0004905670909290037, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 13:58:56,054] Trial 49 finished with value: 5.522269682450728 and parameters: {'seq_len': 28, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.06374664770075229, 'lr': 0.0010881277506888912, 'batch_size': 64}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 13:59:03,621] Trial 50 finished with value: 5.40549470271383 and parameters: {'seq_len': 36, 'hidden_size': 16, 'num_layers': 1, 'dropout': 0.203759164112902, 'lr': 0.00029588489734519744, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 14:00:11,358] Trial 51 finished with value: 3.3733258956954595 and parameters: {'seq_len': 36, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.12496257540397498, 'lr': 0.0006173452114990776, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 14:01:22,582] Trial 52 finished with value: 5.955205216294243 and parameters: {'seq_len': 36, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.09457088460581752, 'lr': 0.0004212189286152479, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 14:02:43,026] Trial 53 finished with value: 3.633388969956375 and parameters: {'seq_len': 44, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.15891218373836163, 'lr': 0.0005839110961588124, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 14:03:46,336] Trial 54 finished with value: 3.687794974872044 and parameters: {'seq_len': 32, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.1761736492851193, 'lr': 0.00033503806684238825, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 14:05:08,266] Trial 55 finished with value: 3.860293761986058 and parameters: {'seq_len': 40, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.23159922593258936, 'lr': 0.0007941724125947002, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 14:05:51,950] Trial 56 finished with value: 4.093273423966908 and parameters: {'seq_len': 32, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.1280467860846788, 'lr': 0.00018566872152965, 'batch_size': 32}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 14:06:50,073] Trial 57 finished with value: 3.7618986513556503 and parameters: {'seq_len': 40, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.10404259127365237, 'lr': 0.00025676575115757126, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 14:07:13,259] Trial 58 finished with value: 4.773889483511448 and parameters: {'seq_len': 48, 'hidden_size': 32, 'num_layers': 2, 'dropout': 0.07937682976385567, 'lr': 0.00033446120264685143, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "[I 2025-06-29 14:08:06,286] Trial 59 finished with value: 3.982052730662482 and parameters: {'seq_len': 36, 'hidden_size': 64, 'num_layers': 2, 'dropout': 0.49666024586671553, 'lr': 0.0004273072022478274, 'batch_size': 16}. Best is trial 38 with value: 2.9652533182283727.\n",
      "Mejores parámetros: {'seq_len': 40, 'hidden_size': 128, 'num_layers': 2, 'dropout': 0.16171591245241718, 'lr': 0.0004648495361268025, 'batch_size': 16}\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar la búsqueda\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=60, show_progress_bar=True)\n",
    "\n",
    "print('Mejores parámetros:', study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a01687bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pronóstico para 2022 guardado en sample_submission.csv\n"
     ]
    }
   ],
   "source": [
    "best = study.best_params\n",
    "# Preparar dataset completo\n",
    "dataset = DengueDataset(df_train, best['seq_len'])\n",
    "loader  = DataLoader(dataset, batch_size=best['batch_size'], shuffle=True)\n",
    "\n",
    "# Instanciar modelo final\n",
    "model     = GRUForecast(1, best['hidden_size'], best['num_layers'], best['dropout'])\n",
    "optimizer = optim.Adam(model.parameters(), lr=best['lr'])\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Entrenar final (ej: 30 épocas)\n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "    for x_batch, y_batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x_batch)\n",
    "        loss   = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Pronóstico iterativo para 2022\n",
    "preds = []\n",
    "for bar, grp in df_train.groupby('id_bar'):\n",
    "    vals = grp.sort_values('date')['dengue'].values.tolist()\n",
    "    seq  = vals[-best['seq_len']:]\n",
    "    for week in range(1, 53):\n",
    "        x_input = torch.tensor(seq[-best['seq_len']:], dtype=torch.float32).unsqueeze(0).unsqueeze(-1)\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(x_input).item()\n",
    "        preds.append({\n",
    "            'id': f\"{bar}_2022_{str(week).zfill(2)}\",\n",
    "            'dengue': round(y_hat, 2)\n",
    "        })\n",
    "        seq.append(y_hat)\n",
    "\n",
    "# Crear DataFrame y guardar\n",
    "df_preds = pd.DataFrame(preds)\n",
    "fecha_actual = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "df_preds.to_csv(f'sample_submission_gru_{fecha_actual}.csv', index=False)\n",
    "print('Pronóstico para 2022 guardado en sample_submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
