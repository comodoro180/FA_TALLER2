{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab0c346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECCIÓN 0: CONFIGURACIÓN INICIAL E IMPORTACIÓN DE LIBRERÍAS\n",
    "# ==============================================================================\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import optuna\n",
    "\n",
    "# AJUSTE: Suprimir advertencias para una salida más limpia\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- Configuración de Gráficos y PyTorch ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "os.makedirs('output', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077834a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# SECCIÓN 1: ANÁLISIS EXPLORATORIO DE DATOS (EDA)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Iniciando Sección 1: Carga y Análisis Exploratorio de Datos ---\")\n",
    "\n",
    "# --- 1.1 Carga de Datos ---\n",
    "DATA_PATH = '../../Datos/'\n",
    "try:\n",
    "    df_train = pd.read_parquet(f'{DATA_PATH}df_train.parquet')\n",
    "    df_train_raw =  df_train[df_train['anio'] < 2021] #pd.read_csv(f'{DATA_PATH}df_train.csv')\n",
    "    df_test_raw =  df_train[df_train['anio'] == 2021] #pd.read_csv(f'{DATA_PATH}df_test.csv')\n",
    "    sample_submission = pd.read_csv(f'{DATA_PATH}sample_submission.csv')\n",
    "    print(\"Archivos cargados exitosamente.\")\n",
    "    # Identificar el nombre real de la columna de casos\n",
    "    TARGET = [col for col in df_train_raw.columns if 'dengue' in col.lower()][0]\n",
    "    print(f\"Variable objetivo identificada como: '{TARGET}'\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: No se encontraron los archivos en la ruta '{DATA_PATH}'.\")\n",
    "    exit()\n",
    "except IndexError:\n",
    "    print(\"Error: No se pudo identificar la columna de casos en 'df_train.csv'. Asegúrate de que exista.\")\n",
    "    exit()\n",
    "\n",
    "# --- 1.2 Preprocesamiento de Datos (Fechas, Categorías, Nulos) ---\n",
    "print(\"\\nIniciando preprocesamiento de datos...\")\n",
    "train_len = len(df_train_raw)\n",
    "full_df_raw = pd.concat([df_train_raw, df_test_raw], axis=0, ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "# Manejo de la variable categórica 'estrato'\n",
    "if 'estrato' in full_df_raw.columns:\n",
    "    print(\"Codificando la variable categórica 'estrato'...\")\n",
    "    full_df_processed = pd.get_dummies(full_df_raw, columns=['estrato'], drop_first=True)\n",
    "else:\n",
    "    full_df_processed = full_df_raw.copy()\n",
    "\n",
    "# Crear el índice de tiempo usando 'anio' y 'semana'\n",
    "full_df_processed['date'] = pd.to_datetime(\n",
    "    full_df_processed['anio'].astype(str) + full_df_processed['semana'].astype(str) + '-1',\n",
    "    format='%Y%W-%w'\n",
    ")\n",
    "\n",
    "# Ordenar los datos cronológicamente\n",
    "full_df_processed.sort_values(by='date', inplace=True)\n",
    "full_df_processed.set_index('date', inplace=True)\n",
    "\n",
    "# Rellenar valores NaN usando interpolación lineal\n",
    "full_df_processed.interpolate(method='linear', inplace=True, limit_direction='both')\n",
    "full_df_processed.fillna(method='bfill', inplace=True)\n",
    "full_df_processed.fillna(method='ffill', inplace=True)\n",
    "\n",
    "# Separar de nuevo en entrenamiento y prueba\n",
    "df_train = full_df_processed.iloc[:train_len].copy()\n",
    "df_test = full_df_processed.iloc[train_len:].copy()\n",
    "print(\"Preprocesamiento completado.\")\n",
    "\n",
    "# --- 1.3 Análisis de la Serie de Tiempo del Target ---\n",
    "plt.figure(figsize=(18, 6))\n",
    "df_train[TARGET].plot(label=f'Casos de Dengue ({TARGET})')\n",
    "plt.title('Casos de Dengue a lo largo del tiempo (Entrenamiento)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Número de Casos')\n",
    "plt.legend()\n",
    "plt.savefig('output/eda_timeseries_plot.png')\n",
    "plt.show()\n",
    "\n",
    "# Descomposición estacional\n",
    "decomposition = seasonal_decompose(df_train[TARGET], model='additive', period=52)\n",
    "fig = decomposition.plot()\n",
    "fig.set_size_inches(14, 10)\n",
    "plt.savefig('output/eda_decomposition.png')\n",
    "plt.show()\n",
    "\n",
    "# --- 1.4 Análisis de Correlación ---\n",
    "plt.figure(figsize=(16, 14))\n",
    "corr_cols = [col for col in df_train.columns if col not in ['anio', 'semana']]\n",
    "correlation_matrix = df_train[corr_cols].corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Matriz de Correlación de Variables')\n",
    "plt.savefig('output/eda_correlation_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675cf0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==============================================================================\n",
    "# SECCIÓN 2: MODELO LSTM CON PYTORCH Y OPTIMIZACIÓN CON OPTUNA\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Iniciando Sección 2: Modelo LSTM y Optimización de Hiperparámetros ---\")\n",
    "\n",
    "# --- 2.1 Preparación de Datos para LSTM ---\n",
    "# Seleccionar features excluyendo columnas de tiempo y el target\n",
    "features = [col for col in df_train.columns if col not in [TARGET, 'anio', 'semana']]\n",
    "if 'ESTRATO' in features:\n",
    "    features.remove('ESTRATO')\n",
    "\n",
    "# AJUSTE: Excluir la columna 'lluvia_min' de las características si existe\n",
    "if 'lluvia_min' in features:\n",
    "    print(\"Excluyendo la columna 'lluvia_min' de las características.\")\n",
    "    features.remove('lluvia_min')\n",
    "\n",
    "features = ['lluvia_mean', 'temperatura_mean']\n",
    "\n",
    "print(f\"\\nCaracterísticas finales para el modelo ({len(features)}): {features}\")\n",
    "num_features = len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b486f6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escalar las características\n",
    "scaler_features = MinMaxScaler()\n",
    "scaler_target = MinMaxScaler()\n",
    "\n",
    "df_train_scaled = df_train.copy()\n",
    "df_test_scaled = df_test.copy()\n",
    "\n",
    "df_train_scaled[features] = scaler_features.fit_transform(df_train[features])\n",
    "df_train_scaled[TARGET] = scaler_target.fit_transform(df_train[[TARGET]])\n",
    "df_test_scaled[features] = scaler_features.transform(df_test[features])\n",
    "\n",
    "def create_sequences(input_data, target_data, sequence_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(input_data) - sequence_length):\n",
    "        X.append(input_data[i:(i + sequence_length)])\n",
    "        y.append(target_data[i + sequence_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "SEQUENCE_LENGTH = 12\n",
    "X_train_data = df_train_scaled[features].values\n",
    "y_train_data = df_train_scaled[TARGET].values\n",
    "X_seq, y_seq = create_sequences(X_train_data, y_train_data, SEQUENCE_LENGTH)\n",
    "\n",
    "split_index = int(len(X_seq) * 0.8)\n",
    "X_train, X_val = X_seq[:split_index], X_seq[split_index:]\n",
    "y_train, y_val = y_seq[:split_index], y_seq[split_index:]\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1).to(device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "# --- 2.2 Definición del Modelo LSTM ---\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout_rate):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers,\n",
    "                            batch_first=True, dropout=dropout_rate if num_layers > 1 else 0)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# --- 2.3 Optimización con Optuna ---\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_layers': trial.suggest_int('n_layers', 1, 3),\n",
    "        'hidden_size': trial.suggest_int('hidden_size', 32, 256, log=True),\n",
    "        'dropout': trial.suggest_float('dropout', 0.1, 0.5),\n",
    "        'lr': trial.suggest_float('lr', 1e-4, 1e-2, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    }\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=params['batch_size'], shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=params['batch_size'], shuffle=False)\n",
    "    model = LSTMModel(num_features, params['hidden_size'], params['n_layers'], 1, params['dropout']).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params['lr'])\n",
    "    epochs = 30\n",
    "    for _ in range(epochs):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(X_batch), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            val_loss += criterion(model(X_batch), y_batch).item()\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "N_TRIALS = 20\n",
    "print(f\"\\nIniciando optimización con Optuna ({N_TRIALS} trials)...\")\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=N_TRIALS, timeout=600)\n",
    "best_params = study.best_params\n",
    "print(f\"\\nOptimización completada. Mejores hiperparámetros: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06bd207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# ==============================================================================\n",
    "# SECCIÓN 3: ENTRENAMIENTO FINAL Y GENERACIÓN DE PRONÓSTICOS\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Iniciando Sección 3: Entrenamiento Final y Pronóstico ---\")\n",
    "\n",
    "# --- 3.1 Entrenamiento del Modelo Final ---\n",
    "X_final_train_tensor = torch.tensor(X_seq, dtype=torch.float32).to(device)\n",
    "y_final_train_tensor = torch.tensor(y_seq, dtype=torch.float32).view(-1, 1).to(device)\n",
    "final_train_loader = DataLoader(TensorDataset(X_final_train_tensor, y_final_train_tensor), batch_size=best_params['batch_size'], shuffle=True)\n",
    "final_model = LSTMModel(num_features, best_params['hidden_size'], best_params['n_layers'], 1, best_params['dropout']).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params['lr'])\n",
    "\n",
    "epochs = 60\n",
    "print(f\"Entrenando el modelo final por {epochs} épocas...\")\n",
    "for epoch in range(epochs):\n",
    "    final_model.train()\n",
    "    for X_batch, y_batch in final_train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(final_model(X_batch), y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}')\n",
    "print(\"Entrenamiento final completado.\")\n",
    "\n",
    "# --- 3.2 Generación de Pronósticos ---\n",
    "print(\"\\nGenerando pronósticos...\")\n",
    "final_model.eval()\n",
    "last_sequence = torch.tensor(X_train_data[-SEQUENCE_LENGTH:], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(df_test_scaled)):\n",
    "        predicted_target_scaled = final_model(last_sequence)\n",
    "        predictions.append(predicted_target_scaled.item())\n",
    "        future_exog_features = df_test_scaled[features].iloc[i].values\n",
    "        new_sequence_np = np.append(last_sequence.cpu().numpy().squeeze(0)[1:], future_exog_features.reshape(1, -1), axis=0)\n",
    "        last_sequence = torch.tensor(new_sequence_np, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "predictions_descaled = scaler_target.inverse_transform(np.array(predictions).reshape(-1, 1))\n",
    "final_predictions = np.round(predictions_descaled).astype(int).flatten()\n",
    "final_predictions[final_predictions < 0] = 0\n",
    "print(\"Pronósticos generados.\")\n",
    "\n",
    "# --- 3.3 Creación del Archivo de Submission ---\n",
    "submission_df = sample_submission.copy()\n",
    "# Ensure predictions match submission length\n",
    "if len(final_predictions) != len(submission_df):\n",
    "    print(f\"Warning: predictions ({len(final_predictions)}) and submission ({len(submission_df)}) length mismatch.\")\n",
    "    if len(final_predictions) > len(submission_df):\n",
    "        final_predictions = final_predictions[:len(submission_df)]\n",
    "    else:\n",
    "        # Pad with zeros if predictions are shorter\n",
    "        final_predictions = np.pad(final_predictions, (0, len(submission_df) - len(final_predictions)), 'constant')\n",
    "# Assign after adjustment\n",
    "submission_df[TARGET] = final_predictions\n",
    "fecha_actual = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "SUBMISSION_FILE = f'output/submission_{fecha_actual}.csv'\n",
    "submission_df.to_csv(SUBMISSION_FILE, index=False)\n",
    "print(f\"\\nArchivo de submission '{SUBMISSION_FILE}' creado.\")\n",
    "print(submission_df.head())\n",
    "\n",
    "# --- 3.4 Visualización de Resultados ---\n",
    "plt.figure(figsize=(18, 7))\n",
    "# Graficar datos históricos\n",
    "df_train[TARGET].plot(label='Datos Históricos', alpha=0.75)\n",
    "# Graficar predicciones\n",
    "submission_index = pd.to_datetime(\n",
    "    df_test_raw['anio'].astype(str) + df_test_raw['semana'].astype(str) + '-1',\n",
    "    format='%Y%W-%w'\n",
    ")\n",
    "# Ensure index and predictions are the same length\n",
    "if len(submission_index) != len(final_predictions):\n",
    "    print(f\"Warning: submission_index ({len(submission_index)}) and final_predictions ({len(final_predictions)}) length mismatch.\")\n",
    "    if len(submission_index) > len(final_predictions):\n",
    "        submission_index = submission_index[-len(final_predictions):]\n",
    "    else:\n",
    "        final_predictions = final_predictions[-len(submission_index):]\n",
    "predictions_series = pd.Series(final_predictions, index=submission_index)\n",
    "predictions_series.plot(label='Pronóstico LSTM', style='r--')\n",
    "\n",
    "plt.title('Pronóstico de Casos de Dengue vs. Datos Históricos')\n",
    "plt.xlabel('Fecha (creada desde anio y semana)')\n",
    "plt.ylabel('Número de Casos')\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('output/final_forecast_vs_history.png')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Proceso Finalizado ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
