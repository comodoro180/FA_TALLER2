{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaebb117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Análisis Exploratorio de Datos (EDA) y Optimización de Hiperparámetros\n",
    "de un modelo LSTM con PyTorch y Optuna para la predicción de Casos de Dengue.\n",
    "\n",
    "Este script realiza lo siguiente:\n",
    "1. Carga y prepara los datos de entrenamiento desde un archivo Parquet.\n",
    "2. Realiza un EDA detallado de la serie de tiempo (estacionariedad, descomposición, autocorrelación).\n",
    "3. Implementa una red LSTM usando PyTorch.\n",
    "4. Utiliza Optuna para encontrar los mejores hiperparámetros para el modelo LSTM.\n",
    "5. Entrena el modelo final con los mejores hiperparámetros y visualiza los resultados.\n",
    "6. Utiliza el modelo final para predecir los casos de dengue para 2022.\n",
    "7. Genera un archivo de submission con el formato requerido.\n",
    "\"\"\"\n",
    "\n",
    "# 1. Importación de Librerías\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Librerías para el EDA de series de tiempo\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import optuna\n",
    "\n",
    "# Configuración de estilo para las gráficas\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# --- ANÁLISIS DEL CONJUNTO DE ENTRENAMIENTO (df_train.parquet) ---\n",
    "\n",
    "# 2. Carga y Preparación de Datos\n",
    "print(\"Cargando datos de entrenamiento...\")\n",
    "DATA_DIR = '../../Datos/'\n",
    "try:\n",
    "    # Cargar desde el formato Parquet en el directorio especificado\n",
    "    df_train = pd.read_parquet(os.path.join(DATA_DIR, 'df_train.parquet'))\n",
    "    df_train['date'] = pd.to_datetime(df_train['anio'].astype(str) + '-' + df_train['semana'].astype(str) + '-1', format='%Y-%W-%w')\n",
    "    df_train.set_index('date', inplace=True)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo 'df_train.parquet' no se encontró en el directorio '{DATA_DIR}'. Asegúrate de que la ruta es correcta.\")\n",
    "    exit()\n",
    "\n",
    "# 3. Agregación y Visualización de la Serie de Tiempo\n",
    "dengue_series = df_train['dengue'].resample('W').sum()\n",
    "print(\"\\nVisualizando la serie de tiempo de casos de dengue...\")\n",
    "dengue_series.plot(title='Casos de Dengue Agregados por Semana')\n",
    "plt.ylabel('Número de Casos de Dengue')\n",
    "plt.xlabel('Fecha')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- ANÁLISIS EXPLORATORIO DE DATOS (EDA) DE LA SERIE DE TIEMPO ---\n",
    "\n",
    "# 4. Análisis de Estacionariedad\n",
    "print(\"\\n--- Analizando la estacionariedad de la serie ---\")\n",
    "# ... (código del EDA se mantiene igual) ...\n",
    "# Prueba de Dickey-Fuller Aumentada (ADF)\n",
    "print('\\nResultados de la Prueba de Dickey-Fuller Aumentada:')\n",
    "dftest = adfuller(dengue_series.dropna(), autolag='AIC')\n",
    "dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\n",
    "for key, value in dftest[4].items():\n",
    "    dfoutput['Critical Value (%s)' % key] = value\n",
    "print(dfoutput)\n",
    "if dfoutput['p-value'] < 0.05:\n",
    "    print(\"\\nInterpretación: La serie es estacionaria.\")\n",
    "else:\n",
    "    print(\"\\nInterpretación: La serie no es estacionaria.\")\n",
    "\n",
    "# 5. Descomposición de la Serie de Tiempo\n",
    "print(\"\\n--- Descomponiendo la serie de tiempo... ---\")\n",
    "if len(dengue_series) >= 104:\n",
    "    decomposition = seasonal_decompose(dengue_series, model='additive', period=52)\n",
    "    fig = decomposition.plot()\n",
    "    fig.set_size_inches(12, 10)\n",
    "    plt.show()\n",
    "\n",
    "# 6. Gráficas de Autocorrelación (ACF) y Autocorrelación Parcial (PACF)\n",
    "print(\"\\n--- Generando gráficas de autocorrelación (ACF) y PACF... ---\")\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "plot_acf(dengue_series.dropna(), ax=ax1, lags=52)\n",
    "ax1.set_title('Función de Autocorrelación (ACF)')\n",
    "plot_pacf(dengue_series.dropna(), ax=ax2, lags=52)\n",
    "ax2.set_title('Función de Autocorrelación Parcial (PACF)')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- IMPLEMENTACIÓN DE LSTM CON PYTORCH Y OPTUNA ---\n",
    "\n",
    "print(\"\\n--- Iniciando optimización de hiperparámetros para LSTM con PyTorch y Optuna ---\")\n",
    "\n",
    "# 7. Preparación de Datos para LSTM\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(dengue_series.values.reshape(-1, 1))\n",
    "\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset) - time_step - 1):\n",
    "        a = dataset[i:(i + time_step), 0]\n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + time_step, 0])\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "\n",
    "# 8. Definición del Modelo LSTM en PyTorch\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layer_size, dropout_rate, output_size=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_layer_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.linear = nn.Linear(hidden_layer_size, output_size)\n",
    "\n",
    "    def forward(self, input_seq):\n",
    "        lstm_out, _ = self.lstm(input_seq)\n",
    "        last_output = lstm_out[:, -1, :]\n",
    "        predictions = self.linear(self.dropout(last_output))\n",
    "        return predictions\n",
    "\n",
    "# 9. Definición de la Función Objetivo de Optuna\n",
    "def objective(trial):\n",
    "    time_step = trial.suggest_int('time_step', 10, 60)\n",
    "    hidden_layer_size = trial.suggest_int('hidden_layer_size', 32, 200, step=16)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    optimizer_name = trial.suggest_categorical('optimizer', ['Adam', 'RMSprop'])\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "\n",
    "    X, y = create_dataset(scaled_data, time_step)\n",
    "    if len(X) < 2: return float('inf')\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "    X_train_tensor = torch.from_numpy(X_train).float().view(-1, time_step, 1)\n",
    "    y_train_tensor = torch.from_numpy(y_train).float().view(-1, 1)\n",
    "    X_val_tensor = torch.from_numpy(X_val).float().view(-1, time_step, 1)\n",
    "    y_val_tensor = torch.from_numpy(y_val).float().view(-1, 1)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = LSTMModel(input_size=1, hidden_layer_size=hidden_layer_size, dropout_rate=dropout_rate)\n",
    "    loss_function = nn.MSELoss()\n",
    "    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    epochs, patience, best_val_loss, epochs_no_improve = 100, 10, float('inf'), 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for seq, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(seq)\n",
    "            single_loss = loss_function(y_pred, labels)\n",
    "            single_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for seq, labels in val_loader:\n",
    "                val_loss += loss_function(model(seq), labels).item()\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "        if epochs_no_improve == patience: break\n",
    "\n",
    "    model.load_state_dict(torch.load('best_model.pth'))\n",
    "    model.eval()\n",
    "    all_preds = [model(seq).detach().numpy() for seq, _ in val_loader]\n",
    "    \n",
    "    predictions = np.concatenate(all_preds)\n",
    "    predictions_inv = scaler.inverse_transform(predictions)\n",
    "    y_val_inv = scaler.inverse_transform(y_val.reshape(-1, 1))\n",
    "    \n",
    "    return np.sqrt(mean_squared_error(y_val_inv, predictions_inv))\n",
    "\n",
    "# 10. Ejecución del Estudio de Optuna\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50, timeout=600)\n",
    "\n",
    "# 11. Resultados y Entrenamiento del Modelo Final\n",
    "best_params = study.best_params\n",
    "print(f\"\\nMejor trial: RMSE = {study.best_value:.4f}\")\n",
    "print(\"Mejores hiperparámetros:\", best_params)\n",
    "\n",
    "print(\"\\n--- Entrenando el modelo final con los mejores hiperparámetros... ---\")\n",
    "time_step = best_params['time_step']\n",
    "X, y = create_dataset(scaled_data, time_step)\n",
    "X_tensor = torch.from_numpy(X).float().view(-1, time_step, 1)\n",
    "y_tensor = torch.from_numpy(y).float().view(-1, 1)\n",
    "final_loader = DataLoader(TensorDataset(X_tensor, y_tensor), batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "final_model = LSTMModel(1, best_params['hidden_layer_size'], best_params['dropout_rate'])\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = getattr(optim, best_params['optimizer'])(final_model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "for i in range(150):\n",
    "    for seq, labels in final_loader:\n",
    "        optimizer.zero_grad()\n",
    "        final_model.train()\n",
    "        y_pred = final_model(seq)\n",
    "        single_loss = loss_function(y_pred, labels)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "    if (i+1)%25 == 0: print(f'Epoch: {i+1:3} loss: {single_loss.item():.8f}')\n",
    "\n",
    "# --- PREDICCIÓN PARA 2022 Y GENERACIÓN DE SUBMISSION ---\n",
    "\n",
    "print(\"\\n--- Prediciendo para 2022 y generando archivo de submission... ---\")\n",
    "\n",
    "# 12. Predicción iterativa para las 52 semanas de 2022\n",
    "n_weeks_2022 = 52\n",
    "# Usar los últimos 'time_step' días del training data como secuencia inicial\n",
    "test_input = scaled_data[-time_step:].flatten().tolist()\n",
    "predictions_2022_scaled = []\n",
    "\n",
    "final_model.eval()\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_weeks_2022):\n",
    "        seq = torch.FloatTensor(test_input[-time_step:]).view(1, time_step, 1)\n",
    "        next_pred = final_model(seq).item()\n",
    "        predictions_2022_scaled.append(next_pred)\n",
    "        test_input.append(next_pred)\n",
    "\n",
    "# 13. Procesar las predicciones\n",
    "# Desescalar las predicciones\n",
    "predictions_2022_total = scaler.inverse_transform(np.array(predictions_2022_scaled).reshape(-1, 1))\n",
    "# Asegurar que los casos sean enteros no negativos\n",
    "predictions_2022_total = np.maximum(0, predictions_2022_total.round()).astype(int).flatten()\n",
    "\n",
    "# Calcular el número de barrios para distribuir la predicción\n",
    "num_neighborhoods = df_train['id_bar'].nunique()\n",
    "# Distribuir el total predicho equitativamente entre los barrios\n",
    "predictions_per_neighborhood = predictions_2022_total / num_neighborhoods\n",
    "# Redondear a entero\n",
    "predictions_per_neighborhood = predictions_per_neighborhood.round().astype(int)\n",
    "\n",
    "# 14. Crear el archivo de submission\n",
    "try:\n",
    "    df_test = pd.read_parquet(os.path.join(DATA_DIR, 'df_test.parquet'))\n",
    "    # Crear un mapa de semana -> predicción por barrio\n",
    "    week_to_prediction_map = {i + 1: pred for i, pred in enumerate(predictions_per_neighborhood)}\n",
    "    \n",
    "    # Asignar las predicciones al dataframe de test\n",
    "    df_test['dengue'] = df_test['semana'].map(week_to_prediction_map)\n",
    "    \n",
    "    # Crear el dataframe final para el submission\n",
    "    submission_df = df_test[['id', 'dengue']].copy()\n",
    "    \n",
    "    # Guardar el archivo\n",
    "    submission_df.to_csv('submission.csv', index=False)\n",
    "    \n",
    "    print(\"\\nArchivo 'submission.csv' generado exitosamente.\")\n",
    "    print(submission_df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: El archivo 'df_test.parquet' no se encontró en el directorio '{DATA_DIR}'. No se pudo generar el archivo de submission.\")\n",
    "\n",
    "print(\"\\nProceso completado.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
