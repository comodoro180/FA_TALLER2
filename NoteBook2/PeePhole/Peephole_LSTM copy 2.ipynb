{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c57cccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# ANÁLISIS DE SERIES DE TIEMPO Y PREDICCIÓN DE DENGUE CON PEEPHOLE LSTM\n",
    "# =============================================================================\n",
    "\n",
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from datetime import datetime, timedelta\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# PyTorch y LSTM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Optuna para optimización de hiperparámetros\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "\n",
    "# Configuración\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b26e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FUNCIONES AUXILIARES PARA MANEJO DE NaN\n",
    "# =============================================================================\n",
    "\n",
    "def clean_and_impute_data(df, strategy='median'):\n",
    "    \"\"\"\n",
    "    Limpia y maneja valores NaN en el DataFrame\n",
    "    \"\"\"\n",
    "    #print(f\"Limpiando datos - Forma inicial: {df.shape}\")\n",
    "    \n",
    "    # Verificar valores NaN\n",
    "    nan_counts = df.isnull().sum()\n",
    "    total_nans = nan_counts.sum()\n",
    "    \n",
    "    if total_nans > 0:\n",
    "        print(f\"Valores NaN encontrados: {total_nans}\")\n",
    "        print(\"Por columna:\")\n",
    "        for col, count in nan_counts[nan_counts > 0].items():\n",
    "            print(f\"  {col}: {count} ({count/len(df)*100:.2f}%)\")\n",
    "        \n",
    "        # Separar columnas numéricas y categóricas\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        categorical_cols = df.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "        \n",
    "        # Imputar columnas numéricas\n",
    "        if len(numeric_cols) > 0:\n",
    "            imputer_numeric = SimpleImputer(strategy=strategy)\n",
    "            df[numeric_cols] = imputer_numeric.fit_transform(df[numeric_cols])\n",
    "        \n",
    "        # Imputar columnas categóricas\n",
    "        if len(categorical_cols) > 0:\n",
    "            imputer_categorical = SimpleImputer(strategy='most_frequent')\n",
    "            df[categorical_cols] = imputer_categorical.fit_transform(df[categorical_cols])\n",
    "        \n",
    "        print(f\"✓ Valores NaN imputados usando estrategia '{strategy}'\")\n",
    "    \n",
    "    # Verificar valores infinitos\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    inf_counts = {}\n",
    "    for col in numeric_cols:\n",
    "        inf_count = np.isinf(df[col]).sum()\n",
    "        if inf_count > 0:\n",
    "            inf_counts[col] = inf_count\n",
    "            # Reemplazar infinitos con NaN y luego imputar\n",
    "            df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "    \n",
    "    if inf_counts:\n",
    "        print(f\"Valores infinitos encontrados y corregidos: {inf_counts}\")\n",
    "    \n",
    "    # Verificación final\n",
    "    final_nans = df.isnull().sum().sum()\n",
    "    final_infs = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()\n",
    "    \n",
    "    #print(f\"✓ Limpieza completada - NaN restantes: {final_nans}, Inf restantes: {final_infs}\")\n",
    "    #print(f\"Forma final: {df.shape}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def validate_tensor_data(tensor, name=\"tensor\"):\n",
    "    \"\"\"\n",
    "    Valida que un tensor no contenga NaN o Inf\n",
    "    \"\"\"\n",
    "    if torch.isnan(tensor).any():\n",
    "        print(f\"¡ADVERTENCIA! {name} contiene valores NaN\")\n",
    "        return False\n",
    "    if torch.isinf(tensor).any():\n",
    "        print(f\"¡ADVERTENCIA! {name} contiene valores infinitos\")\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1. ANÁLISIS EXPLORATORIO DE DATOS - SERIES DE TIEMPO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ANÁLISIS EXPLORATORIO DE DATOS - SERIES DE TIEMPO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "df = pd.read_parquet(\"../../Datos/df_train.parquet\")\n",
    "# Cargar datos\n",
    "print(\"Cargando datos...\")\n",
    "df_train = df[df['anio'] < 2021]  # Filtrar por años recientes\n",
    "df_test = df[df['anio'] >= 2021]  # Datos de prueba para 2021 en adelante\n",
    "\n",
    "#df_train = pd.read_parquet(\"../../Datos/df_train.parquet\")\n",
    "#df_test = pd.read_parquet(\"../../Datos/df_test.parquet\")\n",
    "sample_submission = pd.read_csv(\"../../Datos/sample_submission.csv\")\n",
    "\n",
    "print(f\"Datos de entrenamiento: {df_train.shape}\")\n",
    "print(f\"Datos de prueba: {df_test.shape}\")\n",
    "print(f\"Archivo de submisión: {sample_submission.shape}\")\n",
    "\n",
    "# LIMPIEZA INICIAL DE DATOS\n",
    "print(\"\\n--- LIMPIEZA INICIAL DE DATOS ---\")\n",
    "df_train = clean_and_impute_data(df_train.copy())\n",
    "df_test = clean_and_impute_data(df_test.copy())\n",
    "\n",
    "# Exploración inicial\n",
    "print(\"\\n--- ESTRUCTURA DE DATOS DE ENTRENAMIENTO ---\")\n",
    "print(df_train.info())\n",
    "print(\"\\nPrimeras filas:\")\n",
    "print(df_train.head())\n",
    "\n",
    "print(\"\\n--- ESTADÍSTICAS DESCRIPTIVAS ---\")\n",
    "print(df_train.describe())\n",
    "\n",
    "# Crear columna de fecha para análisis temporal\n",
    "def create_date_column(df):\n",
    "    \"\"\"Crear columna de fecha a partir de año y semana\"\"\"\n",
    "    df = df.copy()\n",
    "    # Crear fecha usando el primer día de cada semana\n",
    "    df['fecha'] = pd.to_datetime(df['anio'].astype(str) + '-W' + df['semana'].astype(str) + '-1', \n",
    "                                format='%Y-W%U-%w')\n",
    "    return df\n",
    "\n",
    "df_train = create_date_column(df_train)\n",
    "df_test = create_date_column(df_test)\n",
    "\n",
    "print(f\"\\nRango temporal entrenamiento: {df_train['fecha'].min()} a {df_train['fecha'].max()}\")\n",
    "print(f\"Rango temporal prueba: {df_test['fecha'].min()} a {df_test['fecha'].max()}\")\n",
    "\n",
    "# Análisis por barrio y tiempo\n",
    "print(\"\\n--- ANÁLISIS TEMPORAL POR BARRIO ---\")\n",
    "print(f\"Número de barrios únicos en train: {df_train['id_bar'].nunique()}\")\n",
    "print(f\"Número de barrios únicos en test: {df_test['id_bar'].nunique()}\")\n",
    "\n",
    "# Verificar continuidad temporal\n",
    "barrios_comunes = set(df_train['id_bar'].unique()) & set(df_test['id_bar'].unique())\n",
    "print(f\"Barrios comunes entre train y test: {len(barrios_comunes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56f0a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# VISUALIZACIONES EXPLORATORIAS\n",
    "# =============================================================================\n",
    "\n",
    "# 1. Serie temporal de dengue agregada\n",
    "print(\"\\n--- GENERANDO VISUALIZACIONES ---\")\n",
    "\n",
    "# Agregar por fecha\n",
    "dengue_temporal = df_train.groupby('fecha')['dengue'].agg(['sum', 'mean', 'count']).reset_index()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Análisis Temporal de Dengue', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Serie temporal total\n",
    "axes[0,0].plot(dengue_temporal['fecha'], dengue_temporal['sum'], linewidth=2, color='red')\n",
    "axes[0,0].set_title('Casos Totales de Dengue por Semana')\n",
    "axes[0,0].set_xlabel('Fecha')\n",
    "axes[0,0].set_ylabel('Casos Totales')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Serie temporal promedio\n",
    "axes[0,1].plot(dengue_temporal['fecha'], dengue_temporal['mean'], linewidth=2, color='blue')\n",
    "axes[0,1].set_title('Casos Promedio de Dengue por Semana')\n",
    "axes[0,1].set_xlabel('Fecha')\n",
    "axes[0,1].set_ylabel('Casos Promedio')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Distribución de dengue\n",
    "axes[1,0].hist(df_train['dengue'], bins=50, alpha=0.7, color='green', edgecolor='black')\n",
    "axes[1,0].set_title('Distribución de Casos de Dengue')\n",
    "axes[1,0].set_xlabel('Casos de Dengue')\n",
    "axes[1,0].set_ylabel('Frecuencia')\n",
    "\n",
    "# Boxplot por año\n",
    "df_train.boxplot(column='dengue', by='anio', ax=axes[1,1])\n",
    "axes[1,1].set_title('Distribución de Dengue por Año')\n",
    "axes[1,1].set_xlabel('Año')\n",
    "axes[1,1].set_ylabel('Casos de Dengue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Análisis estacional\n",
    "print(\"Analizando patrones estacionales...\")\n",
    "\n",
    "# Agregar mes y trimestre\n",
    "df_train['mes'] = df_train['fecha'].dt.month\n",
    "df_train['trimestre'] = df_train['fecha'].dt.quarter\n",
    "\n",
    "# Patrones estacionales\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Por semana del año\n",
    "dengue_por_semana = df_train.groupby('semana')['dengue'].mean()\n",
    "axes[0].plot(dengue_por_semana.index, dengue_por_semana.values, marker='o', linewidth=2)\n",
    "axes[0].set_title('Patrón Estacional por Semana del Año')\n",
    "axes[0].set_xlabel('Semana')\n",
    "axes[0].set_ylabel('Casos Promedio de Dengue')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Por mes\n",
    "dengue_por_mes = df_train.groupby('mes')['dengue'].mean()\n",
    "axes[1].bar(dengue_por_mes.index, dengue_por_mes.values, color='coral', alpha=0.8)\n",
    "axes[1].set_title('Patrón Estacional por Mes')\n",
    "axes[1].set_xlabel('Mes')\n",
    "axes[1].set_ylabel('Casos Promedio de Dengue')\n",
    "axes[1].set_xticks(range(1, 13))\n",
    "\n",
    "# Por trimestre\n",
    "dengue_por_trimestre = df_train.groupby('trimestre')['dengue'].mean()\n",
    "axes[2].bar(dengue_por_trimestre.index, dengue_por_trimestre.values, color='lightblue', alpha=0.8)\n",
    "axes[2].set_title('Patrón Estacional por Trimestre')\n",
    "axes[2].set_xlabel('Trimestre')\n",
    "axes[2].set_ylabel('Casos Promedio de Dengue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 3. Análisis de correlaciones con variables climáticas\n",
    "print(\"Analizando correlaciones con variables climáticas...\")\n",
    "\n",
    "# Variables climáticas\n",
    "# vars_clima = ['lluvia_mean', 'lluvia_var', 'lluvia_max', 'lluvia_min',\n",
    "#               'temperatura_mean', 'temperatura_var', 'temperatura_max', 'temperatura_min']\n",
    "\n",
    "vars_clima = ['lluvia_mean', #'lluvia_var', 'lluvia_max',\n",
    "              'temperatura_mean', #'temperatura_var', 'temperatura_max', 'temperatura_min'\n",
    "              ]\n",
    "\n",
    "# Matriz de correlación\n",
    "corr_matrix = df_train[vars_clima + ['dengue']].corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.3f', cbar_kws={'label': 'Correlación'})\n",
    "plt.title('Matriz de Correlación - Variables Climáticas vs Dengue', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Series temporales por barrio (muestra de algunos barrios)\n",
    "print(\"Analizando series temporales por barrio...\")\n",
    "\n",
    "# Seleccionar barrios con más datos\n",
    "barrios_datos = df_train.groupby('id_bar').size().sort_values(ascending=False)\n",
    "top_barrios = barrios_datos.head(6).index\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, barrio in enumerate(top_barrios):\n",
    "    data_barrio = df_train[df_train['id_bar'] == barrio].sort_values('fecha')\n",
    "    axes[i].plot(data_barrio['fecha'], data_barrio['dengue'], linewidth=2)\n",
    "    axes[i].set_title(f'Barrio {barrio} (n={len(data_barrio)})')\n",
    "    axes[i].set_xlabel('Fecha')\n",
    "    axes[i].set_ylabel('Casos de Dengue')\n",
    "    axes[i].tick_params(axis='x', rotation=45)\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Series Temporales de Dengue por Barrio', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Análisis de tendencias y autocorrelación\n",
    "print(\"Analizando tendencias y autocorrelación...\")\n",
    "\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Preparar serie temporal agregada\n",
    "serie_dengue = dengue_temporal.set_index('fecha')['sum'].asfreq('W-MON')\n",
    "\n",
    "# Descomposición estacional\n",
    "fig, axes = plt.subplots(4, 1, figsize=(15, 12))\n",
    "\n",
    "# Serie original\n",
    "axes[0].plot(serie_dengue.index, serie_dengue.values, linewidth=2, color='black')\n",
    "axes[0].set_title('Serie Original de Dengue', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Casos')\n",
    "\n",
    "# Descomposición\n",
    "decomp = seasonal_decompose(serie_dengue.dropna(), model='additive', period=52)\n",
    "\n",
    "axes[1].plot(decomp.trend.index, decomp.trend.values, linewidth=2, color='blue')\n",
    "axes[1].set_title('Tendencia', fontsize=12)\n",
    "axes[1].set_ylabel('Tendencia')\n",
    "\n",
    "axes[2].plot(decomp.seasonal.index, decomp.seasonal.values, linewidth=2, color='green')\n",
    "axes[2].set_title('Estacionalidad', fontsize=12)\n",
    "axes[2].set_ylabel('Estacional')\n",
    "\n",
    "axes[3].plot(decomp.resid.index, decomp.resid.values, linewidth=2, color='red')\n",
    "axes[3].set_title('Residuos', fontsize=12)\n",
    "axes[3].set_ylabel('Residuos')\n",
    "axes[3].set_xlabel('Fecha')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Autocorrelación\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "plot_acf(serie_dengue.dropna(), lags=40, ax=axes[0], title='Autocorrelación (ACF)')\n",
    "plot_pacf(serie_dengue.dropna(), lags=40, ax=axes[1], title='Autocorrelación Parcial (PACF)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMEN DEL ANÁLISIS EXPLORATORIO\")\n",
    "print(\"=\"*70)\n",
    "print(f\"✓ Datos de entrenamiento: {df_train.shape[0]:,} observaciones\")\n",
    "print(f\"✓ Período de entrenamiento: {df_train['anio'].min()}-{df_train['anio'].max()}\")\n",
    "print(f\"✓ Número de barrios: {df_train['id_bar'].nunique()}\")\n",
    "print(f\"✓ Variables climáticas disponibles: {len(vars_clima)}\")\n",
    "print(f\"✓ Correlación más alta con dengue: {corr_matrix['dengue'].abs().sort_values(ascending=False).index[1]} \"\n",
    "      f\"({corr_matrix['dengue'].abs().sort_values(ascending=False).iloc[1]:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. IMPLEMENTACIÓN DE PEEPHOLE LSTM CON PYTORCH\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"IMPLEMENTACIÓN DE PEEPHOLE LSTM CON PYTORCH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Configurar dispositivo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Dispositivo utilizado: {device}\")\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    \"\"\"Dataset personalizado para series de tiempo con validación de NaN\"\"\"\n",
    "    \n",
    "    def __init__(self, data, target_col, feature_cols, seq_length, target_length=1):\n",
    "        self.data = data.copy()\n",
    "        self.target_col = target_col\n",
    "        self.feature_cols = feature_cols\n",
    "        self.seq_length = seq_length\n",
    "        self.target_length = target_length\n",
    "        \n",
    "        # Preparar datos por barrio\n",
    "        self.sequences = []\n",
    "        self.targets = []\n",
    "        \n",
    "        #print(f\"Creando dataset con {len(feature_cols)} características y secuencias de longitud {seq_length}\")\n",
    "        \n",
    "        valid_sequences = 0\n",
    "        invalid_sequences = 0\n",
    "        \n",
    "        for id_bar in data['id_bar'].unique():\n",
    "            barrio_data = data[data['id_bar'] == id_bar].sort_values('fecha')\n",
    "            \n",
    "            if len(barrio_data) >= seq_length + target_length:\n",
    "                features = barrio_data[feature_cols].values\n",
    "                targets = barrio_data[target_col].values\n",
    "                \n",
    "                # Verificar que no haya NaN en las características\n",
    "                if np.isnan(features).any():\n",
    "                    print(f\"Advertencia: NaN encontrado en características del barrio {id_bar}\")\n",
    "                    continue\n",
    "                \n",
    "                if np.isnan(targets).any():\n",
    "                    print(f\"Advertencia: NaN encontrado en targets del barrio {id_bar}\")\n",
    "                    continue\n",
    "                \n",
    "                for i in range(len(barrio_data) - seq_length - target_length + 1):\n",
    "                    seq_x = features[i:i+seq_length]\n",
    "                    seq_y = targets[i+seq_length:i+seq_length+target_length]\n",
    "                    \n",
    "                    # Validación adicional de NaN\n",
    "                    if not (np.isnan(seq_x).any() or np.isnan(seq_y).any() or \n",
    "                           np.isinf(seq_x).any() or np.isinf(seq_y).any()):\n",
    "                        self.sequences.append(seq_x)\n",
    "                        self.targets.append(seq_y)\n",
    "                        valid_sequences += 1\n",
    "                    else:\n",
    "                        invalid_sequences += 1\n",
    "        \n",
    "        #print(f\"✓ Secuencias válidas: {valid_sequences}\")\n",
    "        if invalid_sequences > 0:\n",
    "            print(f\"⚠ Secuencias inválidas descartadas: {invalid_sequences}\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = torch.FloatTensor(self.sequences[idx])\n",
    "        target = torch.FloatTensor(self.targets[idx])\n",
    "        \n",
    "        # Validación final de tensores\n",
    "        if not (validate_tensor_data(seq, f\"sequence_{idx}\") and \n",
    "                validate_tensor_data(target, f\"target_{idx}\")):\n",
    "            # Reemplazar con tensor de ceros si hay problemas\n",
    "            seq = torch.zeros_like(seq)\n",
    "            target = torch.zeros_like(target)\n",
    "        \n",
    "        return seq, target\n",
    "\n",
    "class PeepholeLSTM(nn.Module):\n",
    "    \"\"\"Implementación de Peephole LSTM con validación de NaN\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.1):\n",
    "        super(PeepholeLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Capas LSTM con peephole connections\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, \n",
    "                           batch_first=True, dropout=dropout if num_layers > 1 else 0)\n",
    "        \n",
    "        # Capas fully connected\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        \n",
    "        # Primera capa\n",
    "        self.fc_layers.append(nn.Linear(hidden_size, hidden_size // 2))\n",
    "        self.fc_layers.append(nn.ReLU())\n",
    "        self.fc_layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        # Segunda capa\n",
    "        self.fc_layers.append(nn.Linear(hidden_size // 2, hidden_size // 4))\n",
    "        self.fc_layers.append(nn.ReLU())\n",
    "        self.fc_layers.append(nn.Dropout(dropout))\n",
    "        \n",
    "        # Capa de salida\n",
    "        self.fc_layers.append(nn.Linear(hidden_size // 4, output_size))\n",
    "        \n",
    "        # Normalización por lotes\n",
    "        self.batch_norm = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Validar entrada\n",
    "        if not validate_tensor_data(x, \"input\"):\n",
    "            print(\"Entrada contiene valores inválidos, reemplazando con ceros\")\n",
    "            x = torch.zeros_like(x)\n",
    "        \n",
    "        # Inicializar estados ocultos\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # LSTM forward\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Tomar la última salida\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        # Validar salida LSTM\n",
    "        if not validate_tensor_data(lstm_out, \"lstm_output\"):\n",
    "            print(\"Salida LSTM contiene valores inválidos\")\n",
    "            lstm_out = torch.zeros_like(lstm_out)\n",
    "        \n",
    "        # Normalización por lotes\n",
    "        if lstm_out.size(0) > 1:  # BatchNorm requiere más de 1 muestra\n",
    "            lstm_out = self.batch_norm(lstm_out)\n",
    "        \n",
    "        # Capas fully connected\n",
    "        out = lstm_out\n",
    "        for i, layer in enumerate(self.fc_layers):\n",
    "            out = layer(out)\n",
    "            # Validar después de cada capa\n",
    "            if not validate_tensor_data(out, f\"fc_layer_{i}\"):\n",
    "                print(f\"Salida de capa FC {i} contiene valores inválidos\")\n",
    "                out = torch.clamp(out, -1e6, 1e6)  # Clamp valores extremos\n",
    "        \n",
    "        return out\n",
    "\n",
    "def prepare_data_for_model(df_train, df_test, feature_cols, target_col, seq_length):\n",
    "    \"\"\"Preparar datos para el modelo con manejo robusto de NaN\"\"\"\n",
    "    \n",
    "    #print(\"Preparando datos para el modelo...\")\n",
    "    \n",
    "    # Limpiar datos antes de procesar\n",
    "    # Filtrar datos de entrenamiento y prueba por año\n",
    "    df_train_clean = clean_and_impute_data(df_train[df_train['anio'] < 2021].copy())\n",
    "    df_test_clean = clean_and_impute_data(df_train[df_train['anio'] >= 2021].copy())\n",
    "    \n",
    "    # Combinar datos para normalización consistente\n",
    "    combined_data = pd.concat([df_train_clean, df_test_clean], ignore_index=True)\n",
    "    \n",
    "    # Verificar que las columnas existan\n",
    "    missing_features = [col for col in feature_cols if col not in combined_data.columns]\n",
    "    if missing_features:\n",
    "        print(f\"⚠ Características faltantes: {missing_features}\")\n",
    "        feature_cols = [col for col in feature_cols if col in combined_data.columns]\n",
    "        print(f\"Usando características disponibles: {feature_cols}\")\n",
    "    \n",
    "    # Normalización de características con manejo de NaN\n",
    "    #print(\"Normalizando características...\")\n",
    "    scaler_features = StandardScaler()\n",
    "    scaler_target = StandardScaler()\n",
    "    \n",
    "    # Verificar datos antes de normalizar\n",
    "    #print(f\"Datos antes de normalizar - NaN en features: {combined_data[feature_cols].isnull().sum().sum()}\")\n",
    "    #print(f\"Datos antes de normalizar - NaN en target: {combined_data[target_col].isnull().sum()}\")\n",
    "    \n",
    "    # Ajustar escaladores con datos combinados\n",
    "    try:\n",
    "        combined_data[feature_cols] = scaler_features.fit_transform(combined_data[feature_cols])\n",
    "        combined_data[[target_col]] = scaler_target.fit_transform(combined_data[[target_col]])\n",
    "    except Exception as e:\n",
    "        print(f\"Error durante normalización: {e}\")\n",
    "        # Limpiar nuevamente si hay problemas\n",
    "        combined_data = clean_and_impute_data(combined_data)\n",
    "        combined_data[feature_cols] = scaler_features.fit_transform(combined_data[feature_cols])\n",
    "        combined_data[[target_col]] = scaler_target.fit_transform(combined_data[[target_col]])\n",
    "    \n",
    "    # Verificar después de normalizar\n",
    "    #print(f\"Datos después de normalizar - NaN en features: {combined_data[feature_cols].isnull().sum().sum()}\")\n",
    "    #print(f\"Datos después de normalizar - NaN en target: {combined_data[target_col].isnull().sum()}\")\n",
    "    \n",
    "    # Separar nuevamente\n",
    "    train_scaled = combined_data.iloc[:len(df_train_clean)].copy()\n",
    "    test_scaled = combined_data.iloc[len(df_train_clean):].copy()\n",
    "    \n",
    "    # Crear datasets\n",
    "    train_dataset = TimeSeriesDataset(train_scaled, target_col, feature_cols, seq_length)\n",
    "    test_dataset = TimeSeriesDataset(test_scaled, target_col, feature_cols, seq_length)\n",
    "    \n",
    "    #print(f\"Secuencias de entrenamiento: {len(train_dataset)}\")\n",
    "    #print(f\"Secuencias de prueba: {len(test_dataset)}\")\n",
    "    \n",
    "    return train_dataset, test_dataset, scaler_features, scaler_target\n",
    "\n",
    "# Definir características para el modelo\n",
    "# feature_cols = ['ESTRATO', 'area_barrio', 'concentraciones', 'vivienda', \n",
    "#                 'equipesado', 'sumideros', 'maquina'] + vars_clima\n",
    "\n",
    "feature_cols = ['area_barrio', 'concentraciones',  \n",
    "                'equipesado'] + vars_clima\n",
    "\n",
    "target_col = 'dengue'\n",
    "\n",
    "print(f\"Características seleccionadas: {len(feature_cols)}\")\n",
    "print(\"Features:\", feature_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ec4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. OPTIMIZACIÓN DE HIPERPARÁMETROS CON OPTUNA\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"OPTIMIZACIÓN DE HIPERPARÁMETROS CON OPTUNA\") \n",
    "print(\"=\"*70)\n",
    "\n",
    "def safe_training_step(model, batch_x, batch_y, optimizer, criterion, device):\n",
    "    \"\"\"Paso de entrenamiento seguro con manejo de NaN\"\"\"\n",
    "    \n",
    "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "    \n",
    "    # Validar entrada\n",
    "    if not (validate_tensor_data(batch_x, \"batch_x\") and validate_tensor_data(batch_y, \"batch_y\")):\n",
    "        return float('inf')\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    try:\n",
    "        outputs = model(batch_x)\n",
    "        \n",
    "        # Validar salidas\n",
    "        if not validate_tensor_data(outputs, \"outputs\"):\n",
    "            return float('inf')\n",
    "        \n",
    "        loss = criterion(outputs, batch_y)\n",
    "        \n",
    "        # Validar pérdida\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            print(\"Loss contiene NaN o Inf, saltando batch\")\n",
    "            return float('inf')\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Verificar gradientes\n",
    "        total_norm = 0\n",
    "        for p in model.parameters():\n",
    "            if p.grad is not None:\n",
    "                param_norm = p.grad.data.norm(2)\n",
    "                total_norm += param_norm.item() ** 2\n",
    "        total_norm = total_norm ** (1. / 2)\n",
    "        \n",
    "        if np.isnan(total_norm) or np.isinf(total_norm):\n",
    "            print(\"Gradientes contienen NaN o Inf, saltando optimización\")\n",
    "            optimizer.zero_grad()\n",
    "            return float('inf')\n",
    "        \n",
    "        optimizer.step()\n",
    "        return loss.item()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error en paso de entrenamiento: {e}\")\n",
    "        optimizer.zero_grad()\n",
    "        return float('inf')\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"Función objetivo para Optuna con manejo de errores mejorado\"\"\"\n",
    "    \n",
    "    # Hiperparámetros a optimizar\n",
    "    seq_length = trial.suggest_int('seq_length', 8, 48)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 96, step=16)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.4)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)\n",
    "    batch_size = trial.suggest_int('batch_size', 16, 64, step=16)\n",
    "    \n",
    "    try:\n",
    "        # Preparar datos\n",
    "        train_dataset, val_dataset, scaler_features, scaler_target = prepare_data_for_model(\n",
    "            df_train, df_test, feature_cols, target_col, seq_length)\n",
    "        \n",
    "        if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
    "            print(\"Dataset vacío, retornando penalización\")\n",
    "            return float('inf')\n",
    "        \n",
    "        # DataLoaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "        \n",
    "        # Modelo\n",
    "        model = PeepholeLSTM(\n",
    "            input_size=len(feature_cols),\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            output_size=1,\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "        \n",
    "        # Optimizador y función de pérdida\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "        criterion = nn.MSELoss()\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.7)\n",
    "        \n",
    "        # Entrenamiento\n",
    "        model.train()\n",
    "        epochs = 50  # Reducido para optimización más rápida\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        patience_count = 0\n",
    "        max_patience = 25\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            valid_batches = 0\n",
    "            \n",
    "            for batch_x, batch_y in train_loader:\n",
    "                loss = safe_training_step(model, batch_x, batch_y, optimizer, criterion, device)\n",
    "                if loss != float('inf'):\n",
    "                    total_loss += loss\n",
    "                    valid_batches += 1\n",
    "            \n",
    "            if valid_batches == 0:\n",
    "                print(\"No hay batches válidos, terminando trial\")\n",
    "                return float('inf')\n",
    "            \n",
    "            avg_train_loss = total_loss / valid_batches\n",
    "            \n",
    "            # Validación\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            valid_val_batches = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for batch_x, batch_y in val_loader:\n",
    "                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                    \n",
    "                    if validate_tensor_data(batch_x) and validate_tensor_data(batch_y):\n",
    "                        outputs = model(batch_x)\n",
    "                        if validate_tensor_data(outputs):\n",
    "                            loss = criterion(outputs, batch_y)\n",
    "                            if not (torch.isnan(loss) or torch.isinf(loss)):\n",
    "                                val_loss += loss.item()\n",
    "                                valid_val_batches += 1\n",
    "            \n",
    "            if valid_val_batches == 0:\n",
    "                return float('inf')\n",
    "            \n",
    "            val_loss /= valid_val_batches\n",
    "            scheduler.step(val_loss)\n",
    "            model.train()\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_count = 0\n",
    "            else:\n",
    "                patience_count += 1\n",
    "                if patience_count >= max_patience:\n",
    "                    break\n",
    "            \n",
    "            # Reporte intermedio para pruning\n",
    "            if epoch % 10 == 0:\n",
    "                trial.report(val_loss, epoch)\n",
    "                if trial.should_prune():\n",
    "                    raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        return best_val_loss\n",
    "        \n",
    "    except optuna.exceptions.TrialPruned:\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error en trial: {e}\")\n",
    "        return float('inf')\n",
    "\n",
    "# Ejecutar optimización\n",
    "print(\"Iniciando optimización de hiperparámetros...\")\n",
    "study = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner())\n",
    "\n",
    "# Reducir número de trials para ejemplo\n",
    "n_trials = 40\n",
    "print(f\"Ejecutando {n_trials} trials...\")\n",
    "\n",
    "study.optimize(objective, n_trials=n_trials)  # 1 hora máximo\n",
    "\n",
    "# Resultados de la optimización\n",
    "print(\"\\n--- RESULTADOS DE LA OPTIMIZACIÓN ---\")\n",
    "print(f\"Número de trials completados: {len(study.trials)}\")\n",
    "print(f\"Mejor valor: {study.best_value:.6f}\")\n",
    "print(\"Mejores hiperparámetros:\")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4aa9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. ENTRENAMIENTO FINAL CON TODOS LOS DATOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENTRENAMIENTO FINAL CON TODOS LOS DATOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Usar mejores hiperparámetros\n",
    "best_params = study.best_params\n",
    "print(\"Usando mejores hiperparámetros encontrados...\")\n",
    "\n",
    "# Combinar todos los datos para entrenamiento final\n",
    "all_data = df_train.copy() #pd.concat([df_train, df_test], ignore_index=True)\n",
    "all_data = clean_and_impute_data(all_data)\n",
    "print(f\"Datos totales para entrenamiento: {all_data.shape}\")\n",
    "\n",
    "# Preparar datos finales\n",
    "seq_length = best_params['seq_length']\n",
    "scaler_features_final = StandardScaler()\n",
    "scaler_target_final = StandardScaler()\n",
    "\n",
    "# Normalizar todos los datos\n",
    "all_data[feature_cols] = scaler_features_final.fit_transform(all_data[feature_cols])\n",
    "all_data[[target_col]] = scaler_target_final.fit_transform(all_data[[target_col]])\n",
    "\n",
    "# Dataset final\n",
    "final_dataset = TimeSeriesDataset(all_data, target_col, feature_cols, seq_length)\n",
    "final_loader = DataLoader(final_dataset, batch_size=best_params['batch_size'], shuffle=True)\n",
    "\n",
    "# Modelo final\n",
    "final_model = PeepholeLSTM(\n",
    "    input_size=len(feature_cols),\n",
    "    hidden_size=best_params['hidden_size'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    output_size=1,\n",
    "    dropout=best_params['dropout']\n",
    ").to(device)\n",
    "\n",
    "# Optimizador y criterio\n",
    "final_optimizer = optim.Adam(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=1e-5)\n",
    "final_criterion = nn.MSELoss()\n",
    "final_scheduler = optim.lr_scheduler.ReduceLROnPlateau(final_optimizer, 'min', patience=10, factor=0.7)\n",
    "\n",
    "# Entrenamiento final\n",
    "print(\"Iniciando entrenamiento final...\")\n",
    "final_epochs = 200\n",
    "train_losses = []\n",
    "\n",
    "final_model.train()\n",
    "for epoch in range(final_epochs):\n",
    "    total_loss = 0\n",
    "    valid_batches = 0\n",
    "    \n",
    "    for batch_x, batch_y in final_loader:\n",
    "        loss = safe_training_step(final_model, batch_x, batch_y, final_optimizer, final_criterion, device)\n",
    "        if loss != float('inf'):\n",
    "            total_loss += loss\n",
    "            valid_batches += 1\n",
    "    \n",
    "    if valid_batches == 0:\n",
    "        print(f\"No hay batches válidos en época {epoch+1}\")\n",
    "        continue\n",
    "    \n",
    "    avg_loss = total_loss / valid_batches\n",
    "    train_losses.append(avg_loss)\n",
    "    final_scheduler.step(avg_loss)\n",
    "    \n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        print(f\"Época {epoch+1}/{final_epochs}, Loss: {avg_loss:.6f}\")\n",
    "\n",
    "# Visualizar curva de entrenamiento\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, linewidth=2, color='blue')\n",
    "plt.title('Curva de Entrenamiento - Modelo Final', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf41d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# =============================================================================\n",
    "# 5. GENERACIÓN DE PREDICCIONES PARA 2022\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"GENERACIÓN DE PREDICCIONES PARA 2022\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "def generate_predictions_2022(model, scaler_features, scaler_target, \n",
    "                             all_data, feature_cols, seq_length, sample_submission):\n",
    "    \"\"\"Generar predicciones para todas las semanas de 2022 con manejo robusto\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    \n",
    "    # Obtener IDs únicos del archivo de submisión\n",
    "    submission_ids = sample_submission['id'].unique()\n",
    "    \n",
    "    print(f\"Generando predicciones para {len(submission_ids)} registros...\")\n",
    "    \n",
    "    for i, submission_id in enumerate(submission_ids):\n",
    "        try:\n",
    "            # Extraer información del ID\n",
    "            parts = submission_id.split('_')\n",
    "            id_bar = int(parts[0])\n",
    "            anio = int(parts[1])\n",
    "            semana = int(parts[2])\n",
    "            \n",
    "            # Obtener datos históricos del barrio\n",
    "            barrio_data = all_data[all_data['id_bar'] == id_bar].sort_values('fecha')\n",
    "            \n",
    "            if len(barrio_data) >= seq_length:\n",
    "                # Tomar las últimas seq_length observaciones\n",
    "                recent_data = barrio_data.tail(seq_length)\n",
    "                features = recent_data[feature_cols].values\n",
    "                \n",
    "                # Verificar datos válidos\n",
    "                if not (np.isnan(features).any() or np.isinf(features).any()):\n",
    "                    # Convertir a tensor\n",
    "                    sequence = torch.FloatTensor(features).unsqueeze(0).to(device)\n",
    "                    \n",
    "                    # Predicción\n",
    "                    with torch.no_grad():\n",
    "                        pred = model(sequence)\n",
    "                        \n",
    "                        if validate_tensor_data(pred, f\"prediction_{i}\"):\n",
    "                            pred_scaled = pred.cpu().numpy().flatten()[0]\n",
    "                            \n",
    "                            # Desnormalizar\n",
    "                            try:\n",
    "                                pred_original = scaler_target.inverse_transform([[pred_scaled]])[0][0]\n",
    "                            except:\n",
    "                                pred_original = 0.0\n",
    "                            \n",
    "                            # Asegurar que sea no negativo\n",
    "                            pred_original = max(0, pred_original)\n",
    "                            \n",
    "                            predictions.append({\n",
    "                                'id': submission_id,\n",
    "                                'dengue': pred_original\n",
    "                            })\n",
    "                        else:\n",
    "                            # Usar media del barrio si hay problemas con predicción\n",
    "                            barrio_mean = barrio_data[target_col].mean() if len(barrio_data) > 0 else 0\n",
    "                            pred_original = scaler_target.inverse_transform([[barrio_mean]])[0][0] if not np.isnan(barrio_mean) else 0\n",
    "                            predictions.append({\n",
    "                                'id': submission_id,\n",
    "                                'dengue': max(0, pred_original)\n",
    "                            })\n",
    "                else:\n",
    "                    # Si hay NaN en features, usar media del barrio\n",
    "                    barrio_mean = barrio_data[target_col].mean() if len(barrio_data) > 0 else 0\n",
    "                    pred_original = scaler_target.inverse_transform([[barrio_mean]])[0][0] if not np.isnan(barrio_mean) else 0\n",
    "                    predictions.append({\n",
    "                        'id': submission_id,\n",
    "                        'dengue': max(0, pred_original)\n",
    "                    })\n",
    "            else:\n",
    "                # Si no hay suficientes datos históricos, usar media general\n",
    "                general_mean = all_data[target_col].mean()\n",
    "                pred_original = scaler_target.inverse_transform([[general_mean]])[0][0] if not np.isnan(general_mean) else 0\n",
    "                predictions.append({\n",
    "                    'id': submission_id,\n",
    "                    'dengue': max(0, pred_original)\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error procesando {submission_id}: {e}\")\n",
    "            # Usar predicción por defecto\n",
    "            predictions.append({\n",
    "                'id': submission_id,\n",
    "                'dengue': 0.1  # Valor mínimo positivo\n",
    "            })\n",
    "        \n",
    "        # Progreso\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Procesados {i + 1}/{len(submission_ids)} registros\")\n",
    "    \n",
    "    return pd.DataFrame(predictions)\n",
    "\n",
    "# Generar predicciones finales\n",
    "final_predictions = generate_predictions_2022(\n",
    "    final_model, scaler_features_final, scaler_target_final, \n",
    "    all_data, feature_cols, seq_length, sample_submission\n",
    ")\n",
    "\n",
    "print(f\"Predicciones generadas: {len(final_predictions)}\")\n",
    "print(\"\\nPrimeras predicciones:\")\n",
    "print(final_predictions.head(10))\n",
    "\n",
    "print(f\"\\nEstadísticas de las predicciones:\")\n",
    "print(final_predictions['dengue'].describe())\n",
    "\n",
    "# Verificar que no hay NaN en predicciones\n",
    "nan_predictions = final_predictions['dengue'].isnull().sum()\n",
    "if nan_predictions > 0:\n",
    "    print(f\"⚠ Encontrados {nan_predictions} valores NaN en predicciones, reemplazando con 0.1\")\n",
    "    final_predictions['dengue'] = final_predictions['dengue'].fillna(0.1)\n",
    "\n",
    "# Verificar formato de submisión\n",
    "print(f\"\\nVerificación del formato:\")\n",
    "print(f\"Columnas requeridas: {list(sample_submission.columns)}\")\n",
    "print(f\"Columnas generadas: {list(final_predictions.columns)}\")\n",
    "print(f\"Filas requeridas: {len(sample_submission)}\")\n",
    "print(f\"Filas generadas: {len(final_predictions)}\")\n",
    "\n",
    "# Visualizar distribución de predicciones\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(final_predictions['dengue'], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "plt.title('Distribución de Predicciones de Dengue 2022')\n",
    "plt.xlabel('Casos Predichos de Dengue')\n",
    "plt.ylabel('Frecuencia')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(final_predictions['dengue'])\n",
    "plt.title('Boxplot de Predicciones de Dengue 2022')\n",
    "plt.ylabel('Casos Predichos de Dengue')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Guardar predicciones\n",
    "output_file = f'predicciones_Peephole_LSTM_{datetime.now().strftime(\"%Y%m%d_%H%M\")}.csv'\n",
    "final_predictions.to_csv(output_file, index=False)\n",
    "print(f\"\\n✓ Predicciones guardadas en: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ffced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. RESUMEN FINAL Y VALIDACIÓN\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMEN FINAL DEL MODELO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"✓ ANÁLISIS EXPLORATORIO COMPLETADO\")\n",
    "print(f\"  - Datos analizados: {df_train.shape[0]:,} observaciones de entrenamiento\")\n",
    "print(f\"  - Período temporal: {df_train['anio'].min()}-{df_train['anio'].max()}\")\n",
    "print(f\"  - Barrios únicos: {df_train['id_bar'].nunique()}\")\n",
    "\n",
    "print(\"\\n✓ OPTIMIZACIÓN DE HIPERPARÁMETROS\")\n",
    "print(f\"  - Trials ejecutados: {len(study.trials)}\")\n",
    "print(f\"  - Mejor score de validación: {study.best_value:.6f}\")\n",
    "print(\"  - Mejores hiperparámetros:\")\n",
    "for key, value in best_params.items():\n",
    "    print(f\"    • {key}: {value}\")\n",
    "\n",
    "print(\"\\n✓ MODELO FINAL ENTRENADO\")\n",
    "print(f\"  - Arquitectura: Peephole LSTM con validación de NaN\")\n",
    "print(f\"  - Características de entrada: {len(feature_cols)}\")\n",
    "print(f\"  - Secuencias de entrenamiento: {len(final_dataset):,}\")\n",
    "print(f\"  - Épocas de entrenamiento: {final_epochs}\")\n",
    "if train_losses:\n",
    "    print(f\"  - Loss final: {train_losses[-1]:.6f}\")\n",
    "\n",
    "print(\"\\n✓ PREDICCIONES GENERADAS\")\n",
    "print(f\"  - Predicciones para 2022: {len(final_predictions):,}\")\n",
    "print(f\"  - Rango de predicciones: {final_predictions['dengue'].min():.2f} - {final_predictions['dengue'].max():.2f}\")\n",
    "print(f\"  - Media de predicciones: {final_predictions['dengue'].mean():.2f}\")\n",
    "print(f\"  - Valores NaN en predicciones: {final_predictions['dengue'].isnull().sum()}\")\n",
    "print(f\"  - Archivo guardado: {output_file}\")\n",
    "\n",
    "print(\"\\n✓ VALIDACIÓN DEL FORMATO\")\n",
    "formato_correcto = (\n",
    "    len(final_predictions) == len(sample_submission) and\n",
    "    list(final_predictions.columns) == list(sample_submission.columns) and\n",
    "    final_predictions['dengue'].isnull().sum() == 0\n",
    ")\n",
    "print(f\"  - Formato de submisión: {'✓ CORRECTO' if formato_correcto else '✗ INCORRECTO'}\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(\"PROCESO COMPLETADO EXITOSAMENTE CON MANEJO ROBUSTO DE NaN\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "# Información del modelo para uso futuro\n",
    "model_info = {\n",
    "    'best_params': best_params,\n",
    "    'feature_cols': feature_cols,\n",
    "    'seq_length': seq_length,\n",
    "    'final_loss': train_losses[-1] if train_losses else 'N/A',\n",
    "    'predictions_file': output_file,\n",
    "    'nan_handling': 'Implemented'\n",
    "}\n",
    "\n",
    "print(f\"\\nInformación del modelo guardada para referencia futura:\")\n",
    "for key, value in model_info.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
