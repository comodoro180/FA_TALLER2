{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1f0251df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo de cómputo: cuda\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Pronóstico de Casos de Dengue por Barrio usando GRU - Versión Optimizada\n",
    "# ## Red Neuronal Recurrente para Series Temporales Multivariadas con MSE Minimizado\n",
    "\n",
    "# ### 1. Configuración e Importación de Librerías\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import optuna\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar dispositivo de cómputo\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Dispositivo de cómputo: {device}\")\n",
    "\n",
    "# Establecer semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "112c5a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensiones del DataFrame: (3680, 20)\n",
      "\n",
      "Primeras filas:\n",
      "          id  id_bar  anio  semana  ESTRATO  area_barrio  dengue  \\\n",
      "0  4_2015_01       4  2015       1      3.0        0.560     0.0   \n",
      "1  5_2015_01       5  2015       1      3.0        0.842     0.0   \n",
      "2  3_2015_01       3  2015       1      1.0        0.781     0.0   \n",
      "3  8_2015_01       8  2015       1      2.0        0.394     0.0   \n",
      "4  9_2015_01       9  2015       1      2.0        0.292     0.0   \n",
      "\n",
      "   concentraciones  vivienda  equipesado  sumideros  maquina  lluvia_mean  \\\n",
      "0              0.0       0.0         0.0        0.0      0.0     0.000651   \n",
      "1              0.0       0.0         0.0        0.0      0.0     0.000651   \n",
      "2              0.0       0.0         0.0        0.0      0.0     0.000651   \n",
      "3              0.0       0.0         0.0        0.0      0.0     0.000651   \n",
      "4              0.0       0.0         0.0        0.0      0.0     0.000651   \n",
      "\n",
      "   lluvia_var  lluvia_max  lluvia_min  temperatura_mean  temperatura_var  \\\n",
      "0    0.000041      0.0625         0.0         26.163889        11.588928   \n",
      "1    0.000041      0.0625         0.0         26.163889        11.588928   \n",
      "2    0.000041      0.0625         0.0         26.163889        11.588928   \n",
      "3    0.000041      0.0625         0.0         26.163889        11.588928   \n",
      "4    0.000041      0.0625         0.0         26.163889        11.588928   \n",
      "\n",
      "   temperatura_max  temperatura_min  \n",
      "0             31.8             20.9  \n",
      "1             31.8             20.9  \n",
      "2             31.8             20.9  \n",
      "3             31.8             20.9  \n",
      "4             31.8             20.9  \n",
      "\n",
      "Información del DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3680 entries, 0 to 3679\n",
      "Data columns (total 20 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   id                3680 non-null   object \n",
      " 1   id_bar            3680 non-null   int64  \n",
      " 2   anio              3680 non-null   int64  \n",
      " 3   semana            3680 non-null   UInt32 \n",
      " 4   ESTRATO           3680 non-null   float64\n",
      " 5   area_barrio       3680 non-null   float64\n",
      " 6   dengue            3680 non-null   float64\n",
      " 7   concentraciones   3680 non-null   float64\n",
      " 8   vivienda          3680 non-null   float64\n",
      " 9   equipesado        3680 non-null   float64\n",
      " 10  sumideros         3680 non-null   float64\n",
      " 11  maquina           3680 non-null   float64\n",
      " 12  lluvia_mean       3680 non-null   float64\n",
      " 13  lluvia_var        3680 non-null   float64\n",
      " 14  lluvia_max        3680 non-null   float64\n",
      " 15  lluvia_min        3680 non-null   float64\n",
      " 16  temperatura_mean  3680 non-null   float64\n",
      " 17  temperatura_var   3680 non-null   float64\n",
      " 18  temperatura_max   3680 non-null   float64\n",
      " 19  temperatura_min   3680 non-null   float64\n",
      "dtypes: UInt32(1), float64(16), int64(2), object(1)\n",
      "memory usage: 593.0+ KB\n",
      "None\n",
      "\n",
      "Rango temporal: 2014-12-29 00:00:00 - 2021-12-27 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# ### 2. Carga y Preparación de Datos\n",
    "\n",
    "# Cargar datos desde el archivo Parquet\n",
    "df = pd.read_parquet('../../Datos/df_train.parquet')\n",
    "print(f\"Dimensiones del DataFrame: {df.shape}\")\n",
    "print(\"\\nPrimeras filas:\")\n",
    "print(df.head())\n",
    "print(\"\\nInformación del DataFrame:\")\n",
    "print(df.info())\n",
    "\n",
    "# Eliminar la columna 'lluvia_min' de df si existe\n",
    "if 'lluvia_min' in df.columns:\n",
    "    df = df.drop(columns=['lluvia_min'])\n",
    "\n",
    "# Eliminar la columna 'id' de df si existe\n",
    "if 'id' in df.columns:\n",
    "    df = df.drop(columns=['id'])\n",
    "\n",
    "# Dejar solo las columnas requeridas\n",
    "#df = df[['id_bar','anio', 'semana', 'ESTRATO', 'dengue', 'lluvia_mean', 'temperatura_mean']]\n",
    "df = df[['id_bar','anio', 'semana', 'dengue', 'lluvia_mean', 'temperatura_mean']]\n",
    "\n",
    "# Eliminar registros con semana=53\n",
    "df = df[df['semana'] != 53]\n",
    "\n",
    "# Crear columna de fecha usando formato de semanas ISO\n",
    "df['fecha'] = pd.to_datetime(df['anio'].astype(str) + df['semana'].astype(str) + '1', format='%G%V%u')\n",
    "\n",
    "# Establecer fecha como índice y ordenar\n",
    "df = df.set_index('fecha').sort_index()\n",
    "print(f\"\\nRango temporal: {df.index.min()} - {df.index.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9327f04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "fecha",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "id_bar",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "anio",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "semana",
         "rawType": "UInt32",
         "type": "integer"
        },
        {
         "name": "dengue",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "lluvia_mean",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "temperatura_mean",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "a46bbb64-912f-4a9a-a56b-67a5f152bfd4",
       "rows": [
        [
         "2014-12-29 00:00:00",
         "4",
         "2015",
         "1",
         "0.0",
         "0.0006510416666666666",
         "26.163888888888888"
        ],
        [
         "2014-12-29 00:00:00",
         "5",
         "2015",
         "1",
         "0.0",
         "0.0006510416666666666",
         "26.163888888888888"
        ],
        [
         "2014-12-29 00:00:00",
         "3",
         "2015",
         "1",
         "0.0",
         "0.0006510416666666666",
         "26.163888888888888"
        ],
        [
         "2014-12-29 00:00:00",
         "8",
         "2015",
         "1",
         "0.0",
         "0.0006510416666666666",
         "26.163888888888888"
        ],
        [
         "2014-12-29 00:00:00",
         "9",
         "2015",
         "1",
         "0.0",
         "0.0006510416666666666",
         "26.163888888888888"
        ],
        [
         "2014-12-29 00:00:00",
         "2",
         "2015",
         "1",
         "0.0",
         "0.0006510416666666666",
         "26.163888888888888"
        ],
        [
         "2014-12-29 00:00:00",
         "6",
         "2015",
         "1",
         "0.0",
         "0.0006510416666666666",
         "26.163888888888888"
        ],
        [
         "2014-12-29 00:00:00",
         "1",
         "2015",
         "1",
         "0.0",
         "0.0006510416666666666",
         "26.163888888888888"
        ],
        [
         "2014-12-29 00:00:00",
         "7",
         "2015",
         "1",
         "0.0",
         "0.0006510416666666666",
         "26.163888888888888"
        ],
        [
         "2014-12-29 00:00:00",
         "0",
         "2015",
         "1",
         "0.0",
         "0.0006510416666666666",
         "26.163888888888888"
        ],
        [
         "2015-01-05 00:00:00",
         "4",
         "2015",
         "2",
         "2.0",
         "0.05436507936507936",
         "26.20654761904762"
        ],
        [
         "2015-01-05 00:00:00",
         "5",
         "2015",
         "2",
         "1.0",
         "0.05436507936507936",
         "26.20654761904762"
        ],
        [
         "2015-01-05 00:00:00",
         "3",
         "2015",
         "2",
         "1.0",
         "0.05436507936507936",
         "26.20654761904762"
        ],
        [
         "2015-01-05 00:00:00",
         "8",
         "2015",
         "2",
         "0.0",
         "0.05436507936507936",
         "26.20654761904762"
        ],
        [
         "2015-01-05 00:00:00",
         "9",
         "2015",
         "2",
         "0.0",
         "0.05436507936507936",
         "26.20654761904762"
        ],
        [
         "2015-01-05 00:00:00",
         "2",
         "2015",
         "2",
         "0.0",
         "0.05436507936507936",
         "26.20654761904762"
        ],
        [
         "2015-01-05 00:00:00",
         "6",
         "2015",
         "2",
         "2.0",
         "0.05436507936507936",
         "26.20654761904762"
        ],
        [
         "2015-01-05 00:00:00",
         "1",
         "2015",
         "2",
         "0.0",
         "0.05436507936507936",
         "26.20654761904762"
        ],
        [
         "2015-01-05 00:00:00",
         "7",
         "2015",
         "2",
         "4.0",
         "0.05436507936507936",
         "26.20654761904762"
        ],
        [
         "2015-01-05 00:00:00",
         "0",
         "2015",
         "2",
         "3.0",
         "0.05436507936507936",
         "26.20654761904762"
        ],
        [
         "2015-01-12 00:00:00",
         "4",
         "2015",
         "3",
         "6.0",
         "0.8180753968253969",
         "25.17593984962406"
        ],
        [
         "2015-01-12 00:00:00",
         "5",
         "2015",
         "3",
         "2.0",
         "0.8180753968253969",
         "25.17593984962406"
        ],
        [
         "2015-01-12 00:00:00",
         "3",
         "2015",
         "3",
         "3.0",
         "0.8180753968253969",
         "25.17593984962406"
        ],
        [
         "2015-01-12 00:00:00",
         "8",
         "2015",
         "3",
         "1.0",
         "0.8180753968253969",
         "25.17593984962406"
        ],
        [
         "2015-01-12 00:00:00",
         "9",
         "2015",
         "3",
         "0.0",
         "0.8180753968253969",
         "25.17593984962406"
        ],
        [
         "2015-01-12 00:00:00",
         "2",
         "2015",
         "3",
         "2.0",
         "0.8180753968253969",
         "25.17593984962406"
        ],
        [
         "2015-01-12 00:00:00",
         "6",
         "2015",
         "3",
         "1.0",
         "0.8180753968253969",
         "25.17593984962406"
        ],
        [
         "2015-01-12 00:00:00",
         "1",
         "2015",
         "3",
         "2.0",
         "0.8180753968253969",
         "25.17593984962406"
        ],
        [
         "2015-01-12 00:00:00",
         "7",
         "2015",
         "3",
         "2.0",
         "0.8180753968253969",
         "25.17593984962406"
        ],
        [
         "2015-01-12 00:00:00",
         "0",
         "2015",
         "3",
         "9.0",
         "0.8180753968253969",
         "25.17593984962406"
        ],
        [
         "2015-01-19 00:00:00",
         "4",
         "2015",
         "4",
         "4.0",
         "0.2552281746031746",
         "24.31845238095238"
        ],
        [
         "2015-01-19 00:00:00",
         "5",
         "2015",
         "4",
         "2.0",
         "0.2552281746031746",
         "24.31845238095238"
        ],
        [
         "2015-01-19 00:00:00",
         "3",
         "2015",
         "4",
         "4.0",
         "0.2552281746031746",
         "24.31845238095238"
        ],
        [
         "2015-01-19 00:00:00",
         "8",
         "2015",
         "4",
         "2.0",
         "0.2552281746031746",
         "24.31845238095238"
        ],
        [
         "2015-01-19 00:00:00",
         "9",
         "2015",
         "4",
         "4.0",
         "0.2552281746031746",
         "24.31845238095238"
        ],
        [
         "2015-01-19 00:00:00",
         "2",
         "2015",
         "4",
         "6.0",
         "0.2552281746031746",
         "24.31845238095238"
        ],
        [
         "2015-01-19 00:00:00",
         "6",
         "2015",
         "4",
         "7.0",
         "0.2552281746031746",
         "24.31845238095238"
        ],
        [
         "2015-01-19 00:00:00",
         "1",
         "2015",
         "4",
         "10.0",
         "0.2552281746031746",
         "24.31845238095238"
        ],
        [
         "2015-01-19 00:00:00",
         "7",
         "2015",
         "4",
         "4.0",
         "0.2552281746031746",
         "24.31845238095238"
        ],
        [
         "2015-01-19 00:00:00",
         "0",
         "2015",
         "4",
         "5.0",
         "0.2552281746031746",
         "24.31845238095238"
        ],
        [
         "2015-01-26 00:00:00",
         "4",
         "2015",
         "5",
         "10.0",
         "0.06081349206349206",
         "25.513095238095236"
        ],
        [
         "2015-01-26 00:00:00",
         "5",
         "2015",
         "5",
         "4.0",
         "0.06081349206349206",
         "25.513095238095236"
        ],
        [
         "2015-01-26 00:00:00",
         "3",
         "2015",
         "5",
         "0.0",
         "0.06081349206349206",
         "25.513095238095236"
        ],
        [
         "2015-01-26 00:00:00",
         "8",
         "2015",
         "5",
         "1.0",
         "0.06081349206349206",
         "25.513095238095236"
        ],
        [
         "2015-01-26 00:00:00",
         "9",
         "2015",
         "5",
         "6.0",
         "0.06081349206349206",
         "25.513095238095236"
        ],
        [
         "2015-01-26 00:00:00",
         "2",
         "2015",
         "5",
         "5.0",
         "0.06081349206349206",
         "25.513095238095236"
        ],
        [
         "2015-01-26 00:00:00",
         "6",
         "2015",
         "5",
         "11.0",
         "0.06081349206349206",
         "25.513095238095236"
        ],
        [
         "2015-01-26 00:00:00",
         "1",
         "2015",
         "5",
         "7.0",
         "0.06081349206349206",
         "25.513095238095236"
        ],
        [
         "2015-01-26 00:00:00",
         "7",
         "2015",
         "5",
         "4.0",
         "0.06081349206349206",
         "25.513095238095236"
        ],
        [
         "2015-01-26 00:00:00",
         "0",
         "2015",
         "5",
         "5.0",
         "0.06081349206349206",
         "25.513095238095236"
        ]
       ],
       "shape": {
        "columns": 6,
        "rows": 3640
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_bar</th>\n",
       "      <th>anio</th>\n",
       "      <th>semana</th>\n",
       "      <th>dengue</th>\n",
       "      <th>lluvia_mean</th>\n",
       "      <th>temperatura_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fecha</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2014-12-29</th>\n",
       "      <td>4</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>26.163889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-29</th>\n",
       "      <td>5</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>26.163889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-29</th>\n",
       "      <td>3</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>26.163889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-29</th>\n",
       "      <td>8</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>26.163889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-12-29</th>\n",
       "      <td>9</td>\n",
       "      <td>2015</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>26.163889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>2</td>\n",
       "      <td>2021</td>\n",
       "      <td>52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142469</td>\n",
       "      <td>24.750446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>6</td>\n",
       "      <td>2021</td>\n",
       "      <td>52</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142469</td>\n",
       "      <td>24.750446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>1</td>\n",
       "      <td>2021</td>\n",
       "      <td>52</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142469</td>\n",
       "      <td>24.750446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>7</td>\n",
       "      <td>2021</td>\n",
       "      <td>52</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.142469</td>\n",
       "      <td>24.750446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-27</th>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>52</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.142469</td>\n",
       "      <td>24.750446</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3640 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id_bar  anio  semana  dengue  lluvia_mean  temperatura_mean\n",
       "fecha                                                                  \n",
       "2014-12-29       4  2015       1     0.0     0.000651         26.163889\n",
       "2014-12-29       5  2015       1     0.0     0.000651         26.163889\n",
       "2014-12-29       3  2015       1     0.0     0.000651         26.163889\n",
       "2014-12-29       8  2015       1     0.0     0.000651         26.163889\n",
       "2014-12-29       9  2015       1     0.0     0.000651         26.163889\n",
       "...            ...   ...     ...     ...          ...               ...\n",
       "2021-12-27       2  2021      52     0.0     0.142469         24.750446\n",
       "2021-12-27       6  2021      52     0.0     0.142469         24.750446\n",
       "2021-12-27       1  2021      52     1.0     0.142469         24.750446\n",
       "2021-12-27       7  2021      52     4.0     0.142469         24.750446\n",
       "2021-12-27       0  2021      52     1.0     0.142469         24.750446\n",
       "\n",
       "[3640 rows x 6 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5657ab24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-06-26 22:54:08,787] A new study created in memory with name: no-name-5f40fdd6-dcbf-4c60-9612-d7539f91b419\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tamaño conjunto entrenamiento: 3120\n",
      "Tamaño conjunto validación: 520\n",
      "\n",
      "Columnas categóricas: ['id_bar', 'ESTRATO']\n",
      "Columnas numéricas: ['dengue', 'lluvia_mean', 'temperatura_mean']\n",
      "Variable objetivo: dengue\n",
      "\n",
      "### Iniciando optimización de hiperparámetros con Optuna ###\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251dcbae2fea4165a963623b0f8d1be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[W 2025-06-26 22:54:08,793] Trial 0 failed with parameters: {'window_size': 25, 'hidden_size': 16, 'num_layers': 2, 'dropout_rate': 0.45985284373248053, 'learning_rate': 0.0010502105436744284, 'batch_size': 64, 'weight_decay': 2.6587543983272695e-05} because of the following error: KeyError(\"None of [Index(['id_bar_encoded', 'ESTRATO_encoded'], dtype='object')] are in the [columns]\").\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Git\\ICESI\\FA_TALLER2\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\usuario\\AppData\\Local\\Temp\\ipykernel_18248\\1433406722.py\", line 251, in objective\n",
      "    X_train_num, X_train_cat, y_train, _ = create_sequences_augmented(\n",
      "                                           ~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        train_scaled, window_size, target_col_transformed, numerical_cols,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        categorical_cols_encoded, augment=True\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"C:\\Users\\usuario\\AppData\\Local\\Temp\\ipykernel_18248\\1433406722.py\", line 104, in create_sequences_augmented\n",
      "    seq_cat = barrio_data[categorical_cols_encoded].iloc[i:i+window_size].values\n",
      "              ~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Git\\ICESI\\FA_TALLER2\\venv\\Lib\\site-packages\\pandas\\core\\frame.py\", line 4108, in __getitem__\n",
      "    indexer = self.columns._get_indexer_strict(key, \"columns\")[1]\n",
      "              ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Git\\ICESI\\FA_TALLER2\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 6200, in _get_indexer_strict\n",
      "    self._raise_if_missing(keyarr, indexer, axis_name)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Git\\ICESI\\FA_TALLER2\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 6249, in _raise_if_missing\n",
      "    raise KeyError(f\"None of [{key}] are in the [{axis_name}]\")\n",
      "KeyError: \"None of [Index(['id_bar_encoded', 'ESTRATO_encoded'], dtype='object')] are in the [columns]\"\n",
      "[W 2025-06-26 22:54:08,794] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['id_bar_encoded', 'ESTRATO_encoded'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 332\u001b[39m\n\u001b[32m    330\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m### Iniciando optimización de hiperparámetros con Optuna ###\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    331\u001b[39m study = optuna.create_study(direction=\u001b[33m'\u001b[39m\u001b[33mminimize\u001b[39m\u001b[33m'\u001b[39m, sampler=optuna.samplers.TPESampler(seed=\u001b[32m42\u001b[39m))\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m \u001b[43mstudy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mMejores hiperparámetros encontrados:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    335\u001b[39m \u001b[38;5;28mprint\u001b[39m(study.best_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git\\ICESI\\FA_TALLER2\\venv\\Lib\\site-packages\\optuna\\study\\study.py:475\u001b[39m, in \u001b[36mStudy.optimize\u001b[39m\u001b[34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m    373\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34moptimize\u001b[39m(\n\u001b[32m    374\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    375\u001b[39m     func: ObjectiveFuncType,\n\u001b[32m   (...)\u001b[39m\u001b[32m    382\u001b[39m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    383\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    473\u001b[39m \u001b[33;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[32m    474\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m475\u001b[39m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git\\ICESI\\FA_TALLER2\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:63\u001b[39m, in \u001b[36m_optimize\u001b[39m\u001b[34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs == \u001b[32m1\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     76\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs == -\u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git\\ICESI\\FA_TALLER2\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:160\u001b[39m, in \u001b[36m_optimize_sequential\u001b[39m\u001b[34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     frozen_trial = \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    162\u001b[39m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[32m    163\u001b[39m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git\\ICESI\\FA_TALLER2\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:248\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    241\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mShould not reach.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    243\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    244\u001b[39m     frozen_trial.state == TrialState.FAIL\n\u001b[32m    245\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    246\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[32m    247\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git\\ICESI\\FA_TALLER2\\venv\\Lib\\site-packages\\optuna\\study\\_optimize.py:197\u001b[39m, in \u001b[36m_run_trial\u001b[39m\u001b[34m(study, func, catch)\u001b[39m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial._trial_id, study._storage):\n\u001b[32m    196\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m         value_or_values = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions.TrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    199\u001b[39m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[32m    200\u001b[39m         state = TrialState.PRUNED\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 251\u001b[39m, in \u001b[36mobjective\u001b[39m\u001b[34m(trial)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;66;03m# Crear secuencias con augmentation\u001b[39;00m\n\u001b[32m    249\u001b[39m categorical_cols_encoded = [col + \u001b[33m'\u001b[39m\u001b[33m_encoded\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m categorical_cols]\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m X_train_num, X_train_cat, y_train, _ = \u001b[43mcreate_sequences_augmented\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col_transformed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumerical_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcategorical_cols_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    254\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m X_val_num, X_val_cat, y_val, _ = create_sequences_augmented(\n\u001b[32m    256\u001b[39m     val_scaled, window_size, target_col_transformed, numerical_cols, \n\u001b[32m    257\u001b[39m     categorical_cols_encoded, augment=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    258\u001b[39m )\n\u001b[32m    260\u001b[39m \u001b[38;5;66;03m# Crear datasets y dataloaders\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 104\u001b[39m, in \u001b[36mcreate_sequences_augmented\u001b[39m\u001b[34m(data, window_size, target_col, feature_cols, categorical_cols_encoded, augment, noise_level)\u001b[39m\n\u001b[32m    101\u001b[39m     seq_num = seq_num + noise\n\u001b[32m    103\u001b[39m \u001b[38;5;66;03m# Secuencia de características categóricas\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m seq_cat = \u001b[43mbarrio_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcategorical_cols_encoded\u001b[49m\u001b[43m]\u001b[49m.iloc[i:i+window_size].values\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# Valor objetivo\u001b[39;00m\n\u001b[32m    106\u001b[39m target = barrio_data[target_col].iloc[i+window_size]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git\\ICESI\\FA_TALLER2\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[32m   4107\u001b[39m         key = \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[32m-> \u001b[39m\u001b[32m4108\u001b[39m     indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcolumns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[32m1\u001b[39m]\n\u001b[32m   4110\u001b[39m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) == \u001b[38;5;28mbool\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git\\ICESI\\FA_TALLER2\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[39m, in \u001b[36mIndex._get_indexer_strict\u001b[39m\u001b[34m(self, key, axis_name)\u001b[39m\n\u001b[32m   6197\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   6198\u001b[39m     keyarr, indexer, new_indexer = \u001b[38;5;28mself\u001b[39m._reindex_non_unique(keyarr)\n\u001b[32m-> \u001b[39m\u001b[32m6200\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6202\u001b[39m keyarr = \u001b[38;5;28mself\u001b[39m.take(indexer)\n\u001b[32m   6203\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[32m   6204\u001b[39m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Git\\ICESI\\FA_TALLER2\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[39m, in \u001b[36mIndex._raise_if_missing\u001b[39m\u001b[34m(self, key, indexer, axis_name)\u001b[39m\n\u001b[32m   6247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[32m   6248\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m nmissing == \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[32m-> \u001b[39m\u001b[32m6249\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m]\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   6251\u001b[39m     not_found = \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask.nonzero()[\u001b[32m0\u001b[39m]].unique())\n\u001b[32m   6252\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not in index\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: \"None of [Index(['id_bar_encoded', 'ESTRATO_encoded'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# ### 3. Preprocesamiento e Ingeniería de Características Mejorada\n",
    "\n",
    "# División de datos según el año\n",
    "train_data = df[df['anio'] < 2021]\n",
    "val_data = df[df['anio'] == 2021]\n",
    "print(f\"\\nTamaño conjunto entrenamiento: {len(train_data)}\")\n",
    "print(f\"Tamaño conjunto validación: {len(val_data)}\")\n",
    "\n",
    "# Identificar columnas numéricas y categóricas\n",
    "categorical_cols = ['id_bar', 'ESTRATO']\n",
    "numerical_cols = [col for col in df.columns if col not in categorical_cols + ['anio', 'semana']]\n",
    "target_col = 'dengue'\n",
    "\n",
    "print(f\"\\nColumnas categóricas: {categorical_cols}\")\n",
    "print(f\"Columnas numéricas: {numerical_cols}\")\n",
    "print(f\"Variable objetivo: {target_col}\")\n",
    "\n",
    "# ### MEJORA 1: Transformación logarítmica para estabilizar la varianza\n",
    "# Aplicar transformación log1p a la variable objetivo para manejar valores sesgados\n",
    "df['dengue_log'] = np.log1p(df['dengue'])\n",
    "target_col_transformed = 'dengue_log'\n",
    "\n",
    "# ### MEJORA 2: Ingeniería de características adicionales\n",
    "# Agregar características temporales cíclicas\n",
    "df['sin_semana'] = np.sin(2 * np.pi * df['semana'] / 52)\n",
    "df['cos_semana'] = np.cos(2 * np.pi * df['semana'] / 52)\n",
    "\n",
    "# Agregar lags de la variable objetivo por barrio\n",
    "lag_features = []\n",
    "for lag in [1, 2, 4, 8, 12]:  # Lags de 1, 2, 4, 8 y 12 semanas\n",
    "    lag_col = f'dengue_lag_{lag}'\n",
    "    df[lag_col] = df.groupby('id_bar')['dengue'].shift(lag)\n",
    "    lag_features.append(lag_col)\n",
    "\n",
    "# Agregar media móvil y desviación estándar móvil\n",
    "for window in [4, 8, 12]:\n",
    "    ma_col = f'dengue_ma_{window}'\n",
    "    std_col = f'dengue_std_{window}'\n",
    "    # Usar transform para mantener el índice original\n",
    "    df[ma_col] = df.groupby('id_bar')['dengue'].transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
    "    df[std_col] = df.groupby('id_bar')['dengue'].transform(lambda x: x.rolling(window=window, min_periods=1).std())\n",
    "    lag_features.extend([ma_col, std_col])\n",
    "\n",
    "# Rellenar valores faltantes en features de lag\n",
    "df[lag_features] = df[lag_features].fillna(0)\n",
    "# Para las desviaciones estándar, también rellenar NaN que pueden aparecer en ventanas pequeñas\n",
    "std_cols = [col for col in lag_features if 'std' in col]\n",
    "df[std_cols] = df[std_cols].fillna(0)\n",
    "\n",
    "# Actualizar lista de columnas numéricas\n",
    "numerical_cols.extend(['sin_semana', 'cos_semana', 'dengue_log'] + lag_features)\n",
    "numerical_cols = list(set(numerical_cols))  # Eliminar duplicados\n",
    "\n",
    "# Codificar variables categóricas para embeddings\n",
    "# label_encoders = {}\n",
    "# for col in categorical_cols:\n",
    "#     le = LabelEncoder()\n",
    "#     df[col + '_encoded'] = le.fit_transform(df[col])\n",
    "#     label_encoders[col] = le\n",
    "#     print(f\"\\nCardinalidad de {col}: {len(le.classes_)}\")\n",
    "\n",
    "# Actualizar división con columnas codificadas\n",
    "train_data = df[df['anio'] < 2021]\n",
    "val_data = df[df['anio'] == 2021]\n",
    "\n",
    "# ### MEJORA 3: Usar RobustScaler para manejar outliers\n",
    "scaler = RobustScaler()\n",
    "scaler.fit(train_data[numerical_cols])\n",
    "\n",
    "# Aplicar escalado\n",
    "train_scaled = train_data.copy()\n",
    "val_scaled = val_data.copy()\n",
    "train_scaled[numerical_cols] = scaler.transform(train_data[numerical_cols])\n",
    "val_scaled[numerical_cols] = scaler.transform(val_data[numerical_cols])\n",
    "\n",
    "# ### MEJORA 4: Función mejorada para crear secuencias con data augmentation\n",
    "def create_sequences_augmented(data, window_size, target_col, feature_cols, categorical_cols_encoded, \n",
    "                              augment=False, noise_level=0.01):\n",
    "    \"\"\"\n",
    "    Crea secuencias de ventanas deslizantes para el modelo GRU con opción de augmentation.\n",
    "    \"\"\"\n",
    "    sequences_num = []\n",
    "    sequences_cat = []\n",
    "    targets = []\n",
    "    barrio_ids = []\n",
    "    \n",
    "    # Agrupar por barrio para mantener continuidad temporal\n",
    "    for barrio in data['id_bar'].unique():\n",
    "        barrio_data = data[data['id_bar'] == barrio].sort_index()\n",
    "        \n",
    "        if len(barrio_data) <= window_size:\n",
    "            continue\n",
    "            \n",
    "        for i in range(len(barrio_data) - window_size):\n",
    "            # Secuencia de características numéricas\n",
    "            seq_num = barrio_data[feature_cols].iloc[i:i+window_size].values\n",
    "            \n",
    "            # Data augmentation: agregar ruido gaussiano pequeño durante entrenamiento\n",
    "            if augment and np.random.random() > 0.5:\n",
    "                noise = np.random.normal(0, noise_level, seq_num.shape)\n",
    "                seq_num = seq_num + noise\n",
    "            \n",
    "            # Secuencia de características categóricas\n",
    "            seq_cat = barrio_data[categorical_cols_encoded].iloc[i:i+window_size].values\n",
    "            # Valor objetivo\n",
    "            target = barrio_data[target_col].iloc[i+window_size]\n",
    "            \n",
    "            sequences_num.append(seq_num)\n",
    "            sequences_cat.append(seq_cat)\n",
    "            targets.append(target)\n",
    "            barrio_ids.append(barrio)\n",
    "    \n",
    "    return (np.array(sequences_num, dtype=np.float32), \n",
    "            np.array(sequences_cat, dtype=np.int64),\n",
    "            np.array(targets, dtype=np.float32),\n",
    "            np.array(barrio_ids))\n",
    "\n",
    "# ### 4. Definición del Modelo GRU Mejorado con PyTorch\n",
    "\n",
    "class DengueDataset(Dataset):\n",
    "    \"\"\"Dataset personalizado para las secuencias de dengue\"\"\"\n",
    "    def __init__(self, sequences_num, sequences_cat, targets):\n",
    "        self.sequences_num = sequences_num\n",
    "        self.sequences_cat = sequences_cat\n",
    "        self.targets = targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return (torch.FloatTensor(self.sequences_num[idx]),\n",
    "                torch.LongTensor(self.sequences_cat[idx]),\n",
    "                torch.FloatTensor([self.targets[idx]]))\n",
    "\n",
    "# ### MEJORA 5: Modelo GRU mejorado con Batch Normalization y Attention\n",
    "class ImprovedGRUModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Modelo GRU mejorado con embeddings, batch normalization, y mecanismo de atención\n",
    "    \"\"\"\n",
    "    def __init__(self, num_features, embedding_dims, hidden_size, num_layers, dropout_rate):\n",
    "        super(ImprovedGRUModel, self).__init__()\n",
    "        \n",
    "        # Embeddings para variables categóricas\n",
    "        self.embeddings = nn.ModuleList()\n",
    "        total_embedding_size = 0\n",
    "        \n",
    "        # for card, dim in embedding_dims:\n",
    "        #     self.embeddings.append(nn.Embedding(card, dim))\n",
    "        #     total_embedding_size += dim\n",
    "        \n",
    "        # Tamaño de entrada para GRU\n",
    "        input_size = num_features + total_embedding_size\n",
    "        \n",
    "        # Batch normalization para entrada\n",
    "        self.input_bn = nn.BatchNorm1d(input_size)\n",
    "        \n",
    "        # Capas GRU bidireccionales\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, \n",
    "                         batch_first=True, dropout=dropout_rate if num_layers > 1 else 0,\n",
    "                         bidirectional=True)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        \n",
    "        # Batch normalization para hidden states\n",
    "        self.hidden_bn = nn.BatchNorm1d(hidden_size * 2)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Capas de salida con residual connection\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x_num, x_cat):\n",
    "        batch_size, seq_len, _ = x_num.shape\n",
    "        \n",
    "        # Procesar embeddings categóricos\n",
    "        embeddings_list = []\n",
    "        for i, embedding in enumerate(self.embeddings):\n",
    "            cat_embedded = embedding(x_cat[:, :, i])\n",
    "            embeddings_list.append(cat_embedded)\n",
    "        \n",
    "        # Concatenar embeddings\n",
    "        cat_embedded = torch.cat(embeddings_list, dim=-1)\n",
    "        \n",
    "        # Concatenar características numéricas y categóricas\n",
    "        x = torch.cat([x_num, cat_embedded], dim=-1)\n",
    "        \n",
    "        # Batch normalization (reshape para BN)\n",
    "        x = x.view(-1, x.size(-1))\n",
    "        x = self.input_bn(x)\n",
    "        x = x.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Pasar por GRU bidireccional\n",
    "        out, _ = self.gru(x)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(out)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)\n",
    "        \n",
    "        # Aplicar attention weights\n",
    "        out = torch.sum(out * attention_weights, dim=1)\n",
    "        \n",
    "        # Batch normalization para hidden states\n",
    "        out = self.hidden_bn(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Capas fully connected con activación\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# ### MEJORA 6: Función de pérdida Huber para robustez ante outliers\n",
    "class HuberLoss(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super(HuberLoss, self).__init__()\n",
    "        self.delta = delta\n",
    "        \n",
    "    def forward(self, y_pred, y_true):\n",
    "        residual = torch.abs(y_pred - y_true)\n",
    "        condition = residual < self.delta\n",
    "        small_error = 0.5 * torch.square(residual)\n",
    "        large_error = self.delta * residual - 0.5 * self.delta * self.delta\n",
    "        return torch.mean(torch.where(condition, small_error, large_error))\n",
    "\n",
    "# ### 5. Optimización de Hiperparámetros Mejorada con Optuna\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Función objetivo mejorada para Optuna\n",
    "    \"\"\"\n",
    "    # Espacio de búsqueda ampliado de hiperparámetros\n",
    "    window_size = trial.suggest_int('window_size', 12, 48)  # Ventanas más grandes\n",
    "    hidden_size = trial.suggest_categorical('hidden_size', [16, 32, 64, 128, 256])\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 4)  # Más capas\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.2, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 5e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    weight_decay = trial.suggest_float('weight_decay', 1e-5, 1e-3, log=True)\n",
    "    \n",
    "    # Crear secuencias con augmentation\n",
    "    categorical_cols_encoded = [col + '_encoded' for col in categorical_cols]\n",
    "    \n",
    "    X_train_num, X_train_cat, y_train, _ = create_sequences_augmented(\n",
    "        train_scaled, window_size, target_col_transformed, numerical_cols, \n",
    "        categorical_cols_encoded, augment=True\n",
    "    )\n",
    "    X_val_num, X_val_cat, y_val, _ = create_sequences_augmented(\n",
    "        val_scaled, window_size, target_col_transformed, numerical_cols, \n",
    "        categorical_cols_encoded, augment=False\n",
    "    )\n",
    "    \n",
    "    # Crear datasets y dataloaders\n",
    "    train_dataset = DengueDataset(X_train_num, X_train_cat, y_train)\n",
    "    val_dataset = DengueDataset(X_val_num, X_val_cat, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Configurar modelo\n",
    "    num_features = len(numerical_cols)\n",
    "    embedding_dims = []\n",
    "    # embedding_dims = [\n",
    "    #     (len(label_encoders['id_bar'].classes_), min(50, len(label_encoders['id_bar'].classes_)//2)),\n",
    "    #     (len(label_encoders['ESTRATO'].classes_), min(10, len(label_encoders['ESTRATO'].classes_)//2))\n",
    "    # ]\n",
    "    \n",
    "    model = ImprovedGRUModel(num_features, embedding_dims, hidden_size, num_layers, dropout_rate).to(device)\n",
    "    criterion = HuberLoss(delta=1.0)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)\n",
    "    \n",
    "    # Entrenamiento con gradient clipping\n",
    "    n_epochs = 50\n",
    "    best_val_loss = float('inf')\n",
    "    patience = 25\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x_num, x_cat, y in train_loader:\n",
    "            x_num, x_cat, y = x_num.to(device), x_cat.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_num, x_cat)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for x_num, x_cat, y in val_loader:\n",
    "                x_num, x_cat, y = x_num.to(device), x_cat.to(device), y.to(device)\n",
    "                output = model(x_num, x_cat)\n",
    "                loss = criterion(output, y)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        if patience_counter >= patience:\n",
    "            break\n",
    "    \n",
    "    return best_val_loss\n",
    "\n",
    "# Ejecutar optimización con Optuna\n",
    "print(\"\\n### Iniciando optimización de hiperparámetros con Optuna ###\")\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(\"\\nMejores hiperparámetros encontrados:\")\n",
    "print(study.best_params)\n",
    "print(f\"\\nMejor pérdida en validación: {study.best_value:.4f}\")\n",
    "\n",
    "# ### 6. Entrenamiento del Modelo Final con Ensemble\n",
    "\n",
    "# Obtener mejores hiperparámetros\n",
    "best_params = study.best_params\n",
    "window_size = best_params['window_size']\n",
    "hidden_size = best_params['hidden_size']\n",
    "num_layers = best_params['num_layers']\n",
    "dropout_rate = best_params['dropout_rate']\n",
    "learning_rate = best_params['learning_rate']\n",
    "batch_size = best_params['batch_size']\n",
    "weight_decay = best_params['weight_decay']\n",
    "\n",
    "print(\"\\n### Entrenando ensemble de modelos finales ###\")\n",
    "\n",
    "# Combinar datos de entrenamiento y validación\n",
    "combined_data = pd.concat([train_data, val_data])\n",
    "combined_scaled = combined_data.copy()\n",
    "combined_scaled[numerical_cols] = scaler.transform(combined_data[numerical_cols])\n",
    "\n",
    "# Crear secuencias con datos combinados\n",
    "categorical_cols_encoded = [col + '_encoded' for col in categorical_cols]\n",
    "\n",
    "# ### MEJORA 7: Entrenar ensemble de modelos\n",
    "n_models = 5\n",
    "models_ensemble = []\n",
    "\n",
    "for model_idx in range(n_models):\n",
    "    print(f\"\\nEntrenando modelo {model_idx + 1}/{n_models}\")\n",
    "    \n",
    "    # Crear secuencias con diferente semilla para augmentation\n",
    "    np.random.seed(42 + model_idx)\n",
    "    X_combined_num, X_combined_cat, y_combined, _ = create_sequences_augmented(\n",
    "        combined_scaled, window_size, target_col_transformed, numerical_cols, \n",
    "        categorical_cols_encoded, augment=True\n",
    "    )\n",
    "    \n",
    "    # Crear dataset y dataloader\n",
    "    combined_dataset = DengueDataset(X_combined_num, X_combined_cat, y_combined)\n",
    "    combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Configurar modelo\n",
    "    num_features = len(numerical_cols)\n",
    "    embedding_dims = [\n",
    "        (len(label_encoders['id_bar'].classes_), min(50, len(label_encoders['id_bar'].classes_)//2)),\n",
    "        (len(label_encoders['ESTRATO'].classes_), min(10, len(label_encoders['ESTRATO'].classes_)//2))\n",
    "    ]\n",
    "    \n",
    "    model = ImprovedGRUModel(num_features, embedding_dims, hidden_size, num_layers, dropout_rate).to(device)\n",
    "    criterion = HuberLoss(delta=1.0)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=20)\n",
    "    \n",
    "    # Entrenamiento\n",
    "    n_epochs = 50\n",
    "    train_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for x_num, x_cat, y in combined_loader:\n",
    "            x_num, x_cat, y = x_num.to(device), x_cat.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_num, x_cat)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        avg_loss = epoch_loss / len(combined_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Época {epoch+1}/{n_epochs}, Pérdida: {avg_loss:.4f}\")\n",
    "    \n",
    "    models_ensemble.append(model)\n",
    "\n",
    "# ### 7. Generación de Pronósticos Mejorados para 2022\n",
    "\n",
    "print(\"\\n### Generando pronósticos con ensemble para 2022 ###\")\n",
    "\n",
    "# Preparar datos para pronóstico\n",
    "all_data = df.copy()\n",
    "all_data_scaled = all_data.copy()\n",
    "all_data_scaled[numerical_cols] = scaler.transform(all_data[numerical_cols])\n",
    "\n",
    "# ### MEJORA 8: Función mejorada para generar pronósticos con ensemble\n",
    "def generate_ensemble_forecasts(models, data, window_size, n_weeks_ahead, year):\n",
    "    \"\"\"\n",
    "    Genera pronósticos autorregresivos usando ensemble de modelos\n",
    "    \"\"\"\n",
    "    all_forecasts = []\n",
    "    \n",
    "    for model in models:\n",
    "        model.eval()\n",
    "        model_forecasts = []\n",
    "        \n",
    "        # Para cada barrio\n",
    "        for barrio in data['id_bar'].unique():\n",
    "            barrio_data = data[data['id_bar'] == barrio].sort_index()\n",
    "            barrio_id = barrio_data['id_bar'].iloc[0]\n",
    "            estrato_encoded = barrio_data['ESTRATO_encoded'].iloc[0]\n",
    "            \n",
    "            # Obtener última secuencia conocida\n",
    "            last_sequence_num = barrio_data[numerical_cols].iloc[-window_size:].values\n",
    "            last_sequence_cat = np.array([[barrio_data['id_bar_encoded'].iloc[0], \n",
    "                                          estrato_encoded] for _ in range(window_size)])\n",
    "            \n",
    "            # Generar pronósticos semana a semana\n",
    "            for week in range(1, n_weeks_ahead + 1):\n",
    "                # Preparar entrada\n",
    "                x_num = torch.FloatTensor(last_sequence_num).unsqueeze(0).to(device)\n",
    "                x_cat = torch.LongTensor(last_sequence_cat).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Hacer predicción\n",
    "                with torch.no_grad():\n",
    "                    pred_scaled = model(x_num, x_cat).cpu().numpy()[0, 0]\n",
    "                \n",
    "                # Desescalar predicción y aplicar transformación inversa\n",
    "                dengue_log_idx = numerical_cols.index(target_col_transformed)\n",
    "                pred_log_original = pred_scaled * scaler.scale_[dengue_log_idx] + scaler.center_[dengue_log_idx]\n",
    "                pred_original_scale = np.expm1(pred_log_original)  # Inversa de log1p\n",
    "                \n",
    "                # Asegurar que la predicción no sea negativa\n",
    "                pred_original_scale = max(0, pred_original_scale)\n",
    "                \n",
    "                # Guardar predicción\n",
    "                model_forecasts.append({\n",
    "                    'id_bar': barrio_id,\n",
    "                    'anio': year,\n",
    "                    'semana': week,\n",
    "                    'dengue': pred_original_scale\n",
    "                })\n",
    "                \n",
    "                # Actualizar secuencia para próxima predicción\n",
    "                new_row = last_sequence_num[-1].copy()\n",
    "                new_row[dengue_log_idx] = pred_scaled\n",
    "                \n",
    "                # Actualizar features de lag\n",
    "                dengue_idx = numerical_cols.index('dengue')\n",
    "                new_row[dengue_idx] = pred_original_scale\n",
    "                \n",
    "                # Actualizar ventana deslizante\n",
    "                last_sequence_num = np.vstack([last_sequence_num[1:], new_row])\n",
    "        \n",
    "        all_forecasts.append(pd.DataFrame(model_forecasts))\n",
    "    \n",
    "    # Combinar predicciones del ensemble (promedio)\n",
    "    ensemble_df = pd.concat(all_forecasts)\n",
    "    ensemble_forecasts = ensemble_df.groupby(['id_bar', 'anio', 'semana'])['dengue'].mean().reset_index()\n",
    "    \n",
    "    return ensemble_forecasts\n",
    "\n",
    "# Generar pronósticos del ensemble\n",
    "forecasts_2022 = generate_ensemble_forecasts(models_ensemble, all_data_scaled, window_size, 52, 2022)\n",
    "\n",
    "print(f\"\\nPronósticos generados: {len(forecasts_2022)} registros\")\n",
    "print(\"\\nEstadísticas de pronósticos:\")\n",
    "print(forecasts_2022['dengue'].describe())\n",
    "\n",
    "# ### 8. Creación del Archivo de Salida\n",
    "\n",
    "# Crear DataFrame con formato requerido\n",
    "output_df = pd.DataFrame()\n",
    "output_df['id'] = (forecasts_2022['id_bar'].astype(str) + '_' + \n",
    "                   forecasts_2022['anio'].astype(str) + '_' + \n",
    "                   forecasts_2022['semana'].apply(lambda x: f\"{x:02d}\"))\n",
    "output_df['dengue'] = forecasts_2022['dengue'].round(2)\n",
    "\n",
    "# Verificar formato\n",
    "print(\"\\nPrimeras filas del archivo de salida:\")\n",
    "print(output_df.head(10))\n",
    "print(f\"\\nTotal de registros: {len(output_df)}\")\n",
    "\n",
    "# Guardar archivo CSV\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_filename = f'pronosticos_GRU_2_optimizado_{timestamp}.csv'\n",
    "output_df.to_csv(output_filename, index=False)\n",
    "print(f\"\\nArchivo '{output_filename}' guardado exitosamente.\")\n",
    "\n",
    "# ### 9. Visualización y Análisis de Resultados\n",
    "\n",
    "# Visualización de algunos pronósticos\n",
    "sample_barrios = forecasts_2022['id_bar'].unique()[:5]\n",
    "fig = go.Figure()\n",
    "\n",
    "for barrio in sample_barrios:\n",
    "    barrio_forecast = forecasts_2022[forecasts_2022['id_bar'] == barrio]\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=barrio_forecast['semana'],\n",
    "        y=barrio_forecast['dengue'],\n",
    "        mode='lines+markers',\n",
    "        name=f'Barrio {barrio}',\n",
    "        line=dict(width=2),\n",
    "        marker=dict(size=6)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Pronósticos de Dengue para 2022 - Modelo Optimizado (Muestra de 5 Barrios)',\n",
    "    xaxis_title='Semana del Año',\n",
    "    yaxis_title='Casos de Dengue',\n",
    "    hovermode='x unified',\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Comparación de distribuciones\n",
    "fig_dist = go.Figure()\n",
    "fig_dist.add_trace(go.Histogram(x=forecasts_2022['dengue'], name='Pronósticos 2022', nbinsx=50))\n",
    "fig_dist.update_layout(\n",
    "    title='Distribución de Pronósticos de Dengue 2022',\n",
    "    xaxis_title='Casos de Dengue',\n",
    "    yaxis_title='Frecuencia',\n",
    "    template='plotly_white'\n",
    ")\n",
    "fig_dist.show()\n",
    "\n",
    "print(\"\\n### Proceso completado exitosamente ###\")\n",
    "print(\"\\nMejoras implementadas para minimizar MSE:\")\n",
    "print(\"1. Transformación logarítmica de la variable objetivo\")\n",
    "print(\"2. Ingeniería de características: lags, medias móviles, características cíclicas\")\n",
    "print(\"3. RobustScaler para manejar outliers\")\n",
    "print(\"4. Data augmentation con ruido gaussiano\")\n",
    "print(\"5. Modelo GRU bidireccional con attention mechanism y batch normalization\")\n",
    "print(\"6. Función de pérdida Huber (robusta ante outliers)\")\n",
    "print(\"7. Ensemble de 5 modelos\")\n",
    "print(\"8. Learning rate scheduling y gradient clipping\")\n",
    "print(\"9. Optimización AdamW con weight decay\")\n",
    "print(\"10. Mayor espacio de búsqueda de hiperparámetros\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
